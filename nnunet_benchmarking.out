wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: arild (arild-ntnu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /cluster/home/aris/idun_ws/nnUNet-benchmarking/wandb/run-20241108_085537-gnmb4vjd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run BraTs_2023_UMamba_Bot_3d_fullres_fold_0_2024.11.08_08:55:36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/arild-ntnu/BraTs_2023_UMamba_Bot
wandb: üöÄ View run at https://wandb.ai/arild-ntnu/BraTs_2023_UMamba_Bot/runs/gnmb4vjd
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################


This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [160, 192, 128], 'median_image_size_in_voxels': [140.0, 171.0, 137.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [True, True, True, True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset137_BraTS2021', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [140, 171, 137], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 95242.25, 'mean': 871.816650390625, 'median': 407.0, 'min': 0.10992202162742615, 'percentile_00_5': 55.0, 'percentile_99_5': 5825.0, 'std': 2023.5313720703125}, '1': {'max': 1905559.25, 'mean': 1698.2144775390625, 'median': 552.0, 'min': 0.0, 'percentile_00_5': 47.0, 'percentile_99_5': 8322.0, 'std': 18787.4140625}, '2': {'max': 4438107.0, 'mean': 2141.349365234375, 'median': 738.0, 'min': 0.0, 'percentile_00_5': 110.0, 'percentile_99_5': 10396.0, 'std': 45159.37890625}, '3': {'max': 580014.3125, 'mean': 995.436279296875, 'median': 512.3143920898438, 'min': 0.0, 'percentile_00_5': 108.0, 'percentile_99_5': 11925.0, 'std': 4629.87939453125}}} 

2024-11-08 08:55:43.360207: unpacking dataset...
2024-11-08 08:55:51.557050: unpacking done...
2024-11-08 08:55:52.079086: do_dummy_2d_data_aug: False
2024-11-08 08:55:52.085401: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTS2021/splits_final.json
2024-11-08 08:55:52.099118: The split file contains 5 splits.
2024-11-08 08:55:52.099774: Desired fold for training: 0
2024-11-08 08:55:52.100383: This split has 1000 training and 251 validation cases.
2024-11-08 08:55:52.862961: Unable to plot network architecture:
2024-11-08 08:55:52.863696: No module named 'hiddenlayer'
2024-11-08 08:55:53.943460: 
2024-11-08 08:55:53.944270: Epoch 0
2024-11-08 08:55:54.247635: Current learning rate: 0.01
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: arild (arild-ntnu). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /cluster/home/aris/idun_ws/nnUNet-benchmarking/wandb/run-20241108_085638-l9gfm8cn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run BraTs_2023_UMamba_Bot_3d_fullres_fold_0_2024.11.08_08:56:38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/arild-ntnu/BraTs_2023_UMamba_Bot
wandb: üöÄ View run at https://wandb.ai/arild-ntnu/BraTs_2023_UMamba_Bot/runs/l9gfm8cn
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaBot: UMambaBot(
  (encoder): ResidualEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): Sequential(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (ln): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
  (mamba): Mamba(
    (in_proj): Linear(in_features=320, out_features=1280, bias=False)
    (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
    (act): SiLU()
    (x_proj): Linear(in_features=640, out_features=52, bias=False)
    (dt_proj): Linear(in_features=20, out_features=640, bias=True)
    (out_proj): Linear(in_features=640, out_features=320, bias=False)
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): Sequential(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(640, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(640, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
    )
    (seg_layers): ModuleList(
      (0): Conv3d(320, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (1): Conv3d(256, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (2): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (3): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (4): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [160, 192, 128], 'median_image_size_in_voxels': [140.0, 171.0, 137.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [True, True, True, True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset137_BraTS2021', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [140, 171, 137], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 95242.25, 'mean': 871.816650390625, 'median': 407.0, 'min': 0.10992202162742615, 'percentile_00_5': 55.0, 'percentile_99_5': 5825.0, 'std': 2023.5313720703125}, '1': {'max': 1905559.25, 'mean': 1698.2144775390625, 'median': 552.0, 'min': 0.0, 'percentile_00_5': 47.0, 'percentile_99_5': 8322.0, 'std': 18787.4140625}, '2': {'max': 4438107.0, 'mean': 2141.349365234375, 'median': 738.0, 'min': 0.0, 'percentile_00_5': 110.0, 'percentile_99_5': 10396.0, 'std': 45159.37890625}, '3': {'max': 580014.3125, 'mean': 995.436279296875, 'median': 512.3143920898438, 'min': 0.0, 'percentile_00_5': 108.0, 'percentile_99_5': 11925.0, 'std': 4629.87939453125}}} 

2024-11-08 08:56:45.932781: unpacking dataset...
2024-11-08 08:56:48.699199: unpacking done...
2024-11-08 08:56:48.702558: do_dummy_2d_data_aug: False
2024-11-08 08:56:48.709087: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTS2021/splits_final.json
2024-11-08 08:56:48.711425: The split file contains 5 splits.
2024-11-08 08:56:48.711998: Desired fold for training: 0
2024-11-08 08:56:48.712600: This split has 1000 training and 251 validation cases.
2024-11-08 08:56:48.750768: Unable to plot network architecture:
2024-11-08 08:56:48.751441: No module named 'hiddenlayer'
2024-11-08 08:56:48.766579: 
2024-11-08 08:56:48.767169: Epoch 0
2024-11-08 08:56:48.768187: Current learning rate: 0.01
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0
2024-11-08 09:01:49.252181: train_loss -0.552
2024-11-08 09:01:49.406615: val_loss -0.7036
2024-11-08 09:01:49.407583: Pseudo dice [0.8491, 0.7528, 0.8016]
2024-11-08 09:01:49.408513: Yayy! New best EMA pseudo Dice: 0.8012
2024-11-08 09:01:50.621990: Epoch time: 355.29 s
2024-11-08 09:01:54.298789: 
2024-11-08 09:01:54.300634: Epoch 1
2024-11-08 09:01:54.301573: Current learning rate: 0.00982
2024-11-08 09:06:38.494781: train_loss -0.7183
2024-11-08 09:06:38.516962: val_loss -0.6453
2024-11-08 09:06:38.533149: Pseudo dice [0.8396, 0.7367, 0.7966]
2024-11-08 09:06:38.535073: Epoch time: 284.2 s
2024-11-08 09:06:42.885051: 
2024-11-08 09:06:42.901938: Epoch 2
2024-11-08 09:06:42.903174: Current learning rate: 0.00964
2024-11-08 09:11:41.447288: train_loss -0.7471
2024-11-08 09:11:41.459434: val_loss -0.7501
2024-11-08 09:11:41.471127: Pseudo dice [0.9052, 0.8351, 0.8229]
2024-11-08 09:11:41.475830: Yayy! New best EMA pseudo Dice: 0.8056
2024-11-08 09:11:42.688026: Epoch time: 298.56 s
2024-11-08 09:11:47.053913: 
2024-11-08 09:11:47.069761: Epoch 3
2024-11-08 09:11:47.070991: Current learning rate: 0.00946
2024-11-08 09:16:46.754580: train_loss -0.7371
2024-11-08 09:16:46.775294: val_loss -0.7574
2024-11-08 09:16:46.779670: Pseudo dice [0.8893, 0.7999, 0.8412]
2024-11-08 09:16:46.781712: Yayy! New best EMA pseudo Dice: 0.8094
2024-11-08 09:16:48.206399: Epoch time: 299.7 s
2024-11-08 09:16:50.211053: 
2024-11-08 09:16:50.217388: Epoch 4
2024-11-08 09:16:50.218688: Current learning rate: 0.00928
2024-11-08 09:21:48.826201: train_loss -0.7794
2024-11-08 09:21:48.835953: val_loss -0.828
2024-11-08 09:21:48.838521: Pseudo dice [0.9135, 0.8931, 0.8871]
2024-11-08 09:21:48.840502: Yayy! New best EMA pseudo Dice: 0.8182
2024-11-08 09:21:49.694930: Epoch time: 298.62 s
2024-11-08 09:21:52.740368: 
2024-11-08 09:21:52.756194: Epoch 5
2024-11-08 09:21:52.757540: Current learning rate: 0.0091
2024-11-08 09:26:51.909397: train_loss -0.7701
2024-11-08 09:26:51.911642: val_loss -0.7851
2024-11-08 09:26:51.912517: Pseudo dice [0.908, 0.8854, 0.8502]
2024-11-08 09:26:51.913644: Yayy! New best EMA pseudo Dice: 0.8245
2024-11-08 09:26:52.679056: Epoch time: 299.17 s
2024-11-08 09:26:55.829739: 
2024-11-08 09:26:55.834410: Epoch 6
2024-11-08 09:26:55.841240: Current learning rate: 0.00891
2024-11-08 09:31:53.250869: train_loss -0.7951
2024-11-08 09:31:53.283989: val_loss -0.8076
2024-11-08 09:31:53.284881: Pseudo dice [0.9166, 0.8949, 0.8648]
2024-11-08 09:31:53.286097: Yayy! New best EMA pseudo Dice: 0.8313
2024-11-08 09:31:54.494456: Epoch time: 297.43 s
2024-11-08 09:31:58.487920: 
2024-11-08 09:31:58.489028: Epoch 7
2024-11-08 09:31:58.489813: Current learning rate: 0.00873
2024-11-08 09:36:49.270940: train_loss -0.7924
2024-11-08 09:36:49.279396: val_loss -0.83
2024-11-08 09:36:49.280149: Pseudo dice [0.9184, 0.8933, 0.8782]
2024-11-08 09:36:49.280951: Yayy! New best EMA pseudo Dice: 0.8378
2024-11-08 09:36:50.178473: Epoch time: 290.78 s
2024-11-08 09:36:52.569538: 
2024-11-08 09:36:52.571464: Epoch 8
2024-11-08 09:36:52.581391: Current learning rate: 0.00855
2024-11-08 09:42:00.454607: train_loss -0.8021
2024-11-08 09:42:00.466439: val_loss -0.8136
2024-11-08 09:42:00.467212: Pseudo dice [0.9138, 0.8908, 0.8867]
2024-11-08 09:42:00.468048: Yayy! New best EMA pseudo Dice: 0.8437
2024-11-08 09:42:01.361169: Epoch time: 307.89 s
2024-11-08 09:42:03.622303: 
2024-11-08 09:42:03.623632: Epoch 9
2024-11-08 09:42:03.624866: Current learning rate: 0.00836
2024-11-08 09:46:51.081910: train_loss -0.8231
2024-11-08 09:46:51.089837: val_loss -0.8437
2024-11-08 09:46:51.090675: Pseudo dice [0.9308, 0.9114, 0.8953]
2024-11-08 09:46:51.091553: Yayy! New best EMA pseudo Dice: 0.8506
2024-11-08 09:46:52.119083: Epoch time: 287.46 s
2024-11-08 09:46:55.849913: 
2024-11-08 09:46:55.863324: Epoch 10
2024-11-08 09:46:55.864543: Current learning rate: 0.00818
2024-11-08 09:52:04.537231: train_loss -0.8282
2024-11-08 09:52:04.549567: val_loss -0.8421
2024-11-08 09:52:04.557858: Pseudo dice [0.9266, 0.9144, 0.8962]
2024-11-08 09:52:04.558837: Yayy! New best EMA pseudo Dice: 0.8568
2024-11-08 09:52:05.398989: Epoch time: 308.69 s
2024-11-08 09:52:07.401077: 
2024-11-08 09:52:07.412986: Epoch 11
2024-11-08 09:52:07.414033: Current learning rate: 0.008
2024-11-08 09:57:07.735578: train_loss -0.8321
2024-11-08 09:57:07.747962: val_loss -0.841
2024-11-08 09:57:07.749001: Pseudo dice [0.9208, 0.9166, 0.8938]
2024-11-08 09:57:07.749876: Yayy! New best EMA pseudo Dice: 0.8622
2024-11-08 09:57:08.630277: Epoch time: 300.34 s
2024-11-08 09:57:11.579141: 
2024-11-08 09:57:11.580467: Epoch 12
2024-11-08 09:57:11.581377: Current learning rate: 0.00781
2024-11-08 10:02:08.936082: train_loss -0.832
2024-11-08 10:02:08.984533: val_loss -0.8644
2024-11-08 10:02:08.985576: Pseudo dice [0.9308, 0.9326, 0.9121]
2024-11-08 10:02:08.986621: Yayy! New best EMA pseudo Dice: 0.8685
2024-11-08 10:02:10.285298: Epoch time: 297.36 s
2024-11-08 10:02:15.918491: 
2024-11-08 10:02:15.931376: Epoch 13
2024-11-08 10:02:15.932287: Current learning rate: 0.00763
2024-11-08 10:07:22.240883: train_loss -0.8367
2024-11-08 10:07:22.429613: val_loss -0.8465
2024-11-08 10:07:22.430398: Pseudo dice [0.9284, 0.9347, 0.889]
2024-11-08 10:07:22.458091: Yayy! New best EMA pseudo Dice: 0.8733
2024-11-08 10:07:23.769481: Epoch time: 306.29 s
2024-11-08 10:07:27.123981: 
2024-11-08 10:07:27.124989: Epoch 14
2024-11-08 10:07:27.125650: Current learning rate: 0.00744
2024-11-08 10:12:21.404196: train_loss -0.8368
2024-11-08 10:12:21.405550: val_loss -0.8422
2024-11-08 10:12:21.406448: Pseudo dice [0.918, 0.9127, 0.8952]
2024-11-08 10:12:21.407270: Yayy! New best EMA pseudo Dice: 0.8769
2024-11-08 10:12:22.338135: Epoch time: 294.28 s
2024-11-08 10:12:31.040152: 
2024-11-08 10:12:31.041365: Epoch 15
2024-11-08 10:12:31.042204: Current learning rate: 0.00725
2024-11-08 10:17:26.458203: train_loss -0.841
2024-11-08 10:17:26.467546: val_loss -0.8345
2024-11-08 10:17:26.468469: Pseudo dice [0.9205, 0.9118, 0.8811]
2024-11-08 10:17:26.469291: Yayy! New best EMA pseudo Dice: 0.8796
2024-11-08 10:17:27.369251: Epoch time: 295.42 s
2024-11-08 10:17:30.113288: 
2024-11-08 10:17:30.114981: Epoch 16
2024-11-08 10:17:30.124599: Current learning rate: 0.00707
2024-11-08 10:22:26.399835: train_loss -0.8438
2024-11-08 10:22:26.423112: val_loss -0.8593
2024-11-08 10:22:26.423948: Pseudo dice [0.935, 0.9178, 0.8872]
2024-11-08 10:22:26.424844: Yayy! New best EMA pseudo Dice: 0.883
2024-11-08 10:22:27.025062: Epoch time: 296.29 s
2024-11-08 10:22:30.091976: 
2024-11-08 10:22:30.093808: Epoch 17
2024-11-08 10:22:30.094878: Current learning rate: 0.00688
2024-11-08 10:27:23.668859: train_loss -0.8454
2024-11-08 10:27:23.683568: val_loss -0.8433
2024-11-08 10:27:23.691358: Pseudo dice [0.9181, 0.8824, 0.8759]
2024-11-08 10:27:23.692323: Yayy! New best EMA pseudo Dice: 0.8839
2024-11-08 10:27:24.741609: Epoch time: 293.58 s
2024-11-08 10:27:27.538607: 
2024-11-08 10:27:27.552917: Epoch 18
2024-11-08 10:27:27.553812: Current learning rate: 0.00669
2024-11-08 10:32:23.917594: train_loss -0.8427
2024-11-08 10:32:23.931897: val_loss -0.8483
2024-11-08 10:32:23.932972: Pseudo dice [0.9158, 0.9162, 0.9002]
2024-11-08 10:32:23.934071: Yayy! New best EMA pseudo Dice: 0.8866
2024-11-08 10:32:25.316645: Epoch time: 296.38 s
2024-11-08 10:32:32.125393: 
2024-11-08 10:32:32.126515: Epoch 19
2024-11-08 10:32:32.127961: Current learning rate: 0.0065
2024-11-08 10:37:24.162510: train_loss -0.841
2024-11-08 10:37:24.172726: val_loss -0.8726
2024-11-08 10:37:24.173687: Pseudo dice [0.9379, 0.915, 0.9105]
2024-11-08 10:37:24.174624: Yayy! New best EMA pseudo Dice: 0.8901
2024-11-08 10:37:35.714927: Epoch time: 292.04 s
2024-11-08 10:37:37.575726: 
2024-11-08 10:37:37.576927: Epoch 20
2024-11-08 10:37:37.577655: Current learning rate: 0.00631
2024-11-08 10:42:23.194014: train_loss -0.8575
2024-11-08 10:42:23.212925: val_loss -0.8445
2024-11-08 10:42:23.213815: Pseudo dice [0.9131, 0.9099, 0.9039]
2024-11-08 10:42:23.214682: Yayy! New best EMA pseudo Dice: 0.8919
2024-11-08 10:42:24.172784: Epoch time: 285.62 s
using pin_memory on device 0
2024-11-08 09:02:47.765028: train_loss -0.607
2024-11-08 09:02:47.766508: val_loss -0.754
2024-11-08 09:02:47.767301: Pseudo dice [0.8841, 0.742, 0.825]
2024-11-08 09:02:47.768023: Yayy! New best EMA pseudo Dice: 0.817
2024-11-08 09:02:48.437012: Epoch time: 359.0 s
2024-11-08 09:02:49.635051: 
2024-11-08 09:02:49.636165: Epoch 1
2024-11-08 09:02:49.636813: Current learning rate: 0.00982
2024-11-08 09:08:14.000995: train_loss -0.7377
2024-11-08 09:08:14.241064: val_loss -0.7743
2024-11-08 09:08:14.242186: Pseudo dice [0.8938, 0.778, 0.8294]
2024-11-08 09:08:14.242903: Yayy! New best EMA pseudo Dice: 0.8187
2024-11-08 09:08:15.013769: Epoch time: 324.37 s
2024-11-08 09:08:16.608603: 
2024-11-08 09:08:16.609586: Epoch 2
2024-11-08 09:08:16.610212: Current learning rate: 0.00964
2024-11-08 09:13:43.668900: train_loss -0.7503
2024-11-08 09:13:43.670680: val_loss -0.8217
2024-11-08 09:13:43.671406: Pseudo dice [0.9061, 0.8627, 0.8726]
2024-11-08 09:13:43.672095: Yayy! New best EMA pseudo Dice: 0.8249
2024-11-08 09:13:44.580323: Epoch time: 327.06 s
2024-11-08 09:13:45.883988: 
2024-11-08 09:13:45.885019: Epoch 3
2024-11-08 09:13:45.885867: Current learning rate: 0.00946
2024-11-08 09:19:11.507535: train_loss -0.7787
2024-11-08 09:19:11.509158: val_loss -0.8061
2024-11-08 09:19:11.510725: Pseudo dice [0.9038, 0.8745, 0.8617]
2024-11-08 09:19:11.511476: Yayy! New best EMA pseudo Dice: 0.8304
2024-11-08 09:19:12.241901: Epoch time: 325.63 s
2024-11-08 09:19:13.759711: 
2024-11-08 09:19:13.760867: Epoch 4
2024-11-08 09:19:13.761575: Current learning rate: 0.00928
2024-11-08 09:24:41.311551: train_loss -0.8015
2024-11-08 09:24:41.312671: val_loss -0.786
2024-11-08 09:24:41.313417: Pseudo dice [0.9204, 0.7957, 0.8631]
2024-11-08 09:24:41.314121: Yayy! New best EMA pseudo Dice: 0.8333
2024-11-08 09:24:42.019578: Epoch time: 327.55 s
2024-11-08 09:24:43.907054: 
2024-11-08 09:24:43.908066: Epoch 5
2024-11-08 09:24:43.908703: Current learning rate: 0.0091
2024-11-08 09:30:10.303602: train_loss -0.7876
2024-11-08 09:30:10.304890: val_loss -0.7948
2024-11-08 09:30:10.305711: Pseudo dice [0.9089, 0.8645, 0.8754]
2024-11-08 09:30:10.306418: Yayy! New best EMA pseudo Dice: 0.8383
2024-11-08 09:30:11.055188: Epoch time: 326.4 s
2024-11-08 09:30:12.591329: 
2024-11-08 09:30:12.592399: Epoch 6
2024-11-08 09:30:12.593106: Current learning rate: 0.00891
2024-11-08 09:35:42.928962: train_loss -0.8098
2024-11-08 09:35:42.930271: val_loss -0.7956
2024-11-08 09:35:42.930946: Pseudo dice [0.905, 0.8687, 0.8698]
2024-11-08 09:35:42.931639: Yayy! New best EMA pseudo Dice: 0.8426
2024-11-08 09:35:43.629863: Epoch time: 330.34 s
2024-11-08 09:35:44.883037: 
2024-11-08 09:35:44.884074: Epoch 7
2024-11-08 09:35:44.884729: Current learning rate: 0.00873
2024-11-08 09:41:15.553290: train_loss -0.816
2024-11-08 09:41:15.557547: val_loss -0.8219
2024-11-08 09:41:15.558315: Pseudo dice [0.9095, 0.8875, 0.8768]
2024-11-08 09:41:15.559042: Yayy! New best EMA pseudo Dice: 0.8474
2024-11-08 09:41:16.312573: Epoch time: 330.67 s
2024-11-08 09:41:17.626162: 
2024-11-08 09:41:17.627741: Epoch 8
2024-11-08 09:41:17.628525: Current learning rate: 0.00855
2024-11-08 09:46:47.352071: train_loss -0.8169
2024-11-08 09:46:47.355854: val_loss -0.8357
2024-11-08 09:46:47.356615: Pseudo dice [0.9191, 0.8689, 0.8888]
2024-11-08 09:46:47.357630: Yayy! New best EMA pseudo Dice: 0.8519
2024-11-08 09:46:48.069478: Epoch time: 329.73 s
2024-11-08 09:46:49.445882: 
2024-11-08 09:46:49.446781: Epoch 9
2024-11-08 09:46:49.447464: Current learning rate: 0.00836
2024-11-08 09:52:17.969978: train_loss -0.8351
2024-11-08 09:52:17.972592: val_loss -0.8282
2024-11-08 09:52:17.973386: Pseudo dice [0.911, 0.899, 0.8776]
2024-11-08 09:52:17.974300: Yayy! New best EMA pseudo Dice: 0.8563
2024-11-08 09:52:18.720301: Epoch time: 328.53 s
2024-11-08 09:52:21.010458: 
2024-11-08 09:52:21.011408: Epoch 10
2024-11-08 09:52:21.012057: Current learning rate: 0.00818
2024-11-08 09:57:47.565962: train_loss -0.8335
2024-11-08 09:57:47.569363: val_loss -0.8557
2024-11-08 09:57:47.570110: Pseudo dice [0.9264, 0.9172, 0.8945]
2024-11-08 09:57:47.571108: Yayy! New best EMA pseudo Dice: 0.862
2024-11-08 09:57:48.273196: Epoch time: 326.56 s
2024-11-08 09:57:49.547039: 
2024-11-08 09:57:49.548051: Epoch 11
2024-11-08 09:57:49.548729: Current learning rate: 0.008
2024-11-08 10:03:20.898728: train_loss -0.8283
2024-11-08 10:03:21.347053: val_loss -0.823
2024-11-08 10:03:21.347925: Pseudo dice [0.9133, 0.8646, 0.8732]
2024-11-08 10:03:21.348944: Yayy! New best EMA pseudo Dice: 0.8641
2024-11-08 10:03:22.211512: Epoch time: 331.35 s
2024-11-08 10:03:25.572763: 
2024-11-08 10:03:25.573672: Epoch 12
2024-11-08 10:03:25.574339: Current learning rate: 0.00781
2024-11-08 10:09:01.188558: train_loss -0.8413
2024-11-08 10:09:01.191896: val_loss -0.8449
2024-11-08 10:09:01.192623: Pseudo dice [0.9152, 0.9128, 0.8922]
2024-11-08 10:09:01.193334: Yayy! New best EMA pseudo Dice: 0.8684
2024-11-08 10:09:02.043896: Epoch time: 335.62 s
2024-11-08 10:09:05.892295: 
2024-11-08 10:09:05.893255: Epoch 13
2024-11-08 10:09:05.893898: Current learning rate: 0.00763
2024-11-08 10:14:35.234087: train_loss -0.8413
2024-11-08 10:14:35.235472: val_loss -0.8321
2024-11-08 10:14:35.236210: Pseudo dice [0.9251, 0.88, 0.8805]
2024-11-08 10:14:35.236974: Yayy! New best EMA pseudo Dice: 0.8711
2024-11-08 10:14:38.903024: Epoch time: 329.34 s
2024-11-08 10:14:53.348571: 
2024-11-08 10:14:53.349886: Epoch 14
2024-11-08 10:14:53.350762: Current learning rate: 0.00744
2024-11-08 10:20:24.227813: train_loss -0.8421
2024-11-08 10:20:24.229972: val_loss -0.8446
2024-11-08 10:20:24.230747: Pseudo dice [0.9199, 0.9167, 0.8984]
2024-11-08 10:20:24.231553: Yayy! New best EMA pseudo Dice: 0.8751
2024-11-08 10:20:25.005555: Epoch time: 330.88 s
2024-11-08 10:20:27.385208: 
2024-11-08 10:20:27.386522: Epoch 15
2024-11-08 10:20:27.387362: Current learning rate: 0.00725
2024-11-08 10:25:54.884861: train_loss -0.8545
2024-11-08 10:25:54.887196: val_loss -0.8461
2024-11-08 10:25:54.887972: Pseudo dice [0.9294, 0.8842, 0.8932]
2024-11-08 10:25:54.888746: Yayy! New best EMA pseudo Dice: 0.8778
2024-11-08 10:25:55.676975: Epoch time: 327.5 s
2024-11-08 10:25:57.015988: 
2024-11-08 10:25:57.017329: Epoch 16
2024-11-08 10:25:57.018049: Current learning rate: 0.00707
2024-11-08 10:31:24.417449: train_loss -0.8494
2024-11-08 10:31:24.418655: val_loss -0.8636
2024-11-08 10:31:24.419326: Pseudo dice [0.9342, 0.9103, 0.9075]
2024-11-08 10:31:24.419968: Yayy! New best EMA pseudo Dice: 0.8818
2024-11-08 10:31:28.625203: Epoch time: 327.4 s
2024-11-08 10:31:30.455811: 
2024-11-08 10:31:30.456935: Epoch 17
2024-11-08 10:31:30.457672: Current learning rate: 0.00688
2024-11-08 10:36:58.232444: train_loss -0.8476
2024-11-08 10:36:58.233959: val_loss -0.866
2024-11-08 10:36:58.234828: Pseudo dice [0.9281, 0.922, 0.9116]
2024-11-08 10:36:58.235625: Yayy! New best EMA pseudo Dice: 0.8857
2024-11-08 10:37:07.549408: Epoch time: 327.78 s
2024-11-08 10:37:09.043673: 
2024-11-08 10:37:09.045380: Epoch 18
2024-11-08 10:37:09.046063: Current learning rate: 0.00669
2024-11-08 10:42:36.702204: train_loss -0.8536
2024-11-08 10:42:36.703478: val_loss -0.8751
2024-11-08 10:42:36.704206: Pseudo dice [0.9426, 0.9277, 0.9101]
2024-11-08 10:42:36.704904: Yayy! New best EMA pseudo Dice: 0.8898
2024-11-08 10:42:38.361552: Epoch time: 327.66 s
2024-11-08 10:42:45.032722: 
2024-11-08 10:42:45.033711: Epoch 19
2024-11-08 10:42:45.034371: Current learning rate: 0.0065
2024-11-08 10:48:14.743564: train_loss -0.8549
2024-11-08 10:48:14.795870: val_loss -0.8565
2024-11-08 10:48:14.796654: Pseudo dice [0.9397, 0.9135, 0.9056]
2024-11-08 10:48:14.797635: Yayy! New best EMA pseudo Dice: 0.8928
2024-11-08 10:48:15.512715: Epoch time: 329.71 s
2024-11-08 10:48:18.912017: 
2024-11-08 10:48:18.913007: Epoch 20
2024-11-08 10:48:18.913670: Current learning rate: 0.00631
2024-11-08 10:53:48.371598: train_loss -0.8628
2024-11-08 10:53:48.374655: val_loss -0.8592
2024-11-08 10:53:48.375416: Pseudo dice [0.9337, 0.9072, 0.894]
2024-11-08 10:53:48.376293: Yayy! New best EMA pseudo Dice: 0.8947
2024-11-08 10:42:27.554289: 
2024-11-08 10:42:27.556084: Epoch 21
2024-11-08 10:42:27.556962: Current learning rate: 0.00612
2024-11-08 10:47:27.763439: train_loss -0.8595
2024-11-08 10:47:27.772684: val_loss -0.8463
2024-11-08 10:47:27.778620: Pseudo dice [0.9355, 0.9091, 0.9011]
2024-11-08 10:47:27.779539: Yayy! New best EMA pseudo Dice: 0.8943
2024-11-08 10:47:28.912730: Epoch time: 300.21 s
2024-11-08 10:47:31.623136: 
2024-11-08 10:47:31.624291: Epoch 22
2024-11-08 10:47:31.625279: Current learning rate: 0.00593
2024-11-08 10:52:33.546246: train_loss -0.8645
2024-11-08 10:52:33.548787: val_loss -0.8542
2024-11-08 10:52:33.557141: Pseudo dice [0.9339, 0.9227, 0.8997]
2024-11-08 10:52:33.559897: Yayy! New best EMA pseudo Dice: 0.8967
2024-11-08 10:52:36.056015: Epoch time: 301.92 s
2024-11-08 10:52:38.741661: 
2024-11-08 10:52:38.755710: Epoch 23
2024-11-08 10:52:38.756619: Current learning rate: 0.00574
2024-11-08 10:57:35.574836: train_loss -0.8568
2024-11-08 10:57:35.585193: val_loss -0.8823
2024-11-08 10:57:35.586298: Pseudo dice [0.9378, 0.9329, 0.9121]
2024-11-08 10:57:35.587171: Yayy! New best EMA pseudo Dice: 0.8998
2024-11-08 10:57:36.322423: Epoch time: 296.83 s
2024-11-08 10:57:38.701664: 
2024-11-08 10:57:38.703071: Epoch 24
2024-11-08 10:57:38.704154: Current learning rate: 0.00555
2024-11-08 11:02:46.512409: train_loss -0.8419
2024-11-08 11:02:46.528404: val_loss -0.789
2024-11-08 11:02:46.529251: Pseudo dice [0.9068, 0.8743, 0.8769]
2024-11-08 11:02:46.530402: Epoch time: 307.81 s
2024-11-08 11:02:50.153930: 
2024-11-08 11:02:50.164509: Epoch 25
2024-11-08 11:02:50.165407: Current learning rate: 0.00536
2024-11-08 11:07:44.716002: train_loss -0.8481
2024-11-08 11:07:44.736575: val_loss -0.8332
2024-11-08 11:07:44.737471: Pseudo dice [0.9198, 0.8828, 0.8922]
2024-11-08 11:07:44.738780: Epoch time: 294.56 s
2024-11-08 11:07:47.932307: 
2024-11-08 11:07:47.945901: Epoch 26
2024-11-08 11:07:47.946812: Current learning rate: 0.00517
2024-11-08 11:12:50.472356: train_loss -0.8486
2024-11-08 11:12:50.488427: val_loss -0.8356
2024-11-08 11:12:50.496637: Pseudo dice [0.9304, 0.9172, 0.9036]
2024-11-08 11:12:50.497580: Yayy! New best EMA pseudo Dice: 0.9003
2024-11-08 11:12:53.428391: Epoch time: 302.54 s
2024-11-08 11:12:56.533258: 
2024-11-08 11:12:56.534450: Epoch 27
2024-11-08 11:12:56.535319: Current learning rate: 0.00497
2024-11-08 11:17:56.304254: train_loss -0.8531
2024-11-08 11:17:56.317011: val_loss -0.8661
2024-11-08 11:17:56.317792: Pseudo dice [0.9356, 0.9165, 0.9081]
2024-11-08 11:17:56.318583: Yayy! New best EMA pseudo Dice: 0.9023
2024-11-08 11:17:57.384880: Epoch time: 299.77 s
2024-11-08 11:18:00.071888: 
2024-11-08 11:18:00.073027: Epoch 28
2024-11-08 11:18:00.073882: Current learning rate: 0.00478
2024-11-08 11:22:45.683618: train_loss -0.864
2024-11-08 11:22:45.700484: val_loss -0.8259
2024-11-08 11:22:45.701606: Pseudo dice [0.9431, 0.855, 0.9126]
2024-11-08 11:22:45.702753: Yayy! New best EMA pseudo Dice: 0.9024
2024-11-08 11:22:46.492693: Epoch time: 285.61 s
2024-11-08 11:22:50.816891: 
2024-11-08 11:22:50.818348: Epoch 29
2024-11-08 11:22:50.819186: Current learning rate: 0.00458
2024-11-08 11:27:45.793790: train_loss -0.8615
2024-11-08 11:27:45.802501: val_loss -0.8828
2024-11-08 11:27:45.819552: Pseudo dice [0.9403, 0.9402, 0.9088]
2024-11-08 11:27:45.820639: Yayy! New best EMA pseudo Dice: 0.9051
2024-11-08 11:27:46.494302: Epoch time: 294.98 s
2024-11-08 11:27:50.096119: 
2024-11-08 11:27:50.097280: Epoch 30
2024-11-08 11:27:50.098101: Current learning rate: 0.00438
2024-11-08 11:32:53.727156: train_loss -0.8715
2024-11-08 11:32:53.736930: val_loss -0.8553
2024-11-08 11:32:53.738064: Pseudo dice [0.9433, 0.9352, 0.92]
2024-11-08 11:32:53.739232: Yayy! New best EMA pseudo Dice: 0.9079
2024-11-08 11:32:55.246026: Epoch time: 303.63 s
2024-11-08 11:32:58.050262: 
2024-11-08 11:32:58.051476: Epoch 31
2024-11-08 11:32:58.052472: Current learning rate: 0.00419
2024-11-08 11:37:53.477567: train_loss -0.8637
2024-11-08 11:37:53.496407: val_loss -0.8649
2024-11-08 11:37:53.497665: Pseudo dice [0.9431, 0.9364, 0.909]
2024-11-08 11:37:53.498579: Yayy! New best EMA pseudo Dice: 0.9101
2024-11-08 11:37:56.598645: Epoch time: 295.43 s
2024-11-08 11:38:01.203067: 
2024-11-08 11:38:01.204285: Epoch 32
2024-11-08 11:38:01.205024: Current learning rate: 0.00399
2024-11-08 11:42:57.911234: train_loss -0.8557
2024-11-08 11:42:57.913014: val_loss -0.863
2024-11-08 11:42:57.913929: Pseudo dice [0.9372, 0.9386, 0.9121]
2024-11-08 11:42:57.914886: Yayy! New best EMA pseudo Dice: 0.912
2024-11-08 11:42:59.985396: Epoch time: 296.71 s
2024-11-08 11:43:03.043902: 
2024-11-08 11:43:03.052975: Epoch 33
2024-11-08 11:43:03.053949: Current learning rate: 0.00379
2024-11-08 11:47:59.224044: train_loss -0.8679
2024-11-08 11:47:59.238296: val_loss -0.8828
2024-11-08 11:47:59.239695: Pseudo dice [0.9375, 0.939, 0.911]
2024-11-08 11:47:59.249161: Yayy! New best EMA pseudo Dice: 0.9137
2024-11-08 11:48:00.045844: Epoch time: 296.18 s
2024-11-08 11:48:02.639016: 
2024-11-08 11:48:02.640017: Epoch 34
2024-11-08 11:48:02.640897: Current learning rate: 0.00359
2024-11-08 11:53:07.403380: train_loss -0.8636
2024-11-08 11:53:07.426303: val_loss -0.8997
2024-11-08 11:53:07.427194: Pseudo dice [0.941, 0.9457, 0.9172]
2024-11-08 11:53:07.428084: Yayy! New best EMA pseudo Dice: 0.9158
2024-11-08 11:53:08.734490: Epoch time: 304.77 s
2024-11-08 11:53:11.666972: 
2024-11-08 11:53:11.668458: Epoch 35
2024-11-08 11:53:11.669336: Current learning rate: 0.00338
2024-11-08 11:58:06.016532: train_loss -0.867
2024-11-08 11:58:06.031865: val_loss -0.8856
2024-11-08 11:58:06.033105: Pseudo dice [0.9447, 0.9265, 0.9144]
2024-11-08 11:58:06.034282: Yayy! New best EMA pseudo Dice: 0.9171
2024-11-08 11:58:07.242918: Epoch time: 294.35 s
2024-11-08 11:58:09.584076: 
2024-11-08 11:58:09.585166: Epoch 36
2024-11-08 11:58:09.586075: Current learning rate: 0.00318
2024-11-08 12:03:11.750762: train_loss -0.8595
2024-11-08 12:03:11.766429: val_loss -0.8959
2024-11-08 12:03:11.775666: Pseudo dice [0.9449, 0.9225, 0.9232]
2024-11-08 12:03:11.776689: Yayy! New best EMA pseudo Dice: 0.9184
2024-11-08 12:03:12.605227: Epoch time: 302.17 s
2024-11-08 12:03:18.337637: 
2024-11-08 12:03:18.339063: Epoch 37
2024-11-08 12:03:18.339831: Current learning rate: 0.00297
2024-11-08 12:08:15.391346: train_loss -0.8726
2024-11-08 12:08:15.419825: val_loss -0.8386
2024-11-08 12:08:15.420784: Pseudo dice [0.938, 0.8996, 0.904]
2024-11-08 12:08:15.422821: Epoch time: 297.05 s
2024-11-08 12:08:20.211413: 
2024-11-08 12:08:20.217149: Epoch 38
2024-11-08 12:08:20.227618: Current learning rate: 0.00277
2024-11-08 12:13:14.026397: train_loss -0.8715
2024-11-08 12:13:14.039088: val_loss -0.8753
2024-11-08 12:13:14.039879: Pseudo dice [0.933, 0.9257, 0.9165]
2024-11-08 12:13:14.040645: Yayy! New best EMA pseudo Dice: 0.9186
2024-11-08 12:13:14.929173: Epoch time: 293.82 s
2024-11-08 12:13:18.098658: 
2024-11-08 12:13:18.099753: Epoch 39
2024-11-08 12:13:18.100654: Current learning rate: 0.00256
2024-11-08 12:18:05.372933: train_loss -0.8719
2024-11-08 12:18:05.386703: val_loss -0.8763
2024-11-08 12:18:05.387791: Pseudo dice [0.9421, 0.926, 0.9141]
2024-11-08 12:18:05.388834: Yayy! New best EMA pseudo Dice: 0.9195
2024-11-08 12:18:06.093104: Epoch time: 287.28 s
2024-11-08 12:18:09.844543: 
2024-11-08 12:18:09.845990: Epoch 40
2024-11-08 12:18:09.846838: Current learning rate: 0.00235
2024-11-08 12:23:10.916418: train_loss -0.8756
2024-11-08 12:23:10.925547: val_loss -0.8805
2024-11-08 12:23:10.931519: Pseudo dice [0.9437, 0.9462, 0.9225]
2024-11-08 12:23:10.932428: Yayy! New best EMA pseudo Dice: 0.9213
2024-11-08 12:23:11.779160: Epoch time: 301.07 s
2024-11-08 12:23:14.708881: 
2024-11-08 12:23:14.721678: Epoch 41
2024-11-08 12:23:14.722642: Current learning rate: 0.00214
2024-11-08 12:28:13.820450: train_loss -0.8822
2024-11-08 12:28:13.836352: val_loss -0.881
2024-11-08 12:28:13.844957: Pseudo dice [0.949, 0.906, 0.9157]
2024-11-08 12:28:13.846122: Yayy! New best EMA pseudo Dice: 0.9215
2024-11-08 12:28:14.649266: Epoch time: 299.11 s
2024-11-08 12:28:17.604156: 
2024-11-08 10:53:49.146387: Epoch time: 329.46 s
2024-11-08 10:53:50.542887: 
2024-11-08 10:53:50.543845: Epoch 21
2024-11-08 10:53:50.544555: Current learning rate: 0.00612
2024-11-08 10:59:21.638979: train_loss -0.8504
2024-11-08 10:59:21.640465: val_loss -0.8898
2024-11-08 10:59:21.641221: Pseudo dice [0.9352, 0.9272, 0.9156]
2024-11-08 10:59:21.641920: Yayy! New best EMA pseudo Dice: 0.8978
2024-11-08 10:59:22.995577: Epoch time: 331.1 s
2024-11-08 10:59:24.210529: 
2024-11-08 10:59:24.211466: Epoch 22
2024-11-08 10:59:24.212161: Current learning rate: 0.00593
2024-11-08 11:04:50.617445: train_loss -0.8644
2024-11-08 11:04:50.620696: val_loss -0.8536
2024-11-08 11:04:50.621439: Pseudo dice [0.9351, 0.9048, 0.8934]
2024-11-08 11:04:50.622219: Yayy! New best EMA pseudo Dice: 0.8991
2024-11-08 11:04:51.401353: Epoch time: 326.41 s
2024-11-08 11:04:53.823224: 
2024-11-08 11:04:53.824105: Epoch 23
2024-11-08 11:04:53.824784: Current learning rate: 0.00574
2024-11-08 11:10:26.156191: train_loss -0.8598
2024-11-08 11:10:26.197054: val_loss -0.8614
2024-11-08 11:10:26.197908: Pseudo dice [0.932, 0.9328, 0.9013]
2024-11-08 11:10:26.199423: Yayy! New best EMA pseudo Dice: 0.9014
2024-11-08 11:10:28.916170: Epoch time: 332.3 s
2024-11-08 11:10:30.986123: 
2024-11-08 11:10:30.987638: Epoch 24
2024-11-08 11:10:30.988402: Current learning rate: 0.00555
2024-11-08 11:16:00.415151: train_loss -0.8746
2024-11-08 11:16:00.416458: val_loss -0.8902
2024-11-08 11:16:00.417186: Pseudo dice [0.9443, 0.9245, 0.912]
2024-11-08 11:16:00.417879: Yayy! New best EMA pseudo Dice: 0.904
2024-11-08 11:16:01.304767: Epoch time: 329.43 s
2024-11-08 11:16:03.810282: 
2024-11-08 11:16:03.811631: Epoch 25
2024-11-08 11:16:03.812325: Current learning rate: 0.00536
2024-11-08 11:21:30.938705: train_loss -0.8705
2024-11-08 11:21:30.941317: val_loss -0.866
2024-11-08 11:21:30.942060: Pseudo dice [0.932, 0.9233, 0.9034]
2024-11-08 11:21:30.942938: Yayy! New best EMA pseudo Dice: 0.9055
2024-11-08 11:21:34.541500: Epoch time: 327.13 s
2024-11-08 11:21:36.164343: 
2024-11-08 11:21:36.165314: Epoch 26
2024-11-08 11:21:36.165982: Current learning rate: 0.00517
2024-11-08 11:27:03.996584: train_loss -0.8631
2024-11-08 11:27:03.998763: val_loss -0.8832
2024-11-08 11:27:03.999546: Pseudo dice [0.9375, 0.8998, 0.9109]
2024-11-08 11:27:04.000272: Yayy! New best EMA pseudo Dice: 0.9066
2024-11-08 11:27:04.746671: Epoch time: 327.83 s
2024-11-08 11:27:06.013480: 
2024-11-08 11:27:06.014660: Epoch 27
2024-11-08 11:27:06.015354: Current learning rate: 0.00497
2024-11-08 11:32:34.854822: train_loss -0.8598
2024-11-08 11:32:34.856076: val_loss -0.8623
2024-11-08 11:32:34.856837: Pseudo dice [0.9391, 0.9312, 0.9111]
2024-11-08 11:32:34.857577: Yayy! New best EMA pseudo Dice: 0.9086
2024-11-08 11:32:36.743983: Epoch time: 328.84 s
2024-11-08 11:32:38.227418: 
2024-11-08 11:32:38.228454: Epoch 28
2024-11-08 11:32:38.229074: Current learning rate: 0.00478
2024-11-08 11:38:06.678583: train_loss -0.8642
2024-11-08 11:38:06.679734: val_loss -0.8604
2024-11-08 11:38:06.680484: Pseudo dice [0.9319, 0.9263, 0.9046]
2024-11-08 11:38:06.681189: Yayy! New best EMA pseudo Dice: 0.9099
2024-11-08 11:38:07.866907: Epoch time: 328.45 s
2024-11-08 11:38:09.123182: 
2024-11-08 11:38:09.124389: Epoch 29
2024-11-08 11:38:09.125065: Current learning rate: 0.00458
2024-11-08 11:43:37.985463: train_loss -0.8727
2024-11-08 11:43:38.196900: val_loss -0.8702
2024-11-08 11:43:38.197946: Pseudo dice [0.9333, 0.9138, 0.8983]
2024-11-08 11:43:38.198679: Yayy! New best EMA pseudo Dice: 0.9104
2024-11-08 11:43:38.998862: Epoch time: 328.86 s
2024-11-08 11:43:41.545094: 
2024-11-08 11:43:41.546013: Epoch 30
2024-11-08 11:43:41.546687: Current learning rate: 0.00438
2024-11-08 11:49:11.833086: train_loss -0.8602
2024-11-08 11:49:11.834344: val_loss -0.8687
2024-11-08 11:49:11.835117: Pseudo dice [0.9321, 0.9274, 0.9056]
2024-11-08 11:49:11.835818: Yayy! New best EMA pseudo Dice: 0.9115
2024-11-08 11:49:12.624446: Epoch time: 330.29 s
2024-11-08 11:49:15.620394: 
2024-11-08 11:49:15.621271: Epoch 31
2024-11-08 11:49:15.621916: Current learning rate: 0.00419
2024-11-08 11:54:44.720830: train_loss -0.8681
2024-11-08 11:54:44.722960: val_loss -0.857
2024-11-08 11:54:44.723712: Pseudo dice [0.9333, 0.91, 0.8963]
2024-11-08 11:54:44.724566: Yayy! New best EMA pseudo Dice: 0.9117
2024-11-08 11:54:45.679086: Epoch time: 329.1 s
2024-11-08 11:54:47.688939: 
2024-11-08 11:54:47.689857: Epoch 32
2024-11-08 11:54:47.690506: Current learning rate: 0.00399
2024-11-08 12:00:15.580940: train_loss -0.8708
2024-11-08 12:00:15.582525: val_loss -0.8631
2024-11-08 12:00:15.583313: Pseudo dice [0.9195, 0.927, 0.9189]
2024-11-08 12:00:15.584027: Yayy! New best EMA pseudo Dice: 0.9127
2024-11-08 12:00:16.353097: Epoch time: 327.89 s
2024-11-08 12:00:17.946515: 
2024-11-08 12:00:17.947437: Epoch 33
2024-11-08 12:00:17.948116: Current learning rate: 0.00379
2024-11-08 12:05:47.646657: train_loss -0.8721
2024-11-08 12:05:47.647913: val_loss -0.87
2024-11-08 12:05:47.648660: Pseudo dice [0.9321, 0.9168, 0.9167]
2024-11-08 12:05:47.649367: Yayy! New best EMA pseudo Dice: 0.9136
2024-11-08 12:05:48.653643: Epoch time: 329.7 s
2024-11-08 12:05:51.610484: 
2024-11-08 12:05:51.611380: Epoch 34
2024-11-08 12:05:51.612014: Current learning rate: 0.00359
2024-11-08 12:11:19.900044: train_loss -0.876
2024-11-08 12:11:19.903139: val_loss -0.8769
2024-11-08 12:11:19.903864: Pseudo dice [0.942, 0.924, 0.9008]
2024-11-08 12:11:19.904652: Yayy! New best EMA pseudo Dice: 0.9145
2024-11-08 12:11:21.075131: Epoch time: 328.29 s
2024-11-08 12:11:23.098901: 
2024-11-08 12:11:23.099793: Epoch 35
2024-11-08 12:11:23.100419: Current learning rate: 0.00338
2024-11-08 12:16:54.649751: train_loss -0.8777
2024-11-08 12:16:54.651519: val_loss -0.8681
2024-11-08 12:16:54.652241: Pseudo dice [0.9455, 0.9246, 0.9223]
2024-11-08 12:16:54.653224: Yayy! New best EMA pseudo Dice: 0.9161
2024-11-08 12:16:56.344397: Epoch time: 331.55 s
2024-11-08 12:16:58.360534: 
2024-11-08 12:16:58.361491: Epoch 36
2024-11-08 12:16:58.362181: Current learning rate: 0.00318
2024-11-08 12:22:30.528362: train_loss -0.8748
2024-11-08 12:22:30.530248: val_loss -0.8805
2024-11-08 12:22:30.530958: Pseudo dice [0.9389, 0.9321, 0.9053]
2024-11-08 12:22:30.531597: Yayy! New best EMA pseudo Dice: 0.917
2024-11-08 12:22:31.286922: Epoch time: 332.17 s
2024-11-08 12:22:34.769879: 
2024-11-08 12:22:34.770863: Epoch 37
2024-11-08 12:22:34.771536: Current learning rate: 0.00297
2024-11-08 12:28:03.736924: train_loss -0.8693
2024-11-08 12:28:03.739127: val_loss -0.8725
2024-11-08 12:28:03.739788: Pseudo dice [0.9415, 0.9179, 0.9147]
2024-11-08 12:28:03.740426: Yayy! New best EMA pseudo Dice: 0.9178
2024-11-08 12:28:06.044557: Epoch time: 328.97 s
2024-11-08 12:28:08.097203: 
2024-11-08 12:28:08.098159: Epoch 38
2024-11-08 12:28:08.098797: Current learning rate: 0.00277
2024-11-08 12:33:35.326024: train_loss -0.8705
2024-11-08 12:33:35.327207: val_loss -0.8629
2024-11-08 12:33:35.328005: Pseudo dice [0.9363, 0.9059, 0.9042]
2024-11-08 12:33:35.329312: Epoch time: 327.23 s
2024-11-08 12:33:36.684654: 
2024-11-08 12:33:36.686264: Epoch 39
2024-11-08 12:33:36.687063: Current learning rate: 0.00256
2024-11-08 12:39:05.188977: train_loss -0.8776
2024-11-08 12:39:05.191979: val_loss -0.8655
2024-11-08 12:39:05.192702: Pseudo dice [0.9318, 0.9213, 0.9021]
2024-11-08 12:39:05.194125: Epoch time: 328.51 s
2024-11-08 12:39:11.481905: 
2024-11-08 12:39:11.483361: Epoch 40
2024-11-08 12:39:11.484028: Current learning rate: 0.00235
2024-11-08 12:44:39.565241: train_loss -0.8769
2024-11-08 12:44:39.566428: val_loss -0.8759
2024-11-08 12:44:39.567169: Pseudo dice [0.9384, 0.935, 0.9228]
2024-11-08 12:44:39.567923: Yayy! New best EMA pseudo Dice: 0.9191
2024-11-08 12:44:40.322793: Epoch time: 328.09 s
2024-11-08 12:44:43.347629: 
2024-11-08 12:44:43.348550: Epoch 41
2024-11-08 12:44:43.349219: Current learning rate: 0.00214
2024-11-08 12:50:11.834022: train_loss -0.8817
2024-11-08 12:50:11.835801: val_loss -0.9017
2024-11-08 12:50:11.836496: Pseudo dice [0.9413, 0.9325, 0.9236]
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         Average Dice ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá
wandb: Best EMA pseudo Dice ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: enhancing tumor Dice ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   lr ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:        training_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: training_loss_per_it ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÉ
wandb:      tumor core Dice ‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá
wandb:             val_loss ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:     whole tumor Dice ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá
wandb: 
wandb: Run summary:
wandb:         Average Dice 0.9245
wandb: Best EMA pseudo Dice 0.9263
wandb: enhancing tumor Dice 0.9218
wandb:                epoch 49
wandb:                   lr 0.0003
wandb:        training_loss -0.88003
wandb: training_loss_per_it -0.94135
wandb:      tumor core Dice 0.9157
wandb:             val_loss -0.88777
wandb:     whole tumor Dice 0.936
wandb: 
wandb: üöÄ View run BraTs_2023_UMamba_Bot_3d_fullres_fold_0_2024.11.08_08:55:36 at: https://wandb.ai/arild-ntnu/BraTs_2023_UMamba_Bot/runs/gnmb4vjd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/arild-ntnu/BraTs_2023_UMamba_Bot
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241108_085537-gnmb4vjd/logs
2024-11-08 12:28:17.605337: Epoch 42
2024-11-08 12:28:17.606460: Current learning rate: 0.00192
2024-11-08 12:33:01.276715: train_loss -0.8735
2024-11-08 12:33:01.299975: val_loss -0.8588
2024-11-08 12:33:01.300986: Pseudo dice [0.9332, 0.9292, 0.913]
2024-11-08 12:33:01.302015: Yayy! New best EMA pseudo Dice: 0.9219
2024-11-08 12:33:01.998204: Epoch time: 283.67 s
2024-11-08 12:33:03.759533: 
2024-11-08 12:33:03.772738: Epoch 43
2024-11-08 12:33:03.773622: Current learning rate: 0.0017
2024-11-08 12:38:06.834585: train_loss -0.8793
2024-11-08 12:38:06.850615: val_loss -0.8642
2024-11-08 12:38:06.858743: Pseudo dice [0.9348, 0.9387, 0.9224]
2024-11-08 12:38:06.860088: Yayy! New best EMA pseudo Dice: 0.9229
2024-11-08 12:38:07.719201: Epoch time: 303.08 s
2024-11-08 12:38:10.494438: 
2024-11-08 12:38:10.495625: Epoch 44
2024-11-08 12:38:10.497170: Current learning rate: 0.00148
2024-11-08 12:43:17.614700: train_loss -0.879
2024-11-08 12:43:17.639392: val_loss -0.8966
2024-11-08 12:43:17.640306: Pseudo dice [0.9405, 0.9414, 0.9245]
2024-11-08 12:43:17.641218: Yayy! New best EMA pseudo Dice: 0.9242
2024-11-08 12:43:18.870493: Epoch time: 307.12 s
2024-11-08 12:43:21.786677: 
2024-11-08 12:43:21.799953: Epoch 45
2024-11-08 12:43:21.800867: Current learning rate: 0.00126
2024-11-08 12:48:21.582437: train_loss -0.8869
2024-11-08 12:48:21.588712: val_loss -0.8783
2024-11-08 12:48:21.598166: Pseudo dice [0.9494, 0.9357, 0.9245]
2024-11-08 12:48:21.599881: Yayy! New best EMA pseudo Dice: 0.9254
2024-11-08 12:48:22.508449: Epoch time: 299.8 s
2024-11-08 12:48:24.789834: 
2024-11-08 12:48:24.791378: Epoch 46
2024-11-08 12:48:24.792243: Current learning rate: 0.00103
2024-11-08 12:53:12.896753: train_loss -0.8761
2024-11-08 12:53:12.907528: val_loss -0.8779
2024-11-08 12:53:12.908432: Pseudo dice [0.9482, 0.9221, 0.9219]
2024-11-08 12:53:12.909448: Yayy! New best EMA pseudo Dice: 0.9259
2024-11-08 12:53:13.732534: Epoch time: 288.11 s
2024-11-08 12:53:15.986247: 
2024-11-08 12:53:15.998897: Epoch 47
2024-11-08 12:53:15.999945: Current learning rate: 0.00079
2024-11-08 12:58:11.778622: train_loss -0.8829
2024-11-08 12:58:11.792011: val_loss -0.8579
2024-11-08 12:58:11.792869: Pseudo dice [0.9452, 0.9237, 0.9205]
2024-11-08 12:58:11.793649: Yayy! New best EMA pseudo Dice: 0.9263
2024-11-08 12:58:13.193070: Epoch time: 295.79 s
2024-11-08 12:58:17.214166: 
2024-11-08 12:58:17.216020: Epoch 48
2024-11-08 12:58:17.217114: Current learning rate: 0.00055
2024-11-08 13:03:21.826328: train_loss -0.8815
2024-11-08 13:03:21.836527: val_loss -0.8775
2024-11-08 13:03:21.837441: Pseudo dice [0.9356, 0.9147, 0.9193]
2024-11-08 13:03:21.838996: Epoch time: 304.61 s
2024-11-08 13:03:24.953769: 
2024-11-08 13:03:24.955730: Epoch 49
2024-11-08 13:03:24.956654: Current learning rate: 0.0003
2024-11-08 13:08:34.009321: train_loss -0.88
2024-11-08 13:08:34.038441: val_loss -0.8878
2024-11-08 13:08:34.039645: Pseudo dice [0.936, 0.9157, 0.9218]
2024-11-08 13:08:34.041606: Epoch time: 309.06 s
2024-11-08 13:08:39.774803: Training done.
2024-11-08 13:08:42.859146: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTS2021/splits_final.json
2024-11-08 13:08:42.875284: The split file contains 5 splits.
2024-11-08 13:08:42.875773: Desired fold for training: 0
2024-11-08 13:08:42.876287: This split has 1000 training and 251 validation cases.
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:13<02:04, 13.87s/it] 20%|‚ñà‚ñà        | 2/10 [00:14<00:47,  5.99s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:14<00:24,  3.44s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:15<00:13,  2.25s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:15<00:07,  1.59s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:16<00:04,  1.19s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:16<00:02,  1.07it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:16<00:01,  1.30it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:17<00:00,  1.52it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.71it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.77s/it]
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::leaky_relu_ encountered 22 time(s)
WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
decoder.seg_layers.0, decoder.seg_layers.1, decoder.seg_layers.2, decoder.seg_layers.3
Total number of FLOPs: 484520566784
Total number of parameters in the model: 31198991
Total number of trainable parameters in the model: 31198991
Mean Latency: 1766.0678915679455
Mean FPS: 2.1461000123907428
2024-11-08 13:09:14.449274: predicting BraTS2021_00000
2024-11-08 13:09:15.441605: predicting BraTS2021_00009
2024-11-08 13:09:18.117332: predicting BraTS2021_00016
2024-11-08 13:09:19.015411: predicting BraTS2021_00024
2024-11-08 13:09:19.916106: predicting BraTS2021_00028
2024-11-08 13:09:21.070223: predicting BraTS2021_00031
2024-11-08 13:09:22.153829: predicting BraTS2021_00035
2024-11-08 13:09:23.039171: predicting BraTS2021_00045
2024-11-08 13:09:23.932066: predicting BraTS2021_00046
2024-11-08 13:09:24.815535: predicting BraTS2021_00051
2024-11-08 13:12:45.585121: predicting BraTS2021_00070
2024-11-08 13:12:46.999930: predicting BraTS2021_00078
2024-11-08 13:12:47.897991: predicting BraTS2021_00085
2024-11-08 13:12:48.797940: predicting BraTS2021_00087
2024-11-08 13:12:49.694819: predicting BraTS2021_00088
2024-11-08 13:12:50.583998: predicting BraTS2021_00089
2024-11-08 13:12:51.472074: predicting BraTS2021_00099
2024-11-08 13:12:52.359709: predicting BraTS2021_00102
2024-11-08 13:12:53.258391: predicting BraTS2021_00104
2024-11-08 13:12:54.158171: predicting BraTS2021_00106
2024-11-08 13:12:55.064600: predicting BraTS2021_00107
2024-11-08 13:12:55.952847: predicting BraTS2021_00110
2024-11-08 13:12:56.839710: predicting BraTS2021_00117
2024-11-08 13:12:57.729358: predicting BraTS2021_00123
2024-11-08 13:12:58.627224: predicting BraTS2021_00126
2024-11-08 13:12:59.516805: predicting BraTS2021_00127
2024-11-08 13:13:00.415221: predicting BraTS2021_00130
2024-11-08 13:13:01.314673: predicting BraTS2021_00136
2024-11-08 13:13:02.212127: predicting BraTS2021_00137
2024-11-08 13:13:03.100025: predicting BraTS2021_00140
2024-11-08 13:13:03.989277: predicting BraTS2021_00144
2024-11-08 13:13:04.876496: predicting BraTS2021_00152
2024-11-08 13:13:05.763947: predicting BraTS2021_00160
2024-11-08 13:13:06.658314: predicting BraTS2021_00185
2024-11-08 13:13:07.549495: predicting BraTS2021_00192
2024-11-08 13:13:08.444408: predicting BraTS2021_00194
2024-11-08 13:13:08.928232: predicting BraTS2021_00211
2024-11-08 13:13:09.827498: predicting BraTS2021_00212
2024-11-08 13:13:10.727169: predicting BraTS2021_00214
2024-11-08 13:13:11.625646: predicting BraTS2021_00217
2024-11-08 13:13:12.521553: predicting BraTS2021_00218
2024-11-08 13:13:13.425530: predicting BraTS2021_00231
2024-11-08 13:13:14.319332: predicting BraTS2021_00243
2024-11-08 13:13:15.251482: predicting BraTS2021_00251
2024-11-08 13:13:16.183777: predicting BraTS2021_00266
2024-11-08 13:13:17.079511: predicting BraTS2021_00270
2024-11-08 13:13:17.965132: predicting BraTS2021_00274
2024-11-08 13:13:18.859981: predicting BraTS2021_00283
2024-11-08 13:13:19.753128: predicting BraTS2021_00286
2024-11-08 13:13:20.662022: predicting BraTS2021_00292
2024-11-08 13:13:21.561326: predicting BraTS2021_00294
2024-11-08 13:13:22.970824: predicting BraTS2021_00304
2024-11-08 13:13:23.868634: predicting BraTS2021_00311
2024-11-08 13:13:24.768275: predicting BraTS2021_00313
2024-11-08 13:13:25.667896: predicting BraTS2021_00334
2024-11-08 13:13:26.557272: predicting BraTS2021_00336
2024-11-08 13:13:27.453134: predicting BraTS2021_00350
2024-11-08 13:13:28.345960: predicting BraTS2021_00353
2024-11-08 13:13:29.245195: predicting BraTS2021_00366
2024-11-08 13:13:30.135909: predicting BraTS2021_00375
2024-11-08 13:13:31.028391: predicting BraTS2021_00376
2024-11-08 13:13:31.930718: predicting BraTS2021_00379
2024-11-08 13:13:32.823032: predicting BraTS2021_00391
2024-11-08 13:13:33.720769: predicting BraTS2021_00399
2024-11-08 13:13:34.613624: predicting BraTS2021_00402
2024-11-08 13:13:35.097711: predicting BraTS2021_00419
2024-11-08 13:13:35.990991: predicting BraTS2021_00423
2024-11-08 13:13:36.888274: predicting BraTS2021_00426
2024-11-08 13:13:37.779269: predicting BraTS2021_00430
2024-11-08 13:13:38.670574: predicting BraTS2021_00433
2024-11-08 13:13:39.570217: predicting BraTS2021_00436
2024-11-08 13:13:40.461970: predicting BraTS2021_00442
2024-11-08 13:13:41.355111: predicting BraTS2021_00464
2024-11-08 13:13:42.258604: predicting BraTS2021_00480
2024-11-08 13:13:43.157288: predicting BraTS2021_00485
2024-11-08 13:13:44.044262: predicting BraTS2021_00495
2024-11-08 13:13:44.941420: predicting BraTS2021_00500
2024-11-08 13:13:46.094024: predicting BraTS2021_00514
2024-11-08 13:13:46.990850: predicting BraTS2021_00516
2024-11-08 13:13:47.886192: predicting BraTS2021_00549
2024-11-08 13:13:48.781389: predicting BraTS2021_00558
2024-11-08 13:13:49.681871: predicting BraTS2021_00583
2024-11-08 13:13:50.577131: predicting BraTS2021_00586
2024-11-08 13:13:51.473720: predicting BraTS2021_00587
2024-11-08 13:13:52.370972: predicting BraTS2021_00597
2024-11-08 13:13:53.272803: predicting BraTS2021_00602
2024-11-08 13:13:54.167919: predicting BraTS2021_00605
2024-11-08 13:13:55.091385: predicting BraTS2021_00607
2024-11-08 13:13:55.984770: predicting BraTS2021_00630
2024-11-08 13:13:56.887285: predicting BraTS2021_00639
2024-11-08 13:13:57.780557: predicting BraTS2021_00641
2024-11-08 13:13:58.678503: predicting BraTS2021_00654
2024-11-08 13:13:59.573792: predicting BraTS2021_00693
2024-11-08 13:14:00.465245: predicting BraTS2021_00709
2024-11-08 13:14:01.359715: predicting BraTS2021_00715
2024-11-08 13:14:02.257335: predicting BraTS2021_00716
2024-11-08 13:14:03.149698: predicting BraTS2021_00727
2024-11-08 13:14:04.661611: predicting BraTS2021_00733
2024-11-08 13:14:05.560289: predicting BraTS2021_00735
2024-11-08 13:14:06.459346: predicting BraTS2021_00753
2024-11-08 13:14:07.346218: predicting BraTS2021_00757
2024-11-08 13:14:08.234985: predicting BraTS2021_00759
2024-11-08 13:14:09.176572: predicting BraTS2021_00772
2024-11-08 13:14:10.096551: predicting BraTS2021_00775
2024-11-08 13:14:10.988233: predicting BraTS2021_00777
2024-11-08 13:14:11.887298: predicting BraTS2021_00782
2024-11-08 13:14:12.791120: predicting BraTS2021_00791
2024-11-08 13:14:13.686219: predicting BraTS2021_00793
2024-11-08 13:14:14.581060: predicting BraTS2021_00810
2024-11-08 13:14:15.473713: predicting BraTS2021_00818
2024-11-08 13:14:16.371388: predicting BraTS2021_00834
2024-11-08 13:14:17.266293: predicting BraTS2021_00836
2024-11-08 13:14:18.166846: predicting BraTS2021_01001
2024-11-08 13:14:18.648441: predicting BraTS2021_01004
2024-11-08 13:14:19.541174: predicting BraTS2021_01005
2024-11-08 13:14:20.433595: predicting BraTS2021_01018
2024-11-08 13:14:21.344500: predicting BraTS2021_01037
2024-11-08 13:14:22.247699: predicting BraTS2021_01043
2024-11-08 13:14:23.142578: predicting BraTS2021_01050
2024-11-08 13:14:24.032309: predicting BraTS2021_01051
2024-11-08 13:14:24.941839: predicting BraTS2021_01058
2024-11-08 13:14:25.840208: predicting BraTS2021_01063
2024-11-08 13:14:26.733880: predicting BraTS2021_01066
2024-11-08 13:14:27.629827: predicting BraTS2021_01072
2024-11-08 13:14:28.108056: predicting BraTS2021_01079
2024-11-08 13:14:29.011024: predicting BraTS2021_01081
2024-11-08 13:14:29.910713: predicting BraTS2021_01088
2024-11-08 13:14:30.806771: predicting BraTS2021_01089
2024-11-08 13:14:31.705950: predicting BraTS2021_01096
2024-11-08 13:14:32.607494: predicting BraTS2021_01101
2024-11-08 13:14:33.502409: predicting BraTS2021_01110
2024-11-08 13:14:34.395205: predicting BraTS2021_01118
2024-11-08 13:14:36.344494: predicting BraTS2021_01123
2024-11-08 13:14:37.234899: predicting BraTS2021_01124
2024-11-08 13:14:38.137246: predicting BraTS2021_01137
2024-11-08 13:14:38.647508: predicting BraTS2021_01141
2024-11-08 13:14:39.546632: predicting BraTS2021_01142
2024-11-08 13:14:40.439958: predicting BraTS2021_01146
2024-11-08 13:14:41.326671: predicting BraTS2021_01147
2024-11-08 13:14:42.218649: predicting BraTS2021_01152
2024-11-08 13:14:43.111507: predicting BraTS2021_01162
2024-11-08 13:14:44.012282: predicting BraTS2021_01169
2024-11-08 13:14:44.485593: predicting BraTS2021_01170
2024-11-08 13:14:45.377390: predicting BraTS2021_01173
2024-11-08 13:14:46.268027: predicting BraTS2021_01181
2024-11-08 13:14:46.745097: predicting BraTS2021_01183
2024-11-08 13:14:47.226074: predicting BraTS2021_01191
2024-11-08 13:14:48.119892: predicting BraTS2021_01198
2024-11-08 13:14:48.604068: predicting BraTS2021_01206
2024-11-08 13:14:49.500396: predicting BraTS2021_01208
2024-11-08 13:14:50.395899: predicting BraTS2021_01209
2024-11-08 13:14:51.292750: predicting BraTS2021_01210
2024-11-08 13:14:52.258379: predicting BraTS2021_01211
2024-11-08 13:14:53.157111: predicting BraTS2021_01212
2024-11-08 13:14:54.074699: predicting BraTS2021_01225
2024-11-08 13:14:54.985492: predicting BraTS2021_01231
2024-11-08 13:14:55.881289: predicting BraTS2021_01237
2024-11-08 13:14:56.781121: predicting BraTS2021_01238
2024-11-08 13:14:57.675372: predicting BraTS2021_01239
2024-11-08 13:14:58.156226: predicting BraTS2021_01240
2024-11-08 13:14:59.054727: predicting BraTS2021_01262
2024-11-08 13:14:59.536173: predicting BraTS2021_01265
2024-11-08 13:15:00.443788: predicting BraTS2021_01273
2024-11-08 13:15:01.340545: predicting BraTS2021_01274
2024-11-08 13:15:02.236240: predicting BraTS2021_01282
2024-11-08 13:15:03.133745: predicting BraTS2021_01283
2024-11-08 13:15:04.035138: predicting BraTS2021_01284
2024-11-08 13:15:04.932861: predicting BraTS2021_01289
2024-11-08 13:15:05.831779: predicting BraTS2021_01294
2024-11-08 13:15:06.729392: predicting BraTS2021_01296
2024-11-08 13:15:07.973243: predicting BraTS2021_01305
2024-11-08 13:15:08.902194: predicting BraTS2021_01306
2024-11-08 13:15:09.800098: predicting BraTS2021_01312
2024-11-08 13:15:10.706586: predicting BraTS2021_01316
2024-11-08 13:15:11.187375: predicting BraTS2021_01319
2024-11-08 13:15:12.084737: predicting BraTS2021_01320
2024-11-08 13:15:12.990626: predicting BraTS2021_01326
2024-11-08 13:15:13.893310: predicting BraTS2021_01328
2024-11-08 13:15:14.794702: predicting BraTS2021_01331
2024-11-08 13:15:15.697906: predicting BraTS2021_01333
2024-11-08 13:15:16.176661: predicting BraTS2021_01335
2024-11-08 13:15:17.075864: predicting BraTS2021_01339
2024-11-08 13:15:17.979932: predicting BraTS2021_01340
2024-11-08 13:15:18.874857: predicting BraTS2021_01342
2024-11-08 13:15:19.771088: predicting BraTS2021_01345
2024-11-08 13:15:20.669410: predicting BraTS2021_01349
2024-11-08 13:15:21.571866: predicting BraTS2021_01350
2024-11-08 13:15:22.467317: predicting BraTS2021_01356
2024-11-08 13:15:23.367036: predicting BraTS2021_01357
2024-11-08 13:15:24.270710: predicting BraTS2021_01359
2024-11-08 13:15:25.172278: predicting BraTS2021_01361
2024-11-08 13:15:26.074612: predicting BraTS2021_01363
2024-11-08 13:15:26.557987: predicting BraTS2021_01367
2024-11-08 13:15:27.456273: predicting BraTS2021_01375
2024-11-08 13:15:27.967457: predicting BraTS2021_01387
2024-11-08 13:15:28.869045: predicting BraTS2021_01388
2024-11-08 13:15:29.777559: predicting BraTS2021_01389
2024-11-08 13:15:30.688605: predicting BraTS2021_01398
2024-11-08 13:15:31.588152: predicting BraTS2021_01399
2024-11-08 13:15:32.073794: predicting BraTS2021_01402
2024-11-08 13:15:32.978280: predicting BraTS2021_01423
2024-11-08 13:15:33.880217: predicting BraTS2021_01425
2024-11-08 13:15:34.792831: predicting BraTS2021_01440
2024-11-08 13:15:35.692470: predicting BraTS2021_01443
2024-11-08 13:15:36.598576: predicting BraTS2021_01450
2024-11-08 13:15:37.498613: predicting BraTS2021_01461
2024-11-08 13:15:38.399325: predicting BraTS2021_01465
2024-11-08 13:15:41.792176: predicting BraTS2021_01467
2024-11-08 13:15:42.693904: predicting BraTS2021_01476
2024-11-08 13:15:43.591578: predicting BraTS2021_01480
2024-11-08 13:15:44.490433: predicting BraTS2021_01481
2024-11-08 13:15:45.390771: predicting BraTS2021_01488
2024-11-08 13:15:46.292656: predicting BraTS2021_01489
2024-11-08 13:15:47.196150: predicting BraTS2021_01490
2024-11-08 13:15:48.097200: predicting BraTS2021_01492
2024-11-08 13:15:48.996004: predicting BraTS2021_01494
2024-11-08 13:15:49.891182: predicting BraTS2021_01496
2024-11-08 13:15:50.792183: predicting BraTS2021_01500
2024-11-08 13:15:51.689037: predicting BraTS2021_01501
2024-11-08 13:15:52.592074: predicting BraTS2021_01507
2024-11-08 13:15:53.493449: predicting BraTS2021_01513
2024-11-08 13:15:54.393108: predicting BraTS2021_01518
2024-11-08 13:15:55.292577: predicting BraTS2021_01521
2024-11-08 13:15:56.194651: predicting BraTS2021_01522
2024-11-08 13:15:57.096429: predicting BraTS2021_01524
2024-11-08 13:15:57.989662: predicting BraTS2021_01530
2024-11-08 13:15:58.887336: predicting BraTS2021_01538
2024-11-08 13:15:59.787086: predicting BraTS2021_01540
2024-11-08 13:16:00.690654: predicting BraTS2021_01547
2024-11-08 13:16:01.594817: predicting BraTS2021_01560
2024-11-08 13:16:02.495039: predicting BraTS2021_01562
2024-11-08 13:16:03.393934: predicting BraTS2021_01566
2024-11-08 13:16:04.302474: predicting BraTS2021_01567
2024-11-08 13:16:05.199836: predicting BraTS2021_01582
2024-11-08 13:16:06.096575: predicting BraTS2021_01584
2024-11-08 13:16:06.998136: predicting BraTS2021_01593
2024-11-08 13:16:07.480979: predicting BraTS2021_01594
2024-11-08 13:16:08.380968: predicting BraTS2021_01599
2024-11-08 13:16:11.033460: predicting BraTS2021_01601
2024-11-08 13:16:11.937987: predicting BraTS2021_01608
2024-11-08 13:16:13.090758: predicting BraTS2021_01609
2024-11-08 13:16:13.990440: predicting BraTS2021_01613
2024-11-08 13:16:14.894506: predicting BraTS2021_01614
2024-11-08 13:16:15.799142: predicting BraTS2021_01626
2024-11-08 13:16:16.698076: predicting BraTS2021_01634
2024-11-08 13:16:17.607564: predicting BraTS2021_01651
2024-11-08 13:16:18.506117: predicting BraTS2021_01652
2024-11-08 13:16:19.408845: predicting BraTS2021_01653
2024-11-08 13:16:21.430194: predicting BraTS2021_01660
2024-11-08 13:16:21.913194: predicting BraTS2021_01661
2024-11-08 13:16:22.388501: predicting BraTS2021_01662
2024-11-08 13:17:04.582836: Validation complete
2024-11-08 13:17:04.583477: Mean Validation Dice:  0.8989852670194538
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.083 MB of 0.083 MB uploadedwandb: | 0.083 MB of 0.083 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:         Average Dice ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà
wandb: Best EMA pseudo Dice ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: enhancing tumor Dice ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà
wandb:                epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   lr ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:        training_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: training_loss_per_it ‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÜ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ
wandb:      tumor core Dice ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà
wandb:             val_loss ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ
wandb:     whole tumor Dice ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:         Average Dice 0.9379
wandb: Best EMA pseudo Dice 0.9262
wandb: enhancing tumor Dice 0.9245
wandb:                epoch 49
wandb:                   lr 0.0003
wandb:        training_loss -0.88514
wandb: training_loss_per_it -0.9352
wandb:      tumor core Dice 0.9457
wandb:             val_loss -0.89983
wandb:     whole tumor Dice 0.9434
wandb: 
wandb: üöÄ View run BraTs_2023_UMamba_Bot_3d_fullres_fold_0_2024.11.08_08:56:38 at: https://wandb.ai/arild-ntnu/BraTs_2023_UMamba_Bot/runs/l9gfm8cn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/arild-ntnu/BraTs_2023_UMamba_Bot
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241108_085638-l9gfm8cn/logs
2024-11-08 12:50:11.837181: Yayy! New best EMA pseudo Dice: 0.9204
2024-11-08 12:50:12.598207: Epoch time: 328.49 s
2024-11-08 12:50:14.560156: 
2024-11-08 12:50:14.561649: Epoch 42
2024-11-08 12:50:14.562626: Current learning rate: 0.00192
2024-11-08 12:55:42.278112: train_loss -0.88
2024-11-08 12:55:42.279109: val_loss -0.866
2024-11-08 12:55:42.279826: Pseudo dice [0.9431, 0.9212, 0.8949]
2024-11-08 12:55:42.280870: Epoch time: 327.72 s
2024-11-08 12:55:44.352759: 
2024-11-08 12:55:44.353678: Epoch 43
2024-11-08 12:55:44.354335: Current learning rate: 0.0017
2024-11-08 13:01:13.129152: train_loss -0.8798
2024-11-08 13:01:13.130591: val_loss -0.8808
2024-11-08 13:01:13.131339: Pseudo dice [0.9401, 0.9214, 0.9192]
2024-11-08 13:01:13.132065: Yayy! New best EMA pseudo Dice: 0.921
2024-11-08 13:01:13.933485: Epoch time: 328.78 s
2024-11-08 13:01:15.144496: 
2024-11-08 13:01:15.145562: Epoch 44
2024-11-08 13:01:15.146271: Current learning rate: 0.00148
2024-11-08 13:06:43.701816: train_loss -0.884
2024-11-08 13:06:43.703856: val_loss -0.8746
2024-11-08 13:06:43.704628: Pseudo dice [0.9474, 0.9202, 0.9147]
2024-11-08 13:06:43.705375: Yayy! New best EMA pseudo Dice: 0.9217
2024-11-08 13:06:44.803967: Epoch time: 328.56 s
2024-11-08 13:06:47.155482: 
2024-11-08 13:06:47.156397: Epoch 45
2024-11-08 13:06:47.157072: Current learning rate: 0.00126
2024-11-08 13:12:16.949933: train_loss -0.8857
2024-11-08 13:12:16.951069: val_loss -0.8869
2024-11-08 13:12:16.951870: Pseudo dice [0.9442, 0.9495, 0.9227]
2024-11-08 13:12:16.952578: Yayy! New best EMA pseudo Dice: 0.9234
2024-11-08 13:12:17.713201: Epoch time: 329.8 s
2024-11-08 13:12:19.376261: 
2024-11-08 13:12:19.377476: Epoch 46
2024-11-08 13:12:19.378176: Current learning rate: 0.00103
2024-11-08 13:17:47.276282: train_loss -0.8806
2024-11-08 13:17:47.277709: val_loss -0.869
2024-11-08 13:17:47.278557: Pseudo dice [0.9436, 0.9243, 0.9171]
2024-11-08 13:17:47.279285: Yayy! New best EMA pseudo Dice: 0.9239
2024-11-08 13:17:48.101373: Epoch time: 327.9 s
2024-11-08 13:17:49.861242: 
2024-11-08 13:17:49.862349: Epoch 47
2024-11-08 13:17:49.863013: Current learning rate: 0.00079
2024-11-08 13:23:18.492670: train_loss -0.8838
2024-11-08 13:23:18.551495: val_loss -0.8889
2024-11-08 13:23:18.552510: Pseudo dice [0.9467, 0.9404, 0.9142]
2024-11-08 13:23:18.553865: Yayy! New best EMA pseudo Dice: 0.9249
2024-11-08 13:23:19.449879: Epoch time: 328.6 s
2024-11-08 13:23:21.794063: 
2024-11-08 13:23:21.795162: Epoch 48
2024-11-08 13:23:21.795861: Current learning rate: 0.00055
2024-11-08 13:28:51.919627: train_loss -0.8907
2024-11-08 13:28:51.921622: val_loss -0.8675
2024-11-08 13:28:51.922345: Pseudo dice [0.9425, 0.9133, 0.9186]
2024-11-08 13:28:51.923402: Epoch time: 330.13 s
2024-11-08 13:28:53.182558: 
2024-11-08 13:28:53.183660: Epoch 49
2024-11-08 13:28:53.184330: Current learning rate: 0.0003
2024-11-08 13:34:24.393799: train_loss -0.8851
2024-11-08 13:34:24.395052: val_loss -0.8998
2024-11-08 13:34:24.395881: Pseudo dice [0.9434, 0.9457, 0.9245]
2024-11-08 13:34:24.396571: Yayy! New best EMA pseudo Dice: 0.9262
2024-11-08 13:34:25.502063: Epoch time: 331.21 s
2024-11-08 13:34:28.577659: Training done.
2024-11-08 13:34:34.680839: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTS2021/splits_final.json
2024-11-08 13:34:34.707647: The split file contains 5 splits.
2024-11-08 13:34:34.708135: Desired fold for training: 0
2024-11-08 13:34:34.708567: This split has 1000 training and 251 validation cases.
  0%|          | 0/10 [00:00<?, ?it/s] 10%|‚ñà         | 1/10 [00:14<02:10, 14.49s/it] 20%|‚ñà‚ñà        | 2/10 [00:15<00:52,  6.51s/it] 30%|‚ñà‚ñà‚ñà       | 3/10 [00:16<00:27,  3.97s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:17<00:16,  2.77s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:18<00:10,  2.11s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:19<00:06,  1.71s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:20<00:04,  1.45s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:21<00:02,  1.29s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:21<00:01,  1.18s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  1.10s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.29s/it]
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::leaky_relu_ encountered 45 time(s)
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 22 time(s)
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::avg_pool3d encountered 5 time(s)
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 8 time(s)
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul_ encountered 2 time(s)
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::exp encountered 1 time(s)
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::neg encountered 1 time(s)
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::silu encountered 1 time(s)
WARNING:fvcore.nn.jit_analysis:Unsupported operator prim::PythonOp.SelectiveScanFn encountered 1 time(s)
WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
decoder.seg_layers.0, decoder.seg_layers.1, decoder.seg_layers.2, decoder.seg_layers.3, mamba.dt_proj, mamba.in_proj
Total number of FLOPs: 995245190656
Total number of parameters in the model: 58410159
Total number of trainable parameters in the model: 58410159
Mean Latency: 2287.939038872719
Mean FPS: 0.9720398461301307
2024-11-08 13:35:05.065120: predicting BraTS2021_00000
2024-11-08 13:35:07.022421: predicting BraTS2021_00009
2024-11-08 13:35:09.053438: predicting BraTS2021_00016
2024-11-08 13:35:10.975458: predicting BraTS2021_00024
2024-11-08 13:35:12.904654: predicting BraTS2021_00028
2024-11-08 13:35:14.832154: predicting BraTS2021_00031
2024-11-08 13:35:16.762658: predicting BraTS2021_00035
2024-11-08 13:35:18.687390: predicting BraTS2021_00045
2024-11-08 13:35:20.620829: predicting BraTS2021_00046
2024-11-08 13:35:23.259313: predicting BraTS2021_00051
2024-11-08 13:35:57.837264: predicting BraTS2021_00070
2024-11-08 13:35:59.827988: predicting BraTS2021_00078
2024-11-08 13:36:01.750317: predicting BraTS2021_00085
2024-11-08 13:36:03.670576: predicting BraTS2021_00087
2024-11-08 13:36:05.587683: predicting BraTS2021_00088
2024-11-08 13:36:07.502933: predicting BraTS2021_00089
2024-11-08 13:36:09.416663: predicting BraTS2021_00099
2024-11-08 13:36:11.331251: predicting BraTS2021_00102
2024-11-08 13:36:13.252717: predicting BraTS2021_00104
2024-11-08 13:36:15.174130: predicting BraTS2021_00106
2024-11-08 13:36:17.099562: predicting BraTS2021_00107
2024-11-08 13:36:19.563963: predicting BraTS2021_00110
2024-11-08 13:36:21.484879: predicting BraTS2021_00117
2024-11-08 13:36:23.403971: predicting BraTS2021_00123
2024-11-08 13:36:25.327930: predicting BraTS2021_00126
2024-11-08 13:36:27.252220: predicting BraTS2021_00127
2024-11-08 13:36:29.184795: predicting BraTS2021_00130
2024-11-08 13:36:31.107200: predicting BraTS2021_00136
2024-11-08 13:36:33.034407: predicting BraTS2021_00137
2024-11-08 13:36:34.955015: predicting BraTS2021_00140
2024-11-08 13:36:36.879860: predicting BraTS2021_00144
2024-11-08 13:36:38.803018: predicting BraTS2021_00152
2024-11-08 13:36:40.727673: predicting BraTS2021_00160
2024-11-08 13:36:42.654824: predicting BraTS2021_00185
2024-11-08 13:36:44.583258: predicting BraTS2021_00192
2024-11-08 13:36:46.514761: predicting BraTS2021_00194
2024-11-08 13:36:47.516586: predicting BraTS2021_00211
2024-11-08 13:36:49.449977: predicting BraTS2021_00212
2024-11-08 13:36:51.383751: predicting BraTS2021_00214
2024-11-08 13:36:53.321494: predicting BraTS2021_00217
2024-11-08 13:36:55.255097: predicting BraTS2021_00218
2024-11-08 13:36:57.194595: predicting BraTS2021_00231
2024-11-08 13:36:59.127115: predicting BraTS2021_00243
2024-11-08 13:37:01.060868: predicting BraTS2021_00251
2024-11-08 13:37:02.989164: predicting BraTS2021_00266
2024-11-08 13:37:04.915020: predicting BraTS2021_00270
2024-11-08 13:37:06.838720: predicting BraTS2021_00274
2024-11-08 13:37:08.770612: predicting BraTS2021_00283
2024-11-08 13:37:10.702952: predicting BraTS2021_00286
2024-11-08 13:37:12.641435: predicting BraTS2021_00292
2024-11-08 13:37:14.578160: predicting BraTS2021_00294
2024-11-08 13:37:16.509016: predicting BraTS2021_00304
2024-11-08 13:37:18.443305: predicting BraTS2021_00311
2024-11-08 13:37:20.378398: predicting BraTS2021_00313
2024-11-08 13:37:22.315909: predicting BraTS2021_00334
2024-11-08 13:37:24.245766: predicting BraTS2021_00336
2024-11-08 13:37:26.176887: predicting BraTS2021_00350
2024-11-08 13:37:28.108572: predicting BraTS2021_00353
2024-11-08 13:37:30.046109: predicting BraTS2021_00366
2024-11-08 13:37:31.979552: predicting BraTS2021_00375
2024-11-08 13:37:33.913456: predicting BraTS2021_00376
2024-11-08 13:37:35.856271: predicting BraTS2021_00379
2024-11-08 13:37:37.791538: predicting BraTS2021_00391
2024-11-08 13:37:39.724660: predicting BraTS2021_00399
2024-11-08 13:37:41.658412: predicting BraTS2021_00402
2024-11-08 13:37:42.666755: predicting BraTS2021_00419
2024-11-08 13:37:44.600888: predicting BraTS2021_00423
2024-11-08 13:37:46.540123: predicting BraTS2021_00426
2024-11-08 13:37:48.468284: predicting BraTS2021_00430
2024-11-08 13:37:50.399700: predicting BraTS2021_00433
2024-11-08 13:37:52.335493: predicting BraTS2021_00436
2024-11-08 13:37:54.264701: predicting BraTS2021_00442
2024-11-08 13:37:56.201032: predicting BraTS2021_00464
2024-11-08 13:37:58.151144: predicting BraTS2021_00480
2024-11-08 13:38:00.086726: predicting BraTS2021_00485
2024-11-08 13:38:02.013994: predicting BraTS2021_00495
2024-11-08 13:38:03.947984: predicting BraTS2021_00500
2024-11-08 13:38:05.882100: predicting BraTS2021_00514
2024-11-08 13:38:07.820071: predicting BraTS2021_00516
2024-11-08 13:38:09.753824: predicting BraTS2021_00549
2024-11-08 13:38:11.688696: predicting BraTS2021_00558
2024-11-08 13:38:13.628607: predicting BraTS2021_00583
2024-11-08 13:38:15.566629: predicting BraTS2021_00586
2024-11-08 13:38:17.502069: predicting BraTS2021_00587
2024-11-08 13:38:19.445650: predicting BraTS2021_00597
2024-11-08 13:38:21.385246: predicting BraTS2021_00602
2024-11-08 13:38:23.310047: predicting BraTS2021_00605
2024-11-08 13:38:26.765393: predicting BraTS2021_00607
2024-11-08 13:38:28.923081: predicting BraTS2021_00630
2024-11-08 13:38:30.863571: predicting BraTS2021_00639
2024-11-08 13:38:32.791991: predicting BraTS2021_00641
2024-11-08 13:38:34.731619: predicting BraTS2021_00654
2024-11-08 13:38:36.665416: predicting BraTS2021_00693
2024-11-08 13:38:38.596799: predicting BraTS2021_00709
2024-11-08 13:38:40.527468: predicting BraTS2021_00715
2024-11-08 13:38:42.466263: predicting BraTS2021_00716
2024-11-08 13:38:44.401317: predicting BraTS2021_00727
2024-11-08 13:38:46.335021: predicting BraTS2021_00733
2024-11-08 13:38:48.274481: predicting BraTS2021_00735
2024-11-08 13:38:50.216339: predicting BraTS2021_00753
2024-11-08 13:38:52.150469: predicting BraTS2021_00757
2024-11-08 13:38:54.081626: predicting BraTS2021_00759
2024-11-08 13:38:56.021403: predicting BraTS2021_00772
2024-11-08 13:38:57.957107: predicting BraTS2021_00775
2024-11-08 13:38:59.889899: predicting BraTS2021_00777
2024-11-08 13:39:01.827085: predicting BraTS2021_00782
2024-11-08 13:39:03.769267: predicting BraTS2021_00791
2024-11-08 13:39:05.704391: predicting BraTS2021_00793
2024-11-08 13:39:07.638450: predicting BraTS2021_00810
2024-11-08 13:39:09.573863: predicting BraTS2021_00818
2024-11-08 13:39:11.520869: predicting BraTS2021_00834
2024-11-08 13:39:13.450475: predicting BraTS2021_00836
2024-11-08 13:39:15.393304: predicting BraTS2021_01001
2024-11-08 13:39:16.399946: predicting BraTS2021_01004
2024-11-08 13:39:18.332607: predicting BraTS2021_01005
2024-11-08 13:39:20.268566: predicting BraTS2021_01018
2024-11-08 13:39:22.206280: predicting BraTS2021_01037
2024-11-08 13:39:24.148397: predicting BraTS2021_01043
2024-11-08 13:39:26.077809: predicting BraTS2021_01050
2024-11-08 13:39:28.012209: predicting BraTS2021_01051
2024-11-08 13:39:29.986139: predicting BraTS2021_01058
2024-11-08 13:39:31.921882: predicting BraTS2021_01063
2024-11-08 13:39:33.857705: predicting BraTS2021_01066
2024-11-08 13:39:35.791921: predicting BraTS2021_01072
2024-11-08 13:39:36.794286: predicting BraTS2021_01079
2024-11-08 13:39:38.732272: predicting BraTS2021_01081
2024-11-08 13:39:40.671175: predicting BraTS2021_01088
2024-11-08 13:39:42.606568: predicting BraTS2021_01089
2024-11-08 13:39:44.540451: predicting BraTS2021_01096
2024-11-08 13:39:46.476829: predicting BraTS2021_01101
2024-11-08 13:39:48.411202: predicting BraTS2021_01110
2024-11-08 13:39:51.012964: predicting BraTS2021_01118
2024-11-08 13:39:52.943529: predicting BraTS2021_01123
2024-11-08 13:39:54.874422: predicting BraTS2021_01124
2024-11-08 13:39:56.808073: predicting BraTS2021_01137
2024-11-08 13:39:57.805889: predicting BraTS2021_01141
2024-11-08 13:39:59.738211: predicting BraTS2021_01142
2024-11-08 13:40:01.669586: predicting BraTS2021_01146
2024-11-08 13:40:03.593225: predicting BraTS2021_01147
2024-11-08 13:40:05.525199: predicting BraTS2021_01152
2024-11-08 13:40:07.457532: predicting BraTS2021_01162
2024-11-08 13:40:09.394459: predicting BraTS2021_01169
2024-11-08 13:40:10.394495: predicting BraTS2021_01170
2024-11-08 13:40:12.322696: predicting BraTS2021_01173
2024-11-08 13:40:14.250610: predicting BraTS2021_01181
2024-11-08 13:40:15.252443: predicting BraTS2021_01183
2024-11-08 13:40:16.252371: predicting BraTS2021_01191
2024-11-08 13:40:18.179718: predicting BraTS2021_01198
2024-11-08 13:40:19.182949: predicting BraTS2021_01206
2024-11-08 13:40:21.117588: predicting BraTS2021_01208
2024-11-08 13:40:23.055355: predicting BraTS2021_01209
2024-11-08 13:40:24.989991: predicting BraTS2021_01210
2024-11-08 13:40:26.922857: predicting BraTS2021_01211
2024-11-08 13:40:28.862318: predicting BraTS2021_01212
2024-11-08 13:40:30.802265: predicting BraTS2021_01225
2024-11-08 13:40:32.738392: predicting BraTS2021_01231
2024-11-08 13:40:34.670052: predicting BraTS2021_01237
2024-11-08 13:40:36.606108: predicting BraTS2021_01238
2024-11-08 13:40:38.541910: predicting BraTS2021_01239
2024-11-08 13:40:40.103029: predicting BraTS2021_01240
2024-11-08 13:40:42.034992: predicting BraTS2021_01262
2024-11-08 13:40:43.036119: predicting BraTS2021_01265
2024-11-08 13:40:44.979792: predicting BraTS2021_01273
2024-11-08 13:40:46.914203: predicting BraTS2021_01274
2024-11-08 13:40:48.854163: predicting BraTS2021_01282
2024-11-08 13:40:50.791617: predicting BraTS2021_01283
2024-11-08 13:40:52.732497: predicting BraTS2021_01284
2024-11-08 13:40:54.671266: predicting BraTS2021_01289
2024-11-08 13:40:56.606100: predicting BraTS2021_01294
2024-11-08 13:40:58.548589: predicting BraTS2021_01296
2024-11-08 13:41:00.483596: predicting BraTS2021_01305
2024-11-08 13:41:02.416368: predicting BraTS2021_01306
2024-11-08 13:41:04.344725: predicting BraTS2021_01312
2024-11-08 13:41:06.287739: predicting BraTS2021_01316
2024-11-08 13:41:07.294521: predicting BraTS2021_01319
2024-11-08 13:41:09.226377: predicting BraTS2021_01320
2024-11-08 13:41:11.170040: predicting BraTS2021_01326
2024-11-08 13:41:13.106670: predicting BraTS2021_01328
2024-11-08 13:41:15.046329: predicting BraTS2021_01331
2024-11-08 13:41:16.981433: predicting BraTS2021_01333
2024-11-08 13:41:17.983092: predicting BraTS2021_01335
2024-11-08 13:41:19.922011: predicting BraTS2021_01339
2024-11-08 13:41:21.872503: predicting BraTS2021_01340
2024-11-08 13:41:23.799906: predicting BraTS2021_01342
2024-11-08 13:41:25.731244: predicting BraTS2021_01345
2024-11-08 13:41:27.664243: predicting BraTS2021_01349
2024-11-08 13:41:29.601450: predicting BraTS2021_01350
2024-11-08 13:41:31.532432: predicting BraTS2021_01356
2024-11-08 13:41:33.466134: predicting BraTS2021_01357
2024-11-08 13:41:35.400222: predicting BraTS2021_01359
2024-11-08 13:41:37.334087: predicting BraTS2021_01361
2024-11-08 13:41:39.270682: predicting BraTS2021_01363
2024-11-08 13:41:40.272700: predicting BraTS2021_01367
2024-11-08 13:41:42.209152: predicting BraTS2021_01375
2024-11-08 13:41:43.214067: predicting BraTS2021_01387
2024-11-08 13:41:45.147269: predicting BraTS2021_01388
2024-11-08 13:41:47.088906: predicting BraTS2021_01389
2024-11-08 13:41:49.029950: predicting BraTS2021_01398
2024-11-08 13:41:50.968314: predicting BraTS2021_01399
2024-11-08 13:41:51.970510: predicting BraTS2021_01402
2024-11-08 13:41:53.907673: predicting BraTS2021_01423
2024-11-08 13:41:55.844681: predicting BraTS2021_01425
2024-11-08 13:41:57.790396: predicting BraTS2021_01440
2024-11-08 13:41:59.723810: predicting BraTS2021_01443
2024-11-08 13:42:01.658780: predicting BraTS2021_01450
2024-11-08 13:42:03.592215: predicting BraTS2021_01461
2024-11-08 13:42:05.532434: predicting BraTS2021_01465
2024-11-08 13:42:07.468861: predicting BraTS2021_01467
2024-11-08 13:42:09.409007: predicting BraTS2021_01476
2024-11-08 13:42:12.095902: predicting BraTS2021_01480
2024-11-08 13:42:14.025582: predicting BraTS2021_01481
2024-11-08 13:42:15.963619: predicting BraTS2021_01488
2024-11-08 13:42:17.901481: predicting BraTS2021_01489
2024-11-08 13:42:19.837631: predicting BraTS2021_01490
2024-11-08 13:42:21.775969: predicting BraTS2021_01492
2024-11-08 13:42:23.709499: predicting BraTS2021_01494
2024-11-08 13:42:25.638968: predicting BraTS2021_01496
2024-11-08 13:42:27.576122: predicting BraTS2021_01500
2024-11-08 13:42:29.509609: predicting BraTS2021_01501
2024-11-08 13:42:31.440288: predicting BraTS2021_01507
2024-11-08 13:42:33.372972: predicting BraTS2021_01513
2024-11-08 13:42:35.311179: predicting BraTS2021_01518
2024-11-08 13:42:37.245694: predicting BraTS2021_01521
2024-11-08 13:42:39.183768: predicting BraTS2021_01522
2024-11-08 13:42:41.124300: predicting BraTS2021_01524
2024-11-08 13:42:43.051261: predicting BraTS2021_01530
2024-11-08 13:42:44.989863: predicting BraTS2021_01538
2024-11-08 13:42:46.928519: predicting BraTS2021_01540
2024-11-08 13:42:48.864019: predicting BraTS2021_01547
2024-11-08 13:42:50.798879: predicting BraTS2021_01560
2024-11-08 13:42:52.729308: predicting BraTS2021_01562
2024-11-08 13:42:54.663828: predicting BraTS2021_01566
2024-11-08 13:42:56.606454: predicting BraTS2021_01567
2024-11-08 13:42:58.539166: predicting BraTS2021_01582
2024-11-08 13:43:00.474304: predicting BraTS2021_01584
2024-11-08 13:43:02.414720: predicting BraTS2021_01593
2024-11-08 13:43:03.416651: predicting BraTS2021_01594
2024-11-08 13:43:05.351586: predicting BraTS2021_01599
2024-11-08 13:43:07.404928: predicting BraTS2021_01601
2024-11-08 13:43:09.344269: predicting BraTS2021_01608
2024-11-08 13:43:11.288971: predicting BraTS2021_01609
2024-11-08 13:43:13.229305: predicting BraTS2021_01613
2024-11-08 13:43:15.171606: predicting BraTS2021_01614
2024-11-08 13:43:17.117337: predicting BraTS2021_01626
2024-11-08 13:43:19.056320: predicting BraTS2021_01634
2024-11-08 13:43:20.998559: predicting BraTS2021_01651
2024-11-08 13:43:22.935752: predicting BraTS2021_01652
2024-11-08 13:43:24.870529: predicting BraTS2021_01653
2024-11-08 13:43:25.872245: predicting BraTS2021_01660
2024-11-08 13:43:26.872906: predicting BraTS2021_01661
2024-11-08 13:43:27.870880: predicting BraTS2021_01662
2024-11-08 13:43:45.028924: Validation complete
2024-11-08 13:43:45.029515: Mean Validation Dice:  0.900991435728757
