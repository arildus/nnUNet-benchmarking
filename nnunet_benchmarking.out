wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: arildstrom (arildus). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /cluster/home/aris/idun_ws/nnUNet-benchmarking/wandb/run-20241029_175656-fwv9pm4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run BraTs_2023_SegResNet_3d_fullres_fold_0_2024.10.29_17:56:54
wandb: ‚≠êÔ∏è View project at https://wandb.ai/arildus/BraTs_2023_SegResNet
wandb: üöÄ View run at https://wandb.ai/arildus/BraTs_2023_SegResNet/runs/fwv9pm4s
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaBot: UMambaBot(
  (encoder): ResidualEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): Sequential(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (ln): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
  (mamba): Mamba(
    (in_proj): Linear(in_features=320, out_features=1280, bias=False)
    (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
    (act): SiLU()
    (x_proj): Linear(in_features=640, out_features=52, bias=False)
    (dt_proj): Linear(in_features=20, out_features=640, bias=True)
    (out_proj): Linear(in_features=640, out_features=320, bias=False)
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): Sequential(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(640, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(640, 320, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
    )
    (seg_layers): ModuleList(
      (0): Conv3d(320, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (1): Conv3d(256, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (2): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (3): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (4): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [160, 192, 128], 'median_image_size_in_voxels': [140.0, 171.0, 137.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [True, True, True, True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset137_BraTs2021', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [140, 171, 137], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 95242.25, 'mean': 871.816650390625, 'median': 407.0, 'min': 0.10992202162742615, 'percentile_00_5': 55.0, 'percentile_99_5': 5825.0, 'std': 2023.5313720703125}, '1': {'max': 1905559.25, 'mean': 1698.2144775390625, 'median': 552.0, 'min': 0.0, 'percentile_00_5': 47.0, 'percentile_99_5': 8322.0, 'std': 18787.4140625}, '2': {'max': 4438107.0, 'mean': 2141.349365234375, 'median': 738.0, 'min': 0.0, 'percentile_00_5': 110.0, 'percentile_99_5': 10396.0, 'std': 45159.37890625}, '3': {'max': 580014.3125, 'mean': 995.436279296875, 'median': 512.3143920898438, 'min': 0.0, 'percentile_00_5': 108.0, 'percentile_99_5': 11925.0, 'std': 4629.87939453125}}} 

2024-10-29 17:57:10.811788: unpacking dataset...
2024-10-29 17:57:42.278061: unpacking done...
2024-10-29 17:57:42.651761: do_dummy_2d_data_aug: False
2024-10-29 17:57:42.659485: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTs2021/splits_final.json
2024-10-29 17:57:42.681551: The split file contains 5 splits.
2024-10-29 17:57:42.682233: Desired fold for training: 0
2024-10-29 17:57:42.682853: This split has 1000 training and 251 validation cases.
2024-10-29 17:57:44.242234: Unable to plot network architecture:
2024-10-29 17:57:44.243030: No module named 'hiddenlayer'
2024-10-29 17:57:44.803011: 
2024-10-29 17:57:44.803737: Epoch 0
2024-10-29 17:57:45.543173: Current learning rate: 0.01
using pin_memory on device 0
using pin_memory on device 0
2024-10-29 16:28:35.680712: train_loss 0.1646
2024-10-29 16:28:35.706412: val_loss 0.0864
2024-10-29 16:28:35.709677: Pseudo dice [0.8707, 0.707, 0.7142]
2024-10-29 16:28:35.712725: Epoch time: 354.13 s
2024-10-29 16:28:35.726603: Yayy! New best EMA pseudo Dice: 0.764
2024-10-29 16:28:40.252135: 
2024-10-29 16:28:40.253793: Epoch 1
2024-10-29 16:28:40.254709: Current learning rate: 0.0001
2024-10-29 16:33:33.402210: train_loss 0.0162
2024-10-29 16:33:33.408742: val_loss -0.0371
2024-10-29 16:33:33.421090: Pseudo dice [0.8835, 0.663, 0.6693]
2024-10-29 16:33:33.433657: Epoch time: 293.15 s
2024-10-29 16:33:37.006118: 
2024-10-29 16:33:37.010605: Epoch 2
2024-10-29 16:33:37.011324: Current learning rate: 0.0001
2024-10-29 16:38:40.942505: train_loss -0.1054
2024-10-29 16:38:40.961743: val_loss -0.158
2024-10-29 16:38:40.962604: Pseudo dice [0.9086, 0.7786, 0.6748]
2024-10-29 16:38:40.963964: Epoch time: 303.94 s
2024-10-29 16:38:40.975939: Yayy! New best EMA pseudo Dice: 0.764
2024-10-29 16:38:45.769930: 
2024-10-29 16:38:45.779059: Epoch 3
2024-10-29 16:38:45.779870: Current learning rate: 9e-05
2024-10-29 16:43:33.748572: train_loss -0.2231
2024-10-29 16:43:33.752910: val_loss -0.2749
2024-10-29 16:43:33.755168: Pseudo dice [0.9097, 0.7892, 0.7813]
2024-10-29 16:43:33.756393: Epoch time: 287.98 s
2024-10-29 16:43:33.761359: Yayy! New best EMA pseudo Dice: 0.7703
2024-10-29 16:43:37.262242: 
2024-10-29 16:43:37.265270: Epoch 4
2024-10-29 16:43:37.266558: Current learning rate: 9e-05
2024-10-29 16:48:25.861629: train_loss -0.3532
2024-10-29 16:48:25.869167: val_loss -0.3931
2024-10-29 16:48:25.870265: Pseudo dice [0.9173, 0.7371, 0.7106]
2024-10-29 16:48:25.872472: Epoch time: 288.6 s
2024-10-29 16:48:26.671760: Yayy! New best EMA pseudo Dice: 0.7721
2024-10-29 16:48:29.448534: 
2024-10-29 16:48:29.450222: Epoch 5
2024-10-29 16:48:29.451349: Current learning rate: 9e-05
2024-10-29 16:53:51.215286: train_loss -0.4731
2024-10-29 16:53:51.241464: val_loss -0.5343
2024-10-29 16:53:51.250142: Pseudo dice [0.9265, 0.8471, 0.8158]
2024-10-29 16:53:51.258440: Epoch time: 321.77 s
2024-10-29 16:53:51.259199: Yayy! New best EMA pseudo Dice: 0.7812
2024-10-29 16:53:54.325545: 
2024-10-29 16:53:54.329667: Epoch 6
2024-10-29 16:53:54.331200: Current learning rate: 9e-05
2024-10-29 16:58:50.764269: train_loss -0.5976
2024-10-29 16:58:50.786680: val_loss -0.6433
2024-10-29 16:58:50.788906: Pseudo dice [0.9208, 0.8974, 0.8541]
2024-10-29 16:58:50.794043: Epoch time: 296.44 s
2024-10-29 16:58:50.812123: Yayy! New best EMA pseudo Dice: 0.7922
2024-10-29 16:58:54.738592: 
2024-10-29 16:58:54.740697: Epoch 7
2024-10-29 16:58:54.741628: Current learning rate: 9e-05
2024-10-29 17:03:57.029224: train_loss -0.6708
2024-10-29 17:03:57.039331: val_loss -0.6972
2024-10-29 17:03:57.048234: Pseudo dice [0.9229, 0.8808, 0.8813]
2024-10-29 17:03:57.049811: Epoch time: 302.29 s
2024-10-29 17:03:57.050694: Yayy! New best EMA pseudo Dice: 0.8024
2024-10-29 17:04:02.456182: 
2024-10-29 17:04:02.474754: Epoch 8
2024-10-29 17:04:02.477447: Current learning rate: 9e-05
2024-10-29 17:09:11.438910: train_loss -0.7326
2024-10-29 17:09:11.441844: val_loss -0.7393
2024-10-29 17:09:11.442646: Pseudo dice [0.9141, 0.8651, 0.8721]
2024-10-29 17:09:11.443952: Epoch time: 308.98 s
2024-10-29 17:09:11.451547: Yayy! New best EMA pseudo Dice: 0.8106
2024-10-29 17:09:15.041233: 
2024-10-29 17:09:15.045743: Epoch 9
2024-10-29 17:09:15.052783: Current learning rate: 8e-05
2024-10-29 17:14:17.853419: train_loss -0.7598
2024-10-29 17:14:17.863618: val_loss -0.7476
2024-10-29 17:14:17.875153: Pseudo dice [0.9028, 0.8896, 0.8764]
2024-10-29 17:14:17.884454: Epoch time: 302.81 s
2024-10-29 17:14:19.154256: Yayy! New best EMA pseudo Dice: 0.8185
2024-10-29 17:14:21.912666: 
2024-10-29 17:14:21.919209: Epoch 10
2024-10-29 17:14:21.919967: Current learning rate: 8e-05
2024-10-29 17:19:37.477179: train_loss -0.7966
2024-10-29 17:19:37.483721: val_loss -0.8034
2024-10-29 17:19:37.484728: Pseudo dice [0.9281, 0.8864, 0.8929]
2024-10-29 17:19:37.492832: Epoch time: 315.56 s
2024-10-29 17:19:37.493555: Yayy! New best EMA pseudo Dice: 0.8269
2024-10-29 17:19:40.781422: 
2024-10-29 17:19:40.783848: Epoch 11
2024-10-29 17:19:40.784742: Current learning rate: 8e-05
2024-10-29 17:24:39.324306: train_loss -0.8092
2024-10-29 17:24:39.333940: val_loss -0.8074
2024-10-29 17:24:39.335216: Pseudo dice [0.9034, 0.8896, 0.8787]
2024-10-29 17:24:39.336805: Epoch time: 298.55 s
2024-10-29 17:24:39.337530: Yayy! New best EMA pseudo Dice: 0.8332
2024-10-29 17:24:44.147170: 
2024-10-29 17:24:44.157413: Epoch 12
2024-10-29 17:24:44.158230: Current learning rate: 8e-05
2024-10-29 17:30:05.963098: train_loss -0.8222
2024-10-29 17:30:06.731607: val_loss -0.8269
2024-10-29 17:30:06.733812: Pseudo dice [0.9263, 0.9086, 0.8805]
2024-10-29 17:30:06.794105: Epoch time: 321.78 s
2024-10-29 17:30:06.801099: Yayy! New best EMA pseudo Dice: 0.8404
2024-10-29 17:30:12.763030: 
2024-10-29 17:30:12.770970: Epoch 13
2024-10-29 17:30:12.777390: Current learning rate: 8e-05
2024-10-29 17:35:05.037534: train_loss -0.8304
2024-10-29 17:35:05.039481: val_loss -0.8241
2024-10-29 17:35:05.040429: Pseudo dice [0.9205, 0.9048, 0.8823]
2024-10-29 17:35:05.041676: Epoch time: 292.28 s
2024-10-29 17:35:05.042341: Yayy! New best EMA pseudo Dice: 0.8466
2024-10-29 17:35:07.188655: 
2024-10-29 17:35:07.190650: Epoch 14
2024-10-29 17:35:07.191378: Current learning rate: 7e-05
2024-10-29 17:40:00.654422: train_loss -0.8288
2024-10-29 17:40:00.663978: val_loss -0.8572
2024-10-29 17:40:00.664878: Pseudo dice [0.9318, 0.9243, 0.8971]
2024-10-29 17:40:00.667255: Epoch time: 293.47 s
2024-10-29 17:40:02.451028: Yayy! New best EMA pseudo Dice: 0.8538
2024-10-29 17:40:09.554625: 
2024-10-29 17:40:09.559868: Epoch 15
2024-10-29 17:40:09.561401: Current learning rate: 7e-05
2024-10-29 17:45:18.233312: train_loss -0.8443
2024-10-29 17:45:18.244391: val_loss -0.8435
2024-10-29 17:45:18.254481: Pseudo dice [0.9206, 0.9107, 0.8853]
2024-10-29 17:45:18.260717: Epoch time: 308.68 s
2024-10-29 17:45:18.261579: Yayy! New best EMA pseudo Dice: 0.8589
2024-10-29 17:45:21.723337: 
2024-10-29 17:45:21.733565: Epoch 16
2024-10-29 17:45:21.734380: Current learning rate: 7e-05
2024-10-29 17:50:25.888708: train_loss -0.847
2024-10-29 17:50:25.893472: val_loss -0.8507
2024-10-29 17:50:25.908063: Pseudo dice [0.9347, 0.9107, 0.9123]
2024-10-29 17:50:25.909543: Epoch time: 304.17 s
2024-10-29 17:50:25.910260: Yayy! New best EMA pseudo Dice: 0.865
2024-10-29 17:50:30.144325: 
2024-10-29 17:50:30.156477: Epoch 17
2024-10-29 17:50:30.157220: Current learning rate: 7e-05
2024-10-29 17:55:38.951442: train_loss -0.85
2024-10-29 17:55:38.952491: val_loss -0.8685
2024-10-29 17:55:38.953460: Pseudo dice [0.9267, 0.933, 0.9076]
2024-10-29 17:55:38.954607: Epoch time: 308.81 s
2024-10-29 17:55:38.955471: Yayy! New best EMA pseudo Dice: 0.8707
2024-10-29 17:55:41.922646: 
2024-10-29 17:55:41.931296: Epoch 18
2024-10-29 17:55:41.932104: Current learning rate: 7e-05
2024-10-29 18:00:33.641155: train_loss -0.8529
2024-10-29 18:00:33.646111: val_loss -0.8714
2024-10-29 18:00:33.646909: Pseudo dice [0.9366, 0.914, 0.9045]
2024-10-29 18:00:33.648063: Epoch time: 291.72 s
2024-10-29 18:00:33.655242: Yayy! New best EMA pseudo Dice: 0.8755
2024-10-29 18:00:37.539871: 
2024-10-29 18:00:37.548216: Epoch 19
2024-10-29 18:00:37.549059: Current learning rate: 7e-05
2024-10-29 18:06:00.183511: train_loss -0.8593
2024-10-29 18:06:00.202297: val_loss -0.8573
2024-10-29 18:06:00.213820: Pseudo dice [0.9236, 0.9003, 0.8867]
2024-10-29 18:06:00.221718: Epoch time: 322.64 s
2024-10-29 18:06:01.723266: Yayy! New best EMA pseudo Dice: 0.8783
2024-10-29 18:06:05.401481: 
2024-10-29 18:06:05.415172: Epoch 20
2024-10-29 18:06:05.416389: Current learning rate: 6e-05
2024-10-29 18:11:08.355139: train_loss -0.8598
2024-10-29 18:11:08.358026: val_loss -0.8696
2024-10-29 18:11:08.359010: Pseudo dice [0.9347, 0.913, 0.9091]
2024-10-29 18:11:08.361094: Epoch time: 302.95 s
2024-10-29 18:11:08.367760: Yayy! New best EMA pseudo Dice: 0.8823
2024-10-29 18:11:12.888102: 
using pin_memory on device 0
2024-10-29 16:10:40.142946: train_loss -0.3809
2024-10-29 16:10:40.197893: val_loss -0.7177
2024-10-29 16:10:40.199060: Pseudo dice [0.8537, 0.7768, 0.8082]
2024-10-29 16:10:40.201107: Epoch time: 473.81 s
2024-10-29 16:10:40.202086: Yayy! New best EMA pseudo Dice: 0.8129
2024-10-29 16:10:50.633333: 
2024-10-29 16:10:50.634933: Epoch 1
2024-10-29 16:10:50.635892: Current learning rate: 0.00982
2024-10-29 16:17:02.597967: train_loss -0.7089
2024-10-29 16:17:02.607600: val_loss -0.7366
2024-10-29 16:17:02.608622: Pseudo dice [0.881, 0.8012, 0.82]
2024-10-29 16:17:02.610480: Epoch time: 371.97 s
2024-10-29 16:17:02.611658: Yayy! New best EMA pseudo Dice: 0.815
2024-10-29 16:17:06.780208: 
2024-10-29 16:17:06.781487: Epoch 2
2024-10-29 16:17:06.782507: Current learning rate: 0.00964
2024-10-29 16:23:27.730402: train_loss -0.7394
2024-10-29 16:23:27.731761: val_loss -0.7638
2024-10-29 16:23:27.741249: Pseudo dice [0.8911, 0.8577, 0.8382]
2024-10-29 16:23:27.743243: Epoch time: 380.95 s
2024-10-29 16:23:27.744161: Yayy! New best EMA pseudo Dice: 0.8197
2024-10-29 16:23:32.321904: 
2024-10-29 16:23:32.323667: Epoch 3
2024-10-29 16:23:32.324798: Current learning rate: 0.00946
2024-10-29 16:30:10.798777: train_loss -0.7555
2024-10-29 16:30:10.824631: val_loss -0.7686
2024-10-29 16:30:10.833974: Pseudo dice [0.8897, 0.847, 0.8592]
2024-10-29 16:30:10.836172: Epoch time: 398.48 s
2024-10-29 16:30:10.844455: Yayy! New best EMA pseudo Dice: 0.8243
2024-10-29 16:30:15.676966: 
2024-10-29 16:30:15.692929: Epoch 4
2024-10-29 16:30:15.694054: Current learning rate: 0.00928
2024-10-29 16:36:49.900259: train_loss -0.7712
2024-10-29 16:36:49.902179: val_loss -0.7922
2024-10-29 16:36:49.903270: Pseudo dice [0.9023, 0.8665, 0.8584]
2024-10-29 16:36:49.913795: Epoch time: 394.2 s
2024-10-29 16:36:50.777209: Yayy! New best EMA pseudo Dice: 0.8294
2024-10-29 16:36:55.187313: 
2024-10-29 16:36:55.188632: Epoch 5
2024-10-29 16:36:55.189943: Current learning rate: 0.0091
2024-10-29 16:43:03.050795: train_loss -0.7941
2024-10-29 16:43:03.066510: val_loss -0.8012
2024-10-29 16:43:03.067523: Pseudo dice [0.9109, 0.8831, 0.8619]
2024-10-29 16:43:03.069148: Epoch time: 367.86 s
2024-10-29 16:43:03.069970: Yayy! New best EMA pseudo Dice: 0.835
2024-10-29 16:43:08.757035: 
2024-10-29 16:43:08.759025: Epoch 6
2024-10-29 16:43:08.760133: Current learning rate: 0.00891
2024-10-29 16:49:33.339138: train_loss -0.8101
2024-10-29 16:49:33.347872: val_loss -0.8058
2024-10-29 16:49:33.348947: Pseudo dice [0.9141, 0.9021, 0.881]
2024-10-29 16:49:33.350864: Epoch time: 384.58 s
2024-10-29 16:49:33.363320: Yayy! New best EMA pseudo Dice: 0.8414
2024-10-29 16:49:38.019929: 
2024-10-29 16:49:38.021789: Epoch 7
2024-10-29 16:49:38.023102: Current learning rate: 0.00873
2024-10-29 16:56:01.118036: train_loss -0.8114
2024-10-29 16:56:01.128195: val_loss -0.8183
2024-10-29 16:56:01.129555: Pseudo dice [0.9205, 0.8925, 0.8753]
2024-10-29 16:56:01.131449: Epoch time: 383.1 s
2024-10-29 16:56:01.141955: Yayy! New best EMA pseudo Dice: 0.8469
2024-10-29 16:56:06.221506: 
2024-10-29 16:56:06.223784: Epoch 8
2024-10-29 16:56:06.225077: Current learning rate: 0.00855
2024-10-29 17:02:24.213370: train_loss -0.8319
2024-10-29 17:02:24.229486: val_loss -0.8063
2024-10-29 17:02:24.230415: Pseudo dice [0.9136, 0.8531, 0.8632]
2024-10-29 17:02:24.232176: Epoch time: 377.99 s
2024-10-29 17:02:24.233429: Yayy! New best EMA pseudo Dice: 0.8499
2024-10-29 17:02:28.807349: 
2024-10-29 17:02:28.812428: Epoch 9
2024-10-29 17:02:28.813481: Current learning rate: 0.00836
2024-10-29 17:08:54.046038: train_loss -0.823
2024-10-29 17:08:54.503875: val_loss -0.8417
2024-10-29 17:08:54.505732: Pseudo dice [0.9228, 0.9182, 0.8915]
2024-10-29 17:08:54.509212: Epoch time: 385.24 s
2024-10-29 17:08:56.846892: Yayy! New best EMA pseudo Dice: 0.856
2024-10-29 17:09:04.830949: 
2024-10-29 17:09:04.832539: Epoch 10
2024-10-29 17:09:04.833824: Current learning rate: 0.00818
2024-10-29 17:15:36.422546: train_loss -0.8264
2024-10-29 17:15:36.425961: val_loss -0.8456
2024-10-29 17:15:36.427469: Pseudo dice [0.9226, 0.9171, 0.8871]
2024-10-29 17:15:36.435997: Epoch time: 391.59 s
2024-10-29 17:15:36.437310: Yayy! New best EMA pseudo Dice: 0.8613
2024-10-29 17:15:42.705420: 
2024-10-29 17:15:42.707756: Epoch 11
2024-10-29 17:15:42.709056: Current learning rate: 0.008
2024-10-29 17:22:05.079292: train_loss -0.8078
2024-10-29 17:22:05.081105: val_loss -0.8474
2024-10-29 17:22:05.082095: Pseudo dice [0.9155, 0.9015, 0.8971]
2024-10-29 17:22:05.084724: Epoch time: 382.38 s
2024-10-29 17:22:05.085698: Yayy! New best EMA pseudo Dice: 0.8656
2024-10-29 17:22:09.153426: 
2024-10-29 17:22:09.154723: Epoch 12
2024-10-29 17:22:09.155584: Current learning rate: 0.00781
2024-10-29 17:28:28.434581: train_loss -0.8057
2024-10-29 17:28:28.447319: val_loss -0.8443
2024-10-29 17:28:28.448534: Pseudo dice [0.9155, 0.9114, 0.8922]
2024-10-29 17:28:28.450228: Epoch time: 379.28 s
2024-10-29 17:28:28.451085: Yayy! New best EMA pseudo Dice: 0.8697
2024-10-29 17:28:33.098362: 
2024-10-29 17:28:33.111310: Epoch 13
2024-10-29 17:28:33.112412: Current learning rate: 0.00763
2024-10-29 17:35:03.063703: train_loss -0.8284
2024-10-29 17:35:03.078792: val_loss -0.8437
2024-10-29 17:35:03.080238: Pseudo dice [0.9275, 0.9078, 0.8953]
2024-10-29 17:35:03.082141: Epoch time: 389.97 s
2024-10-29 17:35:03.083560: Yayy! New best EMA pseudo Dice: 0.8737
2024-10-29 17:35:10.807539: 
2024-10-29 17:35:10.809254: Epoch 14
2024-10-29 17:35:10.810622: Current learning rate: 0.00744
2024-10-29 17:41:24.417134: train_loss -0.8394
2024-10-29 17:41:24.427390: val_loss -0.8481
2024-10-29 17:41:24.428488: Pseudo dice [0.9334, 0.9126, 0.9015]
2024-10-29 17:41:24.437582: Epoch time: 373.61 s
2024-10-29 17:41:25.957701: Yayy! New best EMA pseudo Dice: 0.8779
2024-10-29 17:41:30.271526: 
2024-10-29 17:41:30.272915: Epoch 15
2024-10-29 17:41:30.273827: Current learning rate: 0.00725
2024-10-29 17:48:05.066827: train_loss -0.8312
2024-10-29 17:48:05.142865: val_loss -0.8436
2024-10-29 17:48:05.144147: Pseudo dice [0.9167, 0.9077, 0.8875]
2024-10-29 17:48:05.146323: Epoch time: 394.8 s
2024-10-29 17:48:05.147258: Yayy! New best EMA pseudo Dice: 0.8805
2024-10-29 17:48:10.453744: 
2024-10-29 17:48:10.455310: Epoch 16
2024-10-29 17:48:10.456332: Current learning rate: 0.00707
2024-10-29 17:54:30.691817: train_loss -0.8359
2024-10-29 17:54:30.710872: val_loss -0.8484
2024-10-29 17:54:30.711868: Pseudo dice [0.9229, 0.9118, 0.892]
2024-10-29 17:54:30.713402: Epoch time: 380.24 s
2024-10-29 17:54:30.714241: Yayy! New best EMA pseudo Dice: 0.8834
2024-10-29 17:54:35.686371: 
2024-10-29 17:54:35.688020: Epoch 17
2024-10-29 17:54:35.689169: Current learning rate: 0.00688
2024-10-29 18:01:02.009272: train_loss -0.848
2024-10-29 18:01:02.010787: val_loss -0.8434
2024-10-29 18:01:02.011984: Pseudo dice [0.9242, 0.9036, 0.8998]
2024-10-29 18:01:02.013819: Epoch time: 386.32 s
2024-10-29 18:01:02.014708: Yayy! New best EMA pseudo Dice: 0.886
2024-10-29 18:01:06.723471: 
2024-10-29 18:01:06.725150: Epoch 18
2024-10-29 18:01:06.726441: Current learning rate: 0.00669
2024-10-29 18:07:41.046375: train_loss -0.8388
2024-10-29 18:07:41.048830: val_loss -0.857
2024-10-29 18:07:41.049883: Pseudo dice [0.933, 0.9214, 0.8928]
2024-10-29 18:07:41.051976: Epoch time: 394.32 s
2024-10-29 18:07:41.052953: Yayy! New best EMA pseudo Dice: 0.8889
2024-10-29 18:07:44.367144: 
2024-10-29 18:07:44.368403: Epoch 19
2024-10-29 18:07:44.378284: Current learning rate: 0.0065
2024-10-29 18:14:06.325748: train_loss -0.8467
2024-10-29 18:14:06.327483: val_loss -0.841
2024-10-29 18:14:06.328458: Pseudo dice [0.925, 0.9057, 0.8976]
2024-10-29 18:14:06.335393: Epoch time: 381.96 s
2024-10-29 18:14:07.836039: Yayy! New best EMA pseudo Dice: 0.891
2024-10-29 18:14:13.251709: 
2024-10-29 18:14:13.266136: Epoch 20
2024-10-29 18:14:13.267133: Current learning rate: 0.00631
2024-10-29 18:20:44.244233: train_loss -0.8499
2024-10-29 18:20:44.246628: val_loss -0.8529
2024-10-29 18:20:44.253181: Pseudo dice [0.9341, 0.9203, 0.8924]
2024-10-29 18:20:44.259541: Epoch time: 390.99 s
2024-10-29 18:11:12.899183: Epoch 21
2024-10-29 18:11:12.900930: Current learning rate: 6e-05
2024-10-29 18:16:19.095054: train_loss -0.8628
2024-10-29 18:16:19.105266: val_loss -0.8732
2024-10-29 18:16:19.113405: Pseudo dice [0.9305, 0.9239, 0.9102]
2024-10-29 18:16:19.114775: Epoch time: 306.21 s
2024-10-29 18:16:19.115422: Yayy! New best EMA pseudo Dice: 0.8863
2024-10-29 18:16:25.030963: 
2024-10-29 18:16:25.064066: Epoch 22
2024-10-29 18:16:25.072075: Current learning rate: 6e-05
2024-10-29 18:21:32.156561: train_loss -0.8631
2024-10-29 18:21:32.161591: val_loss -0.8681
2024-10-29 18:21:32.172604: Pseudo dice [0.9318, 0.9213, 0.9109]
2024-10-29 18:21:32.183525: Epoch time: 307.13 s
2024-10-29 18:21:32.192024: Yayy! New best EMA pseudo Dice: 0.8898
2024-10-29 18:21:36.490116: 
2024-10-29 18:21:36.499793: Epoch 23
2024-10-29 18:21:36.500586: Current learning rate: 6e-05
2024-10-29 18:26:46.218474: train_loss -0.873
2024-10-29 18:26:46.233876: val_loss -0.8777
2024-10-29 18:26:46.241068: Pseudo dice [0.9345, 0.9143, 0.9214]
2024-10-29 18:26:46.248400: Epoch time: 309.73 s
2024-10-29 18:26:46.253582: Yayy! New best EMA pseudo Dice: 0.8931
2024-10-29 18:26:50.100840: 
2024-10-29 18:26:50.101893: Epoch 24
2024-10-29 18:26:50.102670: Current learning rate: 6e-05
2024-10-29 18:31:50.416839: train_loss -0.8602
2024-10-29 18:31:50.420747: val_loss -0.882
2024-10-29 18:31:50.427929: Pseudo dice [0.9382, 0.9392, 0.9102]
2024-10-29 18:31:50.436340: Epoch time: 300.32 s
2024-10-29 18:31:51.768565: Yayy! New best EMA pseudo Dice: 0.8967
2024-10-29 18:31:55.000766: 
2024-10-29 18:31:55.001935: Epoch 25
2024-10-29 18:31:55.002672: Current learning rate: 5e-05
2024-10-29 18:36:55.197820: train_loss -0.8723
2024-10-29 18:36:55.274641: val_loss -0.8666
2024-10-29 18:36:55.282512: Pseudo dice [0.9424, 0.9075, 0.911]
2024-10-29 18:36:55.315594: Epoch time: 299.96 s
2024-10-29 18:36:55.316528: Yayy! New best EMA pseudo Dice: 0.8991
2024-10-29 18:37:03.875892: 
2024-10-29 18:37:03.876998: Epoch 26
2024-10-29 18:37:03.877739: Current learning rate: 5e-05
2024-10-29 18:41:52.705392: train_loss -0.8658
2024-10-29 18:41:52.712886: val_loss -0.8802
2024-10-29 18:41:52.724005: Pseudo dice [0.9444, 0.9141, 0.9068]
2024-10-29 18:41:52.729546: Epoch time: 288.83 s
2024-10-29 18:41:52.736033: Yayy! New best EMA pseudo Dice: 0.9014
2024-10-29 18:41:57.462526: 
2024-10-29 18:41:57.474791: Epoch 27
2024-10-29 18:41:57.475672: Current learning rate: 5e-05
2024-10-29 18:47:08.979026: train_loss -0.8747
2024-10-29 18:47:08.997116: val_loss -0.8645
2024-10-29 18:47:08.998893: Pseudo dice [0.9348, 0.9157, 0.8981]
2024-10-29 18:47:09.004058: Epoch time: 311.52 s
2024-10-29 18:47:09.006777: Yayy! New best EMA pseudo Dice: 0.9029
2024-10-29 18:47:13.664932: 
2024-10-29 18:47:13.672630: Epoch 28
2024-10-29 18:47:13.673386: Current learning rate: 5e-05
2024-10-29 18:52:31.172773: train_loss -0.8779
2024-10-29 18:52:31.187629: val_loss -0.8673
2024-10-29 18:52:31.192260: Pseudo dice [0.9124, 0.8988, 0.9007]
2024-10-29 18:52:31.193587: Epoch time: 317.51 s
2024-10-29 18:52:31.194255: Yayy! New best EMA pseudo Dice: 0.903
2024-10-29 18:52:35.323463: 
2024-10-29 18:52:35.338295: Epoch 29
2024-10-29 18:52:35.339073: Current learning rate: 5e-05
2024-10-29 18:57:45.356469: train_loss -0.87
2024-10-29 18:57:45.367286: val_loss -0.8653
2024-10-29 18:57:45.380030: Pseudo dice [0.9379, 0.9167, 0.9008]
2024-10-29 18:57:45.381480: Epoch time: 310.03 s
2024-10-29 18:57:46.594957: Yayy! New best EMA pseudo Dice: 0.9045
2024-10-29 18:57:48.522152: 
2024-10-29 18:57:48.523318: Epoch 30
2024-10-29 18:57:48.524042: Current learning rate: 4e-05
2024-10-29 19:02:42.428060: train_loss -0.8789
2024-10-29 19:02:42.437373: val_loss -0.8928
2024-10-29 19:02:42.445981: Pseudo dice [0.9421, 0.9092, 0.9091]
2024-10-29 19:02:42.450470: Epoch time: 293.91 s
2024-10-29 19:02:42.454316: Yayy! New best EMA pseudo Dice: 0.9061
2024-10-29 19:02:47.960642: 
2024-10-29 19:02:47.980490: Epoch 31
2024-10-29 19:02:47.981876: Current learning rate: 4e-05
2024-10-29 19:07:51.775451: train_loss -0.8748
2024-10-29 19:07:51.788557: val_loss -0.8683
2024-10-29 19:07:51.789320: Pseudo dice [0.9334, 0.9151, 0.9052]
2024-10-29 19:07:51.790456: Epoch time: 303.83 s
2024-10-29 19:07:51.791088: Yayy! New best EMA pseudo Dice: 0.9073
2024-10-29 19:07:54.669186: 
2024-10-29 19:07:54.670833: Epoch 32
2024-10-29 19:07:54.671591: Current learning rate: 4e-05
2024-10-29 19:12:51.935147: train_loss -0.8753
2024-10-29 19:12:51.943162: val_loss -0.8956
2024-10-29 19:12:51.943913: Pseudo dice [0.9501, 0.9345, 0.9235]
2024-10-29 19:12:51.945081: Epoch time: 297.27 s
2024-10-29 19:12:51.945830: Yayy! New best EMA pseudo Dice: 0.9101
2024-10-29 19:12:55.137126: 
2024-10-29 19:12:55.151571: Epoch 33
2024-10-29 19:12:55.152543: Current learning rate: 4e-05
2024-10-29 19:18:01.425601: train_loss -0.8733
2024-10-29 19:18:01.453394: val_loss -0.8773
2024-10-29 19:18:01.454227: Pseudo dice [0.9334, 0.9016, 0.9018]
2024-10-29 19:18:01.455395: Epoch time: 306.29 s
2024-10-29 19:18:01.456053: Yayy! New best EMA pseudo Dice: 0.9104
2024-10-29 19:18:05.297029: 
2024-10-29 19:18:05.301870: Epoch 34
2024-10-29 19:18:05.302767: Current learning rate: 4e-05
2024-10-29 19:23:03.980063: train_loss -0.8793
2024-10-29 19:23:03.990349: val_loss -0.8626
2024-10-29 19:23:03.999133: Pseudo dice [0.9378, 0.9191, 0.9178]
2024-10-29 19:23:04.000305: Epoch time: 298.68 s
2024-10-29 19:23:05.029784: Yayy! New best EMA pseudo Dice: 0.9118
2024-10-29 19:23:08.588626: 
2024-10-29 19:23:08.600458: Epoch 35
2024-10-29 19:23:08.601185: Current learning rate: 3e-05
2024-10-29 19:28:22.278742: train_loss -0.8769
2024-10-29 19:28:22.285489: val_loss -0.8724
2024-10-29 19:28:22.290499: Pseudo dice [0.9404, 0.9295, 0.9093]
2024-10-29 19:28:22.291660: Epoch time: 313.69 s
2024-10-29 19:28:22.292375: Yayy! New best EMA pseudo Dice: 0.9133
2024-10-29 19:28:26.208600: 
2024-10-29 19:28:26.209836: Epoch 36
2024-10-29 19:28:26.210541: Current learning rate: 3e-05
2024-10-29 19:33:36.991244: train_loss -0.8783
2024-10-29 19:33:36.992894: val_loss -0.893
2024-10-29 19:33:36.993702: Pseudo dice [0.9402, 0.9453, 0.9192]
2024-10-29 19:33:36.995237: Epoch time: 310.78 s
2024-10-29 19:33:36.995973: Yayy! New best EMA pseudo Dice: 0.9154
2024-10-29 19:33:41.397904: 
2024-10-29 19:33:41.398989: Epoch 37
2024-10-29 19:33:41.399787: Current learning rate: 3e-05
2024-10-29 19:38:43.481188: train_loss -0.89
2024-10-29 19:38:43.503506: val_loss -0.8978
2024-10-29 19:38:43.504371: Pseudo dice [0.9409, 0.9453, 0.9229]
2024-10-29 19:38:43.505593: Epoch time: 302.08 s
2024-10-29 19:38:43.506248: Yayy! New best EMA pseudo Dice: 0.9175
2024-10-29 19:38:47.635158: 
2024-10-29 19:38:47.647951: Epoch 38
2024-10-29 19:38:47.648955: Current learning rate: 3e-05
2024-10-29 19:43:56.564986: train_loss -0.8794
2024-10-29 19:43:56.581709: val_loss -0.8984
2024-10-29 19:43:56.582521: Pseudo dice [0.9398, 0.94, 0.9199]
2024-10-29 19:43:56.583968: Epoch time: 308.93 s
2024-10-29 19:43:56.584824: Yayy! New best EMA pseudo Dice: 0.9191
2024-10-29 19:44:01.808181: 
2024-10-29 19:44:01.816500: Epoch 39
2024-10-29 19:44:01.817376: Current learning rate: 3e-05
2024-10-29 19:49:13.528394: train_loss -0.8863
2024-10-29 19:49:13.533302: val_loss -0.8727
2024-10-29 19:49:13.534029: Pseudo dice [0.944, 0.9224, 0.9123]
2024-10-29 19:49:13.535155: Epoch time: 311.72 s
2024-10-29 19:49:14.670337: Yayy! New best EMA pseudo Dice: 0.9198
2024-10-29 19:49:17.259518: 
2024-10-29 19:49:17.260811: Epoch 40
2024-10-29 19:49:17.261579: Current learning rate: 2e-05
2024-10-29 19:54:11.311151: train_loss -0.8851
2024-10-29 19:54:11.323212: val_loss -0.8712
2024-10-29 19:54:11.332108: Pseudo dice [0.9433, 0.9164, 0.9166]
2024-10-29 19:54:11.333459: Epoch time: 294.05 s
2024-10-29 19:54:11.342006: Yayy! New best EMA pseudo Dice: 0.9204
2024-10-29 19:54:15.183248: 
2024-10-29 19:54:15.190090: Epoch 41
2024-10-29 19:54:15.191082: Current learning rate: 2e-05
2024-10-29 19:59:25.078499: train_loss -0.8805
2024-10-29 19:59:25.090399: val_loss -0.8836
2024-10-29 19:59:25.091223: Pseudo dice [0.9455, 0.9302, 0.9259]
2024-10-29 19:59:25.092479: Epoch time: 309.9 s
2024-10-29 18:20:44.260491: Yayy! New best EMA pseudo Dice: 0.8935
2024-10-29 18:20:48.779228: 
2024-10-29 18:20:48.789264: Epoch 21
2024-10-29 18:20:48.790259: Current learning rate: 0.00612
2024-10-29 18:27:11.728665: train_loss -0.8465
2024-10-29 18:27:11.729869: val_loss -0.8576
2024-10-29 18:27:11.730759: Pseudo dice [0.9272, 0.92, 0.8993]
2024-10-29 18:27:11.732731: Epoch time: 382.95 s
2024-10-29 18:27:11.733570: Yayy! New best EMA pseudo Dice: 0.8957
2024-10-29 18:27:16.617805: 
2024-10-29 18:27:16.629328: Epoch 22
2024-10-29 18:27:16.630273: Current learning rate: 0.00593
2024-10-29 18:33:35.640728: train_loss -0.8482
2024-10-29 18:33:35.654996: val_loss -0.8444
2024-10-29 18:33:35.668639: Pseudo dice [0.9254, 0.9255, 0.9054]
2024-10-29 18:33:35.670482: Epoch time: 379.02 s
2024-10-29 18:33:35.671432: Yayy! New best EMA pseudo Dice: 0.898
2024-10-29 18:33:40.276786: 
2024-10-29 18:33:40.278420: Epoch 23
2024-10-29 18:33:40.284926: Current learning rate: 0.00574
2024-10-29 18:40:13.388690: train_loss -0.8559
2024-10-29 18:40:13.508133: val_loss -0.8484
2024-10-29 18:40:13.511930: Pseudo dice [0.9237, 0.93, 0.9007]
2024-10-29 18:40:13.694591: Epoch time: 393.11 s
2024-10-29 18:40:13.702414: Yayy! New best EMA pseudo Dice: 0.9
2024-10-29 18:40:18.507792: 
2024-10-29 18:40:18.509085: Epoch 24
2024-10-29 18:40:18.510207: Current learning rate: 0.00555
2024-10-29 18:46:54.291463: train_loss -0.8579
2024-10-29 18:46:54.310837: val_loss -0.844
2024-10-29 18:46:54.327477: Pseudo dice [0.931, 0.9194, 0.906]
2024-10-29 18:46:54.330133: Epoch time: 395.78 s
2024-10-29 18:46:58.877682: Yayy! New best EMA pseudo Dice: 0.9019
2024-10-29 18:47:03.700508: 
2024-10-29 18:47:03.701923: Epoch 25
2024-10-29 18:47:03.702957: Current learning rate: 0.00536
2024-10-29 18:53:24.513205: train_loss -0.8554
2024-10-29 18:53:24.528341: val_loss -0.861
2024-10-29 18:53:24.529716: Pseudo dice [0.9325, 0.9192, 0.9087]
2024-10-29 18:53:24.532934: Epoch time: 380.81 s
2024-10-29 18:53:24.534320: Yayy! New best EMA pseudo Dice: 0.9037
2024-10-29 18:53:36.598737: 
2024-10-29 18:53:36.606770: Epoch 26
2024-10-29 18:53:36.607868: Current learning rate: 0.00517
2024-10-29 18:59:54.889910: train_loss -0.8557
2024-10-29 18:59:54.905691: val_loss -0.8698
2024-10-29 18:59:54.906755: Pseudo dice [0.9355, 0.9317, 0.9079]
2024-10-29 18:59:54.908289: Epoch time: 378.29 s
2024-10-29 18:59:54.909221: Yayy! New best EMA pseudo Dice: 0.9058
2024-10-29 18:59:58.476702: 
2024-10-29 18:59:58.478116: Epoch 27
2024-10-29 18:59:58.479030: Current learning rate: 0.00497
2024-10-29 19:06:22.394665: train_loss -0.8578
2024-10-29 19:06:22.418545: val_loss -0.8726
2024-10-29 19:06:22.419909: Pseudo dice [0.9366, 0.9161, 0.9119]
2024-10-29 19:06:22.421682: Epoch time: 383.92 s
2024-10-29 19:06:22.422528: Yayy! New best EMA pseudo Dice: 0.9074
2024-10-29 19:06:27.617640: 
2024-10-29 19:06:27.624124: Epoch 28
2024-10-29 19:06:27.625423: Current learning rate: 0.00478
2024-10-29 19:12:35.613581: train_loss -0.8528
2024-10-29 19:12:35.627644: val_loss -0.8472
2024-10-29 19:12:35.628681: Pseudo dice [0.9298, 0.8964, 0.8926]
2024-10-29 19:12:35.630381: Epoch time: 368.0 s
2024-10-29 19:12:38.738338: 
2024-10-29 19:12:38.752990: Epoch 29
2024-10-29 19:12:38.753999: Current learning rate: 0.00458
2024-10-29 19:19:07.575358: train_loss -0.8394
2024-10-29 19:19:07.577462: val_loss -0.8666
2024-10-29 19:19:07.578709: Pseudo dice [0.9278, 0.922, 0.904]
2024-10-29 19:19:07.593373: Epoch time: 388.84 s
2024-10-29 19:19:09.213695: Yayy! New best EMA pseudo Dice: 0.9083
2024-10-29 19:19:13.299621: 
2024-10-29 19:19:13.301083: Epoch 30
2024-10-29 19:19:13.311445: Current learning rate: 0.00438
2024-10-29 19:25:31.058603: train_loss -0.8626
2024-10-29 19:25:31.225687: val_loss -0.8711
2024-10-29 19:25:31.229518: Pseudo dice [0.9382, 0.9309, 0.9105]
2024-10-29 19:25:31.231707: Epoch time: 377.69 s
2024-10-29 19:25:31.232833: Yayy! New best EMA pseudo Dice: 0.9102
2024-10-29 19:25:36.266323: 
2024-10-29 19:25:36.267489: Epoch 31
2024-10-29 19:25:36.268600: Current learning rate: 0.00419
2024-10-29 19:32:00.840442: train_loss -0.8665
2024-10-29 19:32:00.910184: val_loss -0.8639
2024-10-29 19:32:00.911654: Pseudo dice [0.9351, 0.9274, 0.9137]
2024-10-29 19:32:00.914569: Epoch time: 384.58 s
2024-10-29 19:32:00.915694: Yayy! New best EMA pseudo Dice: 0.9117
2024-10-29 19:32:05.098478: 
2024-10-29 19:32:05.099955: Epoch 32
2024-10-29 19:32:05.101008: Current learning rate: 0.00399
2024-10-29 19:38:49.965930: train_loss -0.8584
2024-10-29 19:38:49.967420: val_loss -0.8576
2024-10-29 19:38:49.968462: Pseudo dice [0.924, 0.9163, 0.9108]
2024-10-29 19:38:49.970078: Epoch time: 404.87 s
2024-10-29 19:38:49.974195: Yayy! New best EMA pseudo Dice: 0.9122
2024-10-29 19:38:56.363462: 
2024-10-29 19:38:56.365241: Epoch 33
2024-10-29 19:38:56.366567: Current learning rate: 0.00379
2024-10-29 19:45:12.577137: train_loss -0.8656
2024-10-29 19:45:12.583065: val_loss -0.8578
2024-10-29 19:45:12.584215: Pseudo dice [0.9346, 0.9175, 0.9097]
2024-10-29 19:45:12.586139: Epoch time: 376.21 s
2024-10-29 19:45:12.593726: Yayy! New best EMA pseudo Dice: 0.9131
2024-10-29 19:45:17.801450: 
2024-10-29 19:45:17.804031: Epoch 34
2024-10-29 19:45:17.805117: Current learning rate: 0.00359
2024-10-29 19:51:33.995791: train_loss -0.8714
2024-10-29 19:51:34.020870: val_loss -0.8657
2024-10-29 19:51:34.022103: Pseudo dice [0.9368, 0.9322, 0.9084]
2024-10-29 19:51:34.023952: Epoch time: 376.2 s
2024-10-29 19:51:35.592706: Yayy! New best EMA pseudo Dice: 0.9143
2024-10-29 19:51:40.069428: 
2024-10-29 19:51:40.070731: Epoch 35
2024-10-29 19:51:40.071708: Current learning rate: 0.00338
2024-10-29 19:58:05.227424: train_loss -0.862
2024-10-29 19:58:05.236459: val_loss -0.8607
2024-10-29 19:58:05.237423: Pseudo dice [0.9391, 0.9322, 0.9064]
2024-10-29 19:58:05.239324: Epoch time: 385.16 s
2024-10-29 19:58:05.247549: Yayy! New best EMA pseudo Dice: 0.9155
2024-10-29 19:58:10.835320: 
2024-10-29 19:58:10.836616: Epoch 36
2024-10-29 19:58:10.837503: Current learning rate: 0.00318
2024-10-29 20:04:39.828241: train_loss -0.8632
2024-10-29 20:04:39.838621: val_loss -0.8726
2024-10-29 20:04:39.841317: Pseudo dice [0.9411, 0.9226, 0.9148]
2024-10-29 20:04:39.843060: Epoch time: 388.99 s
2024-10-29 20:04:39.844010: Yayy! New best EMA pseudo Dice: 0.9166
2024-10-29 20:04:45.293423: 
2024-10-29 20:04:45.296113: Epoch 37
2024-10-29 20:04:45.297394: Current learning rate: 0.00297
2024-10-29 20:11:12.994966: train_loss -0.8718
2024-10-29 20:11:13.014442: val_loss -0.8475
2024-10-29 20:11:13.015340: Pseudo dice [0.9363, 0.9088, 0.9084]
2024-10-29 20:11:13.016822: Epoch time: 387.7 s
2024-10-29 20:11:13.017527: Yayy! New best EMA pseudo Dice: 0.9167
2024-10-29 20:11:17.610626: 
2024-10-29 20:11:17.624352: Epoch 38
2024-10-29 20:11:17.625745: Current learning rate: 0.00277
2024-10-29 20:17:53.373180: train_loss -0.8664
2024-10-29 20:17:53.385028: val_loss -0.8699
2024-10-29 20:17:53.385963: Pseudo dice [0.9406, 0.9268, 0.9143]
2024-10-29 20:17:53.387573: Epoch time: 395.76 s
2024-10-29 20:17:53.388328: Yayy! New best EMA pseudo Dice: 0.9177
2024-10-29 20:18:02.329309: 
2024-10-29 20:18:02.343009: Epoch 39
2024-10-29 20:18:02.344074: Current learning rate: 0.00256
2024-10-29 20:24:39.055601: train_loss -0.872
2024-10-29 20:24:39.058162: val_loss -0.862
2024-10-29 20:24:39.059327: Pseudo dice [0.936, 0.9264, 0.9122]
2024-10-29 20:24:39.078486: Epoch time: 396.73 s
2024-10-29 20:24:39.952161: Yayy! New best EMA pseudo Dice: 0.9185
2024-10-29 20:24:43.680120: 
2024-10-29 20:24:43.681268: Epoch 40
2024-10-29 20:24:43.688450: Current learning rate: 0.00235
2024-10-29 20:31:04.408261: train_loss -0.8762
2024-10-29 20:31:04.810161: val_loss -0.8828
2024-10-29 20:31:04.819386: Pseudo dice [0.9394, 0.9454, 0.9184]
2024-10-29 20:31:04.823528: Epoch time: 380.73 s
2024-10-29 20:31:04.824623: Yayy! New best EMA pseudo Dice: 0.9201
2024-10-29 20:31:10.765090: 
2024-10-29 20:31:10.774575: Epoch 41
2024-10-29 20:31:10.775936: Current learning rate: 0.00214
2024-10-29 20:37:28.846913: train_loss -0.8777
2024-10-29 20:37:28.859058: val_loss -0.8657
2024-10-29 20:37:28.860183: Pseudo dice [0.9407, 0.9342, 0.9147]
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         Average Dice ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: Best EMA pseudo Dice ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: enhancing tumor Dice ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   lr ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:        training_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: training_loss_per_it ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:      tumor core Dice ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà
wandb:             val_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     whole tumor Dice ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:         Average Dice 0.9394
wandb: Best EMA pseudo Dice 0.9269
wandb: enhancing tumor Dice 0.9286
wandb:                epoch 49
wandb:                   lr 0.0
wandb:        training_loss -0.8875
wandb: training_loss_per_it -0.91315
wandb:      tumor core Dice 0.9364
wandb:             val_loss -0.89859
wandb:     whole tumor Dice 0.9533
wandb: 
wandb: üöÄ View run BraTs_2023_SegResNet_3d_fullres_fold_0_2024.10.29_16:22:34 at: https://wandb.ai/arildus/BraTs_2023_SegResNet/runs/74vn9x57
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/arildus/BraTs_2023_SegResNet
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_162235-74vn9x57/logs
2024-10-29 19:59:25.093118: Yayy! New best EMA pseudo Dice: 0.9217
2024-10-29 19:59:29.308949: 
2024-10-29 19:59:29.320714: Epoch 42
2024-10-29 19:59:29.321663: Current learning rate: 2e-05
2024-10-29 20:04:29.915930: train_loss -0.8846
2024-10-29 20:04:29.930162: val_loss -0.8682
2024-10-29 20:04:29.944272: Pseudo dice [0.9329, 0.8973, 0.9087]
2024-10-29 20:04:29.945574: Epoch time: 300.61 s
2024-10-29 20:04:32.808894: 
2024-10-29 20:04:32.816137: Epoch 43
2024-10-29 20:04:32.838825: Current learning rate: 2e-05
2024-10-29 20:09:32.274689: train_loss -0.8916
2024-10-29 20:09:32.284461: val_loss -0.8728
2024-10-29 20:09:32.285255: Pseudo dice [0.9373, 0.9256, 0.914]
2024-10-29 20:09:32.286515: Epoch time: 299.47 s
2024-10-29 20:09:34.640803: 
2024-10-29 20:09:34.642863: Epoch 44
2024-10-29 20:09:34.643817: Current learning rate: 1e-05
2024-10-29 20:14:27.032191: train_loss -0.8859
2024-10-29 20:14:27.034669: val_loss -0.8848
2024-10-29 20:14:27.035609: Pseudo dice [0.9436, 0.9326, 0.923]
2024-10-29 20:14:27.039782: Epoch time: 292.39 s
2024-10-29 20:14:27.717466: Yayy! New best EMA pseudo Dice: 0.9225
2024-10-29 20:14:29.561660: 
2024-10-29 20:14:29.564998: Epoch 45
2024-10-29 20:14:29.565795: Current learning rate: 1e-05
2024-10-29 20:19:32.624008: train_loss -0.8864
2024-10-29 20:19:32.655868: val_loss -0.8779
2024-10-29 20:19:32.658787: Pseudo dice [0.9487, 0.9157, 0.92]
2024-10-29 20:19:32.663922: Epoch time: 303.06 s
2024-10-29 20:19:32.675669: Yayy! New best EMA pseudo Dice: 0.9231
2024-10-29 20:19:34.940964: 
2024-10-29 20:19:34.943958: Epoch 46
2024-10-29 20:19:34.952063: Current learning rate: 1e-05
2024-10-29 20:24:47.265405: train_loss -0.8925
2024-10-29 20:24:47.296458: val_loss -0.906
2024-10-29 20:24:47.309424: Pseudo dice [0.9459, 0.9443, 0.9276]
2024-10-29 20:24:47.329896: Epoch time: 312.33 s
2024-10-29 20:24:47.338945: Yayy! New best EMA pseudo Dice: 0.9247
2024-10-29 20:24:50.250115: 
2024-10-29 20:24:50.251810: Epoch 47
2024-10-29 20:24:50.252681: Current learning rate: 1e-05
2024-10-29 20:29:56.193153: train_loss -0.8866
2024-10-29 20:29:56.199200: val_loss -0.871
2024-10-29 20:29:56.200083: Pseudo dice [0.9457, 0.9142, 0.9154]
2024-10-29 20:29:56.201705: Epoch time: 305.94 s
2024-10-29 20:29:56.211541: Yayy! New best EMA pseudo Dice: 0.9247
2024-10-29 20:30:02.094436: 
2024-10-29 20:30:02.110693: Epoch 48
2024-10-29 20:30:02.124349: Current learning rate: 1e-05
2024-10-29 20:34:53.657622: train_loss -0.8927
2024-10-29 20:34:53.662036: val_loss -0.8841
2024-10-29 20:34:53.662788: Pseudo dice [0.9457, 0.9375, 0.9136]
2024-10-29 20:34:53.664000: Epoch time: 291.56 s
2024-10-29 20:34:53.664649: Yayy! New best EMA pseudo Dice: 0.9255
2024-10-29 20:34:57.043631: 
2024-10-29 20:34:57.055062: Epoch 49
2024-10-29 20:34:57.056053: Current learning rate: 0.0
2024-10-29 20:40:03.354402: train_loss -0.8875
2024-10-29 20:40:03.356167: val_loss -0.8986
2024-10-29 20:40:03.357056: Pseudo dice [0.9533, 0.9364, 0.9286]
2024-10-29 20:40:03.358512: Epoch time: 306.31 s
2024-10-29 20:40:03.359512: Yayy! New best EMA pseudo Dice: 0.9269
2024-10-29 20:40:11.545504: Training done.
2024-10-29 20:40:16.438060: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTs2021/splits_final.json
2024-10-29 20:40:16.450269: The split file contains 5 splits.
2024-10-29 20:40:16.450776: Desired fold for training: 0
2024-10-29 20:40:16.451243: This split has 1000 training and 251 validation cases.
2024-10-29 20:40:16.455525: predicting BraTS2021_00000
2024-10-29 20:40:43.832295: predicting BraTS2021_00009
2024-10-29 20:40:46.610704: predicting BraTS2021_00016
2024-10-29 20:40:49.265367: predicting BraTS2021_00024
2024-10-29 20:40:51.918543: predicting BraTS2021_00028
2024-10-29 20:40:54.571699: predicting BraTS2021_00031
2024-10-29 20:40:57.226388: predicting BraTS2021_00035
2024-10-29 20:40:59.876342: predicting BraTS2021_00045
2024-10-29 20:41:02.531924: predicting BraTS2021_00046
2024-10-29 20:41:05.183450: predicting BraTS2021_00051
2024-10-29 20:44:54.333542: predicting BraTS2021_00070
2024-10-29 20:44:57.057364: predicting BraTS2021_00078
2024-10-29 20:44:59.710876: predicting BraTS2021_00085
2024-10-29 20:45:02.365033: predicting BraTS2021_00087
2024-10-29 20:45:05.017950: predicting BraTS2021_00088
2024-10-29 20:45:07.668573: predicting BraTS2021_00089
2024-10-29 20:45:10.322692: predicting BraTS2021_00099
2024-10-29 20:45:12.974955: predicting BraTS2021_00102
2024-10-29 20:45:15.627358: predicting BraTS2021_00104
2024-10-29 20:45:18.279876: predicting BraTS2021_00106
2024-10-29 20:45:20.934530: predicting BraTS2021_00107
2024-10-29 20:45:23.588732: predicting BraTS2021_00110
2024-10-29 20:45:26.241302: predicting BraTS2021_00117
2024-10-29 20:45:28.894441: predicting BraTS2021_00123
2024-10-29 20:45:31.548987: predicting BraTS2021_00126
2024-10-29 20:45:34.202606: predicting BraTS2021_00127
2024-10-29 20:45:36.857951: predicting BraTS2021_00130
2024-10-29 20:45:39.509239: predicting BraTS2021_00136
2024-10-29 20:45:42.163241: predicting BraTS2021_00137
2024-10-29 20:45:44.814668: predicting BraTS2021_00140
2024-10-29 20:45:47.467245: predicting BraTS2021_00144
2024-10-29 20:45:50.126655: predicting BraTS2021_00152
2024-10-29 20:45:52.774699: predicting BraTS2021_00160
2024-10-29 20:45:55.425939: predicting BraTS2021_00185
2024-10-29 20:45:58.078621: predicting BraTS2021_00192
2024-10-29 20:46:00.730502: predicting BraTS2021_00194
2024-10-29 20:46:02.096023: predicting BraTS2021_00211
2024-10-29 20:46:04.747649: predicting BraTS2021_00212
2024-10-29 20:46:07.400887: predicting BraTS2021_00214
2024-10-29 20:46:10.055926: predicting BraTS2021_00217
2024-10-29 20:46:12.709222: predicting BraTS2021_00218
2024-10-29 20:46:15.365625: predicting BraTS2021_00231
2024-10-29 20:46:18.016721: predicting BraTS2021_00243
2024-10-29 20:46:20.668694: predicting BraTS2021_00251
2024-10-29 20:46:23.321664: predicting BraTS2021_00266
2024-10-29 20:46:25.972863: predicting BraTS2021_00270
2024-10-29 20:46:28.623353: predicting BraTS2021_00274
2024-10-29 20:46:31.276198: predicting BraTS2021_00283
2024-10-29 20:46:33.928872: predicting BraTS2021_00286
2024-10-29 20:46:36.583477: predicting BraTS2021_00292
2024-10-29 20:46:39.237233: predicting BraTS2021_00294
2024-10-29 20:46:41.889930: predicting BraTS2021_00304
2024-10-29 20:46:44.543731: predicting BraTS2021_00311
2024-10-29 20:46:47.195398: predicting BraTS2021_00313
2024-10-29 20:46:49.847965: predicting BraTS2021_00334
2024-10-29 20:46:52.500967: predicting BraTS2021_00336
2024-10-29 20:46:55.152725: predicting BraTS2021_00350
2024-10-29 20:46:57.804248: predicting BraTS2021_00353
2024-10-29 20:47:00.478215: predicting BraTS2021_00366
2024-10-29 20:47:03.129903: predicting BraTS2021_00375
2024-10-29 20:47:05.783700: predicting BraTS2021_00376
2024-10-29 20:47:08.437269: predicting BraTS2021_00379
2024-10-29 20:47:11.089702: predicting BraTS2021_00391
2024-10-29 20:47:13.742725: predicting BraTS2021_00399
2024-10-29 20:47:16.396128: predicting BraTS2021_00402
2024-10-29 20:47:17.760296: predicting BraTS2021_00419
2024-10-29 20:47:20.413415: predicting BraTS2021_00423
2024-10-29 20:47:23.065420: predicting BraTS2021_00426
2024-10-29 20:47:25.719279: predicting BraTS2021_00430
2024-10-29 20:47:28.369635: predicting BraTS2021_00433
2024-10-29 20:47:31.023904: predicting BraTS2021_00436
2024-10-29 20:47:33.674773: predicting BraTS2021_00442
2024-10-29 20:47:36.328411: predicting BraTS2021_00464
2024-10-29 20:47:38.984957: predicting BraTS2021_00480
2024-10-29 20:47:41.638039: predicting BraTS2021_00485
2024-10-29 20:47:44.289830: predicting BraTS2021_00495
2024-10-29 20:47:46.974257: predicting BraTS2021_00500
2024-10-29 20:47:49.628812: predicting BraTS2021_00514
2024-10-29 20:47:52.282903: predicting BraTS2021_00516
2024-10-29 20:47:54.935046: predicting BraTS2021_00549
2024-10-29 20:47:57.588115: predicting BraTS2021_00558
2024-10-29 20:48:00.242526: predicting BraTS2021_00583
2024-10-29 20:48:02.894910: predicting BraTS2021_00586
2024-10-29 20:48:05.547671: predicting BraTS2021_00587
2024-10-29 20:48:08.206726: predicting BraTS2021_00597
2024-10-29 20:48:10.861669: predicting BraTS2021_00602
2024-10-29 20:48:13.511396: predicting BraTS2021_00605
2024-10-29 20:48:16.166706: predicting BraTS2021_00607
2024-10-29 20:48:18.819225: predicting BraTS2021_00630
2024-10-29 20:48:21.473759: predicting BraTS2021_00639
2024-10-29 20:48:24.124369: predicting BraTS2021_00641
2024-10-29 20:48:26.777707: predicting BraTS2021_00654
2024-10-29 20:48:29.432394: predicting BraTS2021_00693
2024-10-29 20:48:32.083290: predicting BraTS2021_00709
2024-10-29 20:48:34.734252: predicting BraTS2021_00715
2024-10-29 20:48:37.387810: predicting BraTS2021_00716
2024-10-29 20:48:40.041741: predicting BraTS2021_00727
2024-10-29 20:48:42.694647: predicting BraTS2021_00733
2024-10-29 20:48:45.347417: predicting BraTS2021_00735
2024-10-29 20:48:47.999725: predicting BraTS2021_00753
2024-10-29 20:48:50.651433: predicting BraTS2021_00757
2024-10-29 20:48:53.304849: predicting BraTS2021_00759
2024-10-29 20:48:55.957404: predicting BraTS2021_00772
2024-10-29 20:48:58.612099: predicting BraTS2021_00775
2024-10-29 20:49:01.263749: predicting BraTS2021_00777
2024-10-29 20:49:03.915943: predicting BraTS2021_00782
2024-10-29 20:49:06.570119: predicting BraTS2021_00791
2024-10-29 20:49:09.221352: predicting BraTS2021_00793
2024-10-29 20:49:11.873721: predicting BraTS2021_00810
2024-10-29 20:49:14.527826: predicting BraTS2021_00818
2024-10-29 20:49:17.181886: predicting BraTS2021_00834
2024-10-29 20:49:19.834563: predicting BraTS2021_00836
2024-10-29 20:49:22.487530: predicting BraTS2021_01001
2024-10-29 20:49:23.852363: predicting BraTS2021_01004
2024-10-29 20:49:26.502469: predicting BraTS2021_01005
2024-10-29 20:49:29.155259: predicting BraTS2021_01018
2024-10-29 20:49:31.808242: predicting BraTS2021_01037
2024-10-29 20:49:34.463027: predicting BraTS2021_01043
2024-10-29 20:49:37.114151: predicting BraTS2021_01050
2024-10-29 20:49:39.766135: predicting BraTS2021_01051
2024-10-29 20:49:42.419595: predicting BraTS2021_01058
2024-10-29 20:49:45.071358: predicting BraTS2021_01063
2024-10-29 20:49:47.723561: predicting BraTS2021_01066
2024-10-29 20:49:50.376834: predicting BraTS2021_01072
2024-10-29 20:49:51.778227: predicting BraTS2021_01079
2024-10-29 20:49:54.433824: predicting BraTS2021_01081
2024-10-29 20:49:57.084935: predicting BraTS2021_01088
2024-10-29 20:49:59.736671: predicting BraTS2021_01089
2024-10-29 20:50:02.388933: predicting BraTS2021_01096
2024-10-29 20:50:05.041329: predicting BraTS2021_01101
2024-10-29 20:50:07.694051: predicting BraTS2021_01110
2024-10-29 20:50:10.346014: predicting BraTS2021_01118
2024-10-29 20:50:12.998316: predicting BraTS2021_01123
2024-10-29 20:50:15.652412: predicting BraTS2021_01124
2024-10-29 20:50:18.303679: predicting BraTS2021_01137
2024-10-29 20:50:19.667906: predicting BraTS2021_01141
2024-10-29 20:50:22.320970: predicting BraTS2021_01142
2024-10-29 20:50:24.974342: predicting BraTS2021_01146
2024-10-29 20:50:27.625130: predicting BraTS2021_01147
2024-10-29 20:50:30.277236: predicting BraTS2021_01152
2024-10-29 20:50:32.928600: predicting BraTS2021_01162
2024-10-29 20:50:35.582511: predicting BraTS2021_01169
2024-10-29 20:50:36.945513: predicting BraTS2021_01170
2024-10-29 20:50:39.596934: predicting BraTS2021_01173
2024-10-29 20:50:42.247838: predicting BraTS2021_01181
2024-10-29 20:50:43.610834: predicting BraTS2021_01183
2024-10-29 20:50:45.011180: predicting BraTS2021_01191
2024-10-29 20:50:47.663319: predicting BraTS2021_01198
2024-10-29 20:50:49.064414: predicting BraTS2021_01206
2024-10-29 20:50:51.717948: predicting BraTS2021_01208
2024-10-29 20:50:54.372826: predicting BraTS2021_01209
2024-10-29 20:50:57.025126: predicting BraTS2021_01210
2024-10-29 20:50:59.677593: predicting BraTS2021_01211
2024-10-29 20:51:02.332296: predicting BraTS2021_01212
2024-10-29 20:51:04.985945: predicting BraTS2021_01225
2024-10-29 20:51:07.638561: predicting BraTS2021_01231
2024-10-29 20:51:10.290257: predicting BraTS2021_01237
2024-10-29 20:51:12.944731: predicting BraTS2021_01238
2024-10-29 20:51:15.596090: predicting BraTS2021_01239
2024-10-29 20:51:16.999125: predicting BraTS2021_01240
2024-10-29 20:51:19.651707: predicting BraTS2021_01262
2024-10-29 20:51:21.014760: predicting BraTS2021_01265
2024-10-29 20:51:23.669665: predicting BraTS2021_01273
2024-10-29 20:51:26.322607: predicting BraTS2021_01274
2024-10-29 20:51:28.973944: predicting BraTS2021_01282
2024-10-29 20:51:31.628086: predicting BraTS2021_01283
2024-10-29 20:51:34.283728: predicting BraTS2021_01284
2024-10-29 20:51:36.936388: predicting BraTS2021_01289
2024-10-29 20:51:39.590180: predicting BraTS2021_01294
2024-10-29 20:51:42.243052: predicting BraTS2021_01296
2024-10-29 20:51:44.895635: predicting BraTS2021_01305
2024-10-29 20:51:47.548283: predicting BraTS2021_01306
2024-10-29 20:51:50.199869: predicting BraTS2021_01312
2024-10-29 20:51:52.855551: predicting BraTS2021_01316
2024-10-29 20:51:54.218969: predicting BraTS2021_01319
2024-10-29 20:51:56.871363: predicting BraTS2021_01320
2024-10-29 20:51:59.525841: predicting BraTS2021_01326
2024-10-29 20:52:02.179282: predicting BraTS2021_01328
2024-10-29 20:52:04.833494: predicting BraTS2021_01331
2024-10-29 20:52:07.486151: predicting BraTS2021_01333
2024-10-29 20:52:08.850393: predicting BraTS2021_01335
2024-10-29 20:52:11.504450: predicting BraTS2021_01339
2024-10-29 20:52:14.159866: predicting BraTS2021_01340
2024-10-29 20:52:16.810021: predicting BraTS2021_01342
2024-10-29 20:52:19.510936: predicting BraTS2021_01345
2024-10-29 20:52:22.163188: predicting BraTS2021_01349
2024-10-29 20:52:24.815928: predicting BraTS2021_01350
2024-10-29 20:52:27.466870: predicting BraTS2021_01356
2024-10-29 20:52:30.119518: predicting BraTS2021_01357
2024-10-29 20:52:32.771348: predicting BraTS2021_01359
2024-10-29 20:52:35.424157: predicting BraTS2021_01361
2024-10-29 20:52:38.078368: predicting BraTS2021_01363
2024-10-29 20:52:39.442735: predicting BraTS2021_01367
2024-10-29 20:52:42.094650: predicting BraTS2021_01375
2024-10-29 20:52:43.494274: predicting BraTS2021_01387
2024-10-29 20:52:46.148288: predicting BraTS2021_01388
2024-10-29 20:52:48.803562: predicting BraTS2021_01389
2024-10-29 20:52:51.456661: predicting BraTS2021_01398
2024-10-29 20:52:54.110063: predicting BraTS2021_01399
2024-10-29 20:52:55.474457: predicting BraTS2021_01402
2024-10-29 20:52:58.128218: predicting BraTS2021_01423
2024-10-29 20:53:00.780541: predicting BraTS2021_01425
2024-10-29 20:53:03.435234: predicting BraTS2021_01440
2024-10-29 20:53:06.086344: predicting BraTS2021_01443
2024-10-29 20:53:08.740351: predicting BraTS2021_01450
2024-10-29 20:53:11.392603: predicting BraTS2021_01461
2024-10-29 20:53:14.045403: predicting BraTS2021_01465
2024-10-29 20:53:16.697844: predicting BraTS2021_01467
2024-10-29 20:53:19.365325: predicting BraTS2021_01476
2024-10-29 20:53:22.018001: predicting BraTS2021_01480
2024-10-29 20:53:24.670099: predicting BraTS2021_01481
2024-10-29 20:53:27.325023: predicting BraTS2021_01488
2024-10-29 20:53:29.978313: predicting BraTS2021_01489
2024-10-29 20:53:32.632073: predicting BraTS2021_01490
2024-10-29 20:53:35.282795: predicting BraTS2021_01492
2024-10-29 20:53:37.934474: predicting BraTS2021_01494
2024-10-29 20:53:40.586450: predicting BraTS2021_01496
2024-10-29 20:53:43.242454: predicting BraTS2021_01500
2024-10-29 20:53:45.893128: predicting BraTS2021_01501
2024-10-29 20:53:48.546794: predicting BraTS2021_01507
2024-10-29 20:53:51.199878: predicting BraTS2021_01513
2024-10-29 20:53:53.853233: predicting BraTS2021_01518
2024-10-29 20:53:56.506066: predicting BraTS2021_01521
2024-10-29 20:53:59.159118: predicting BraTS2021_01522
2024-10-29 20:54:01.812465: predicting BraTS2021_01524
2024-10-29 20:54:04.466363: predicting BraTS2021_01530
2024-10-29 20:54:07.117419: predicting BraTS2021_01538
2024-10-29 20:54:09.770060: predicting BraTS2021_01540
2024-10-29 20:54:12.421820: predicting BraTS2021_01547
2024-10-29 20:54:15.075251: predicting BraTS2021_01560
2024-10-29 20:54:17.724775: predicting BraTS2021_01562
2024-10-29 20:54:20.377528: predicting BraTS2021_01566
2024-10-29 20:54:23.032009: predicting BraTS2021_01567
2024-10-29 20:54:25.684105: predicting BraTS2021_01582
2024-10-29 20:54:28.336588: predicting BraTS2021_01584
2024-10-29 20:54:30.989489: predicting BraTS2021_01593
2024-10-29 20:54:32.390634: predicting BraTS2021_01594
2024-10-29 20:54:35.042692: predicting BraTS2021_01599
2024-10-29 20:54:37.693453: predicting BraTS2021_01601
2024-10-29 20:54:40.347976: predicting BraTS2021_01608
2024-10-29 20:54:43.003681: predicting BraTS2021_01609
2024-10-29 20:54:45.655401: predicting BraTS2021_01613
2024-10-29 20:54:48.309662: predicting BraTS2021_01614
2024-10-29 20:54:50.963098: predicting BraTS2021_01626
2024-10-29 20:54:53.615123: predicting BraTS2021_01634
2024-10-29 20:54:56.268724: predicting BraTS2021_01651
2024-10-29 20:54:58.921266: predicting BraTS2021_01652
2024-10-29 20:55:01.573675: predicting BraTS2021_01653
2024-10-29 20:55:02.935369: predicting BraTS2021_01660
2024-10-29 20:55:04.336027: predicting BraTS2021_01661
2024-10-29 20:55:05.699917: predicting BraTS2021_01662
2024-10-29 20:55:56.406917: Validation complete
2024-10-29 20:55:56.407548: Mean Validation Dice:  0.9005898937976559
using pin_memory on device 0
2024-10-29 18:08:01.795834: train_loss -0.5414
2024-10-29 18:08:01.810973: val_loss -0.7478
2024-10-29 18:08:01.811740: Pseudo dice [0.8692, 0.7721, 0.8257]
2024-10-29 18:08:01.813274: Epoch time: 616.89 s
2024-10-29 18:08:01.813954: Yayy! New best EMA pseudo Dice: 0.8224
2024-10-29 18:08:10.903705: 
2024-10-29 18:08:10.904622: Epoch 1
2024-10-29 18:08:10.905349: Current learning rate: 0.00982
2024-10-29 18:16:29.198858: train_loss -0.7338
2024-10-29 18:16:29.200477: val_loss -0.7382
2024-10-29 18:16:29.201354: Pseudo dice [0.8594, 0.7464, 0.8326]
2024-10-29 18:16:29.203093: Epoch time: 498.3 s
2024-10-29 18:16:30.726172: 
2024-10-29 18:16:30.727350: Epoch 2
2024-10-29 18:16:30.728120: Current learning rate: 0.00964
2024-10-29 18:24:49.596054: train_loss -0.7535
2024-10-29 18:24:49.597530: val_loss -0.7828
2024-10-29 18:24:49.598500: Pseudo dice [0.8937, 0.8349, 0.8569]
2024-10-29 18:24:49.600274: Epoch time: 498.87 s
2024-10-29 18:24:49.601059: Yayy! New best EMA pseudo Dice: 0.8254
2024-10-29 18:24:52.384683: 
2024-10-29 18:24:52.385771: Epoch 3
2024-10-29 18:24:52.386477: Current learning rate: 0.00946
2024-10-29 18:33:11.962157: train_loss -0.772
2024-10-29 18:33:11.963656: val_loss -0.8048
2024-10-29 18:33:11.964474: Pseudo dice [0.9185, 0.8648, 0.8666]
2024-10-29 18:33:11.966020: Epoch time: 499.58 s
2024-10-29 18:33:11.966782: Yayy! New best EMA pseudo Dice: 0.8312
2024-10-29 18:33:14.441003: 
2024-10-29 18:33:14.442104: Epoch 4
2024-10-29 18:33:14.442880: Current learning rate: 0.00928
2024-10-29 18:41:33.628255: train_loss -0.798
2024-10-29 18:41:33.629975: val_loss -0.7819
2024-10-29 18:41:33.630846: Pseudo dice [0.887, 0.8234, 0.8329]
2024-10-29 18:41:33.632416: Epoch time: 499.19 s
2024-10-29 18:41:34.964303: Yayy! New best EMA pseudo Dice: 0.8329
2024-10-29 18:41:37.494574: 
2024-10-29 18:41:37.495618: Epoch 5
2024-10-29 18:41:37.496352: Current learning rate: 0.0091
2024-10-29 18:49:57.947521: train_loss -0.7877
2024-10-29 18:49:57.949444: val_loss -0.8367
2024-10-29 18:49:57.950417: Pseudo dice [0.9188, 0.8863, 0.8626]
2024-10-29 18:49:57.952044: Epoch time: 500.46 s
2024-10-29 18:49:57.952833: Yayy! New best EMA pseudo Dice: 0.8385
2024-10-29 18:50:00.762398: 
2024-10-29 18:50:00.763969: Epoch 6
2024-10-29 18:50:00.764800: Current learning rate: 0.00891
2024-10-29 18:58:22.140952: train_loss -0.8254
2024-10-29 18:58:22.229346: val_loss -0.8315
2024-10-29 18:58:22.230173: Pseudo dice [0.9231, 0.8915, 0.8862]
2024-10-29 18:58:22.231571: Epoch time: 501.38 s
2024-10-29 18:58:22.232271: Yayy! New best EMA pseudo Dice: 0.8447
2024-10-29 18:58:25.018006: 
2024-10-29 18:58:25.019166: Epoch 7
2024-10-29 18:58:25.020012: Current learning rate: 0.00873
2024-10-29 19:06:48.944552: train_loss -0.8305
2024-10-29 19:06:48.946721: val_loss -0.8377
2024-10-29 19:06:48.947712: Pseudo dice [0.9228, 0.8976, 0.8874]
2024-10-29 19:06:48.949332: Epoch time: 503.93 s
2024-10-29 19:06:48.950163: Yayy! New best EMA pseudo Dice: 0.8505
2024-10-29 19:06:52.584949: 
2024-10-29 19:06:52.586000: Epoch 8
2024-10-29 19:06:52.586793: Current learning rate: 0.00855
2024-10-29 19:15:33.443591: train_loss -0.8373
2024-10-29 19:15:33.619669: val_loss -0.8703
2024-10-29 19:15:33.620453: Pseudo dice [0.9269, 0.9192, 0.9038]
2024-10-29 19:15:33.684760: Epoch time: 520.86 s
2024-10-29 19:15:33.685623: Yayy! New best EMA pseudo Dice: 0.8571
2024-10-29 19:15:37.650471: 
2024-10-29 19:15:37.651582: Epoch 9
2024-10-29 19:15:37.652356: Current learning rate: 0.00836
2024-10-29 19:24:02.717274: train_loss -0.8402
2024-10-29 19:24:02.718876: val_loss -0.8461
2024-10-29 19:24:02.719721: Pseudo dice [0.9231, 0.8885, 0.8906]
2024-10-29 19:24:02.721238: Epoch time: 505.07 s
2024-10-29 19:24:03.710576: Yayy! New best EMA pseudo Dice: 0.8615
2024-10-29 19:24:06.048291: 
2024-10-29 19:24:06.049370: Epoch 10
2024-10-29 19:24:06.050143: Current learning rate: 0.00818
2024-10-29 19:32:25.338450: train_loss -0.8307
2024-10-29 19:32:25.340070: val_loss -0.838
2024-10-29 19:32:25.341055: Pseudo dice [0.9247, 0.8986, 0.8972]
2024-10-29 19:32:25.342576: Epoch time: 499.29 s
2024-10-29 19:32:25.343338: Yayy! New best EMA pseudo Dice: 0.866
2024-10-29 19:32:27.854520: 
2024-10-29 19:32:27.855587: Epoch 11
2024-10-29 19:32:27.856369: Current learning rate: 0.008
2024-10-29 19:40:51.689265: train_loss -0.834
2024-10-29 19:40:51.690963: val_loss -0.8554
2024-10-29 19:40:51.691934: Pseudo dice [0.9267, 0.904, 0.8864]
2024-10-29 19:40:51.693708: Epoch time: 503.84 s
2024-10-29 19:40:51.694608: Yayy! New best EMA pseudo Dice: 0.87
2024-10-29 19:40:54.347869: 
2024-10-29 19:40:54.349019: Epoch 12
2024-10-29 19:40:54.349778: Current learning rate: 0.00781
2024-10-29 19:49:16.391098: train_loss -0.8401
2024-10-29 19:49:16.392839: val_loss -0.8387
2024-10-29 19:49:16.393801: Pseudo dice [0.9157, 0.8804, 0.889]
2024-10-29 19:49:16.395554: Epoch time: 502.05 s
2024-10-29 19:49:16.396437: Yayy! New best EMA pseudo Dice: 0.8725
2024-10-29 19:49:19.069346: 
2024-10-29 19:49:19.070462: Epoch 13
2024-10-29 19:49:19.071250: Current learning rate: 0.00763
2024-10-29 19:57:40.935499: train_loss -0.8451
2024-10-29 19:57:40.937213: val_loss -0.8363
2024-10-29 19:57:40.938137: Pseudo dice [0.9195, 0.9128, 0.9023]
2024-10-29 19:57:40.939814: Epoch time: 501.87 s
2024-10-29 19:57:40.940638: Yayy! New best EMA pseudo Dice: 0.8764
2024-10-29 19:57:43.376979: 
2024-10-29 19:57:43.378044: Epoch 14
2024-10-29 19:57:43.378788: Current learning rate: 0.00744
2024-10-29 20:06:05.304888: train_loss -0.8415
2024-10-29 20:06:05.306909: val_loss -0.8718
2024-10-29 20:06:05.307856: Pseudo dice [0.9344, 0.9265, 0.9021]
2024-10-29 20:06:05.309393: Epoch time: 501.93 s
2024-10-29 20:06:06.276244: Yayy! New best EMA pseudo Dice: 0.8808
2024-10-29 20:06:09.157790: 
2024-10-29 20:06:09.158784: Epoch 15
2024-10-29 20:06:09.159597: Current learning rate: 0.00725
2024-10-29 20:14:31.833734: train_loss -0.8527
2024-10-29 20:14:31.835433: val_loss -0.8668
2024-10-29 20:14:31.836313: Pseudo dice [0.9302, 0.93, 0.9064]
2024-10-29 20:14:31.837922: Epoch time: 502.68 s
2024-10-29 20:14:31.838704: Yayy! New best EMA pseudo Dice: 0.885
2024-10-29 20:14:34.252364: 
2024-10-29 20:14:34.253595: Epoch 16
2024-10-29 20:14:34.254373: Current learning rate: 0.00707
2024-10-29 20:23:09.219601: train_loss -0.8521
2024-10-29 20:23:09.367949: val_loss -0.8563
2024-10-29 20:23:09.368890: Pseudo dice [0.9318, 0.917, 0.8959]
2024-10-29 20:23:09.526226: Epoch time: 514.95 s
2024-10-29 20:23:09.527045: Yayy! New best EMA pseudo Dice: 0.888
2024-10-29 20:23:14.400527: 
2024-10-29 20:23:14.401661: Epoch 17
2024-10-29 20:23:14.402379: Current learning rate: 0.00688
2024-10-29 20:31:35.753380: train_loss -0.8429
2024-10-29 20:31:35.755375: val_loss -0.8576
2024-10-29 20:31:35.756309: Pseudo dice [0.9315, 0.9081, 0.911]
2024-10-29 20:31:35.757759: Epoch time: 501.35 s
2024-10-29 20:31:35.758503: Yayy! New best EMA pseudo Dice: 0.8909
2024-10-29 20:31:38.402220: 
2024-10-29 20:31:38.403261: Epoch 18
2024-10-29 20:31:38.404060: Current learning rate: 0.00669
2024-10-29 20:39:59.190592: train_loss -0.8563
2024-10-29 20:39:59.192312: val_loss -0.8607
2024-10-29 20:39:59.193364: Pseudo dice [0.9339, 0.9113, 0.8964]
2024-10-29 20:39:59.195053: Epoch time: 500.79 s
2024-10-29 20:39:59.195820: Yayy! New best EMA pseudo Dice: 0.8932
2024-10-29 20:40:01.728086: 
2024-10-29 20:40:01.729135: Epoch 19
2024-10-29 20:40:01.729858: Current learning rate: 0.0065
2024-10-29 20:48:23.694381: train_loss -0.8425
2024-10-29 20:48:23.696081: val_loss -0.8596
2024-10-29 20:48:23.697401: Pseudo dice [0.9251, 0.9341, 0.9061]
2024-10-29 20:48:23.698977: Epoch time: 501.97 s
2024-10-29 20:48:24.779906: Yayy! New best EMA pseudo Dice: 0.896
2024-10-29 20:48:27.353806: 
2024-10-29 20:48:27.354980: Epoch 20
2024-10-29 20:48:27.355784: Current learning rate: 0.00631
2024-10-29 20:56:48.902446: train_loss -0.8529
2024-10-29 20:56:48.910283: val_loss -0.8918
2024-10-29 20:56:48.911121: Pseudo dice [0.9421, 0.924, 0.9159]
2024-10-29 20:56:48.912693: Epoch time: 501.55 s
2024-10-29 20:56:48.913416: Yayy! New best EMA pseudo Dice: 0.8992
2024-10-29 20:56:51.836520: 
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:         Average Dice ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: Best EMA pseudo Dice ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: enhancing tumor Dice ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   lr ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:        training_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: training_loss_per_it ‚ñà‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÅ
wandb:      tumor core Dice ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà
wandb:             val_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:     whole tumor Dice ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:         Average Dice 0.9347
wandb: Best EMA pseudo Dice 0.9266
wandb: enhancing tumor Dice 0.9202
wandb:                epoch 49
wandb:                   lr 0.0003
wandb:        training_loss -0.88217
wandb: training_loss_per_it -0.9322
wandb:      tumor core Dice 0.9362
wandb:             val_loss -0.8902
wandb:     whole tumor Dice 0.9478
wandb: 
wandb: üöÄ View run BraTs_2023_nnUNet_3d_fullres_fold_0_2024.10.29_16:02:43 at: https://wandb.ai/arildus/BraTs_2023_nnUNet/runs/u9nz8hh4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/arildus/BraTs_2023_nnUNet
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_160243-u9nz8hh4/logs
2024-10-29 20:37:28.864200: Epoch time: 378.08 s
2024-10-29 20:37:28.865448: Yayy! New best EMA pseudo Dice: 0.921
2024-10-29 20:37:34.004370: 
2024-10-29 20:37:34.017870: Epoch 42
2024-10-29 20:37:34.018925: Current learning rate: 0.00192
2024-10-29 20:44:00.483178: train_loss -0.877
2024-10-29 20:44:00.485692: val_loss -0.8665
2024-10-29 20:44:00.486829: Pseudo dice [0.937, 0.9231, 0.9091]
2024-10-29 20:44:00.489007: Epoch time: 386.48 s
2024-10-29 20:44:00.490717: Yayy! New best EMA pseudo Dice: 0.9212
2024-10-29 20:44:04.726385: 
2024-10-29 20:44:04.727836: Epoch 43
2024-10-29 20:44:04.728877: Current learning rate: 0.0017
2024-10-29 20:50:25.799774: train_loss -0.8728
2024-10-29 20:50:25.814176: val_loss -0.864
2024-10-29 20:50:25.815070: Pseudo dice [0.943, 0.9231, 0.9142]
2024-10-29 20:50:25.816617: Epoch time: 381.07 s
2024-10-29 20:50:25.820492: Yayy! New best EMA pseudo Dice: 0.9218
2024-10-29 20:50:30.652104: 
2024-10-29 20:50:30.653584: Epoch 44
2024-10-29 20:50:30.654668: Current learning rate: 0.00148
2024-10-29 20:56:48.775255: train_loss -0.8841
2024-10-29 20:56:48.787478: val_loss -0.8698
2024-10-29 20:56:48.788523: Pseudo dice [0.942, 0.9195, 0.9199]
2024-10-29 20:56:48.790545: Epoch time: 378.12 s
2024-10-29 20:56:50.071637: Yayy! New best EMA pseudo Dice: 0.9223
2024-10-29 20:56:54.129734: 
2024-10-29 20:56:54.140414: Epoch 45
2024-10-29 20:56:54.141664: Current learning rate: 0.00126
2024-10-29 21:03:24.318246: train_loss -0.8713
2024-10-29 21:03:24.337294: val_loss -0.8695
2024-10-29 21:03:24.338227: Pseudo dice [0.9442, 0.937, 0.9237]
2024-10-29 21:03:24.339910: Epoch time: 390.19 s
2024-10-29 21:03:24.340883: Yayy! New best EMA pseudo Dice: 0.9236
2024-10-29 21:03:29.880668: 
2024-10-29 21:03:29.882216: Epoch 46
2024-10-29 21:03:29.883227: Current learning rate: 0.00103
2024-10-29 21:09:53.644964: train_loss -0.8836
2024-10-29 21:09:53.661504: val_loss -0.87
2024-10-29 21:09:53.669823: Pseudo dice [0.9418, 0.9286, 0.9191]
2024-10-29 21:09:53.671524: Epoch time: 383.77 s
2024-10-29 21:09:53.672537: Yayy! New best EMA pseudo Dice: 0.9242
2024-10-29 21:09:58.512845: 
2024-10-29 21:09:58.514397: Epoch 47
2024-10-29 21:09:58.515526: Current learning rate: 0.00079
2024-10-29 21:16:24.599392: train_loss -0.8755
2024-10-29 21:16:24.622466: val_loss -0.874
2024-10-29 21:16:24.623616: Pseudo dice [0.942, 0.9362, 0.9188]
2024-10-29 21:16:24.625360: Epoch time: 386.09 s
2024-10-29 21:16:24.626221: Yayy! New best EMA pseudo Dice: 0.925
2024-10-29 21:16:29.288499: 
2024-10-29 21:16:29.290309: Epoch 48
2024-10-29 21:16:29.291511: Current learning rate: 0.00055
2024-10-29 21:23:09.431163: train_loss -0.8875
2024-10-29 21:23:09.509725: val_loss -0.8958
2024-10-29 21:23:09.510735: Pseudo dice [0.9421, 0.9363, 0.9179]
2024-10-29 21:23:09.513540: Epoch time: 400.13 s
2024-10-29 21:23:09.514827: Yayy! New best EMA pseudo Dice: 0.9257
2024-10-29 21:23:15.209413: 
2024-10-29 21:23:15.210814: Epoch 49
2024-10-29 21:23:15.211891: Current learning rate: 0.0003
2024-10-29 21:29:39.156117: train_loss -0.8822
2024-10-29 21:29:39.176077: val_loss -0.8902
2024-10-29 21:29:39.183305: Pseudo dice [0.9478, 0.9362, 0.9202]
2024-10-29 21:29:39.187259: Epoch time: 383.95 s
2024-10-29 21:29:39.188490: Yayy! New best EMA pseudo Dice: 0.9266
2024-10-29 21:29:45.056282: Training done.
2024-10-29 21:29:50.034295: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTs2021/splits_final.json
2024-10-29 21:29:50.106046: The split file contains 5 splits.
2024-10-29 21:29:50.106608: Desired fold for training: 0
2024-10-29 21:29:50.107113: This split has 1000 training and 251 validation cases.
2024-10-29 21:29:50.112079: predicting BraTS2021_00000
2024-10-29 21:31:16.211200: predicting BraTS2021_00009
2024-10-29 21:31:17.569006: predicting BraTS2021_00016
2024-10-29 21:31:18.907453: predicting BraTS2021_00024
2024-10-29 21:31:20.252745: predicting BraTS2021_00028
2024-10-29 21:31:21.585979: predicting BraTS2021_00031
2024-10-29 21:31:22.934373: predicting BraTS2021_00035
2024-10-29 21:31:24.279850: predicting BraTS2021_00045
2024-10-29 21:31:25.634486: predicting BraTS2021_00046
2024-10-29 21:31:26.985172: predicting BraTS2021_00051
2024-10-29 21:35:50.142999: predicting BraTS2021_00070
2024-10-29 21:35:51.514076: predicting BraTS2021_00078
2024-10-29 21:35:52.861615: predicting BraTS2021_00085
2024-10-29 21:35:54.212939: predicting BraTS2021_00087
2024-10-29 21:35:55.556598: predicting BraTS2021_00088
2024-10-29 21:35:56.895264: predicting BraTS2021_00089
2024-10-29 21:35:58.243166: predicting BraTS2021_00099
2024-10-29 21:35:59.954618: predicting BraTS2021_00102
2024-10-29 21:36:01.302099: predicting BraTS2021_00104
2024-10-29 21:36:02.644397: predicting BraTS2021_00106
2024-10-29 21:36:03.992172: predicting BraTS2021_00107
2024-10-29 21:36:05.332457: predicting BraTS2021_00110
2024-10-29 21:36:06.671304: predicting BraTS2021_00117
2024-10-29 21:36:08.012929: predicting BraTS2021_00123
2024-10-29 21:36:09.361151: predicting BraTS2021_00126
2024-10-29 21:36:10.706016: predicting BraTS2021_00127
2024-10-29 21:36:12.057353: predicting BraTS2021_00130
2024-10-29 21:36:13.402848: predicting BraTS2021_00136
2024-10-29 21:36:14.750484: predicting BraTS2021_00137
2024-10-29 21:36:16.089360: predicting BraTS2021_00140
2024-10-29 21:36:17.429481: predicting BraTS2021_00144
2024-10-29 21:36:18.769488: predicting BraTS2021_00152
2024-10-29 21:36:20.115030: predicting BraTS2021_00160
2024-10-29 21:36:21.455442: predicting BraTS2021_00185
2024-10-29 21:36:22.800379: predicting BraTS2021_00192
2024-10-29 21:36:24.145951: predicting BraTS2021_00194
2024-10-29 21:36:25.110713: predicting BraTS2021_00211
2024-10-29 21:36:26.507475: predicting BraTS2021_00212
2024-10-29 21:36:27.851580: predicting BraTS2021_00214
2024-10-29 21:36:29.205077: predicting BraTS2021_00217
2024-10-29 21:36:30.552804: predicting BraTS2021_00218
2024-10-29 21:36:31.906493: predicting BraTS2021_00231
2024-10-29 21:36:33.253289: predicting BraTS2021_00243
2024-10-29 21:36:34.592941: predicting BraTS2021_00251
2024-10-29 21:36:35.935415: predicting BraTS2021_00266
2024-10-29 21:36:37.275583: predicting BraTS2021_00270
2024-10-29 21:36:38.614156: predicting BraTS2021_00274
2024-10-29 21:36:39.959600: predicting BraTS2021_00283
2024-10-29 21:36:41.306460: predicting BraTS2021_00286
2024-10-29 21:36:42.707499: predicting BraTS2021_00292
2024-10-29 21:36:44.207976: predicting BraTS2021_00294
2024-10-29 21:36:45.556264: predicting BraTS2021_00304
2024-10-29 21:36:46.898823: predicting BraTS2021_00311
2024-10-29 21:36:48.243437: predicting BraTS2021_00313
2024-10-29 21:36:49.588686: predicting BraTS2021_00334
2024-10-29 21:36:50.933417: predicting BraTS2021_00336
2024-10-29 21:36:52.280270: predicting BraTS2021_00350
2024-10-29 21:36:53.621035: predicting BraTS2021_00353
2024-10-29 21:36:54.966413: predicting BraTS2021_00366
2024-10-29 21:36:56.309953: predicting BraTS2021_00375
2024-10-29 21:36:57.654733: predicting BraTS2021_00376
2024-10-29 21:36:59.005579: predicting BraTS2021_00379
2024-10-29 21:37:00.344617: predicting BraTS2021_00391
2024-10-29 21:37:01.688334: predicting BraTS2021_00399
2024-10-29 21:37:03.032328: predicting BraTS2021_00402
2024-10-29 21:37:03.749430: predicting BraTS2021_00419
2024-10-29 21:37:05.091821: predicting BraTS2021_00423
2024-10-29 21:37:06.437712: predicting BraTS2021_00426
2024-10-29 21:37:07.780427: predicting BraTS2021_00430
2024-10-29 21:37:09.120275: predicting BraTS2021_00433
2024-10-29 21:37:10.466082: predicting BraTS2021_00436
2024-10-29 21:37:11.801511: predicting BraTS2021_00442
2024-10-29 21:37:13.144591: predicting BraTS2021_00464
2024-10-29 21:37:14.534165: predicting BraTS2021_00480
2024-10-29 21:37:15.880716: predicting BraTS2021_00485
2024-10-29 21:37:17.219247: predicting BraTS2021_00495
2024-10-29 21:37:18.557825: predicting BraTS2021_00500
2024-10-29 21:37:19.904048: predicting BraTS2021_00514
2024-10-29 21:37:21.249181: predicting BraTS2021_00516
2024-10-29 21:37:22.592364: predicting BraTS2021_00549
2024-10-29 21:37:23.932681: predicting BraTS2021_00558
2024-10-29 21:37:25.284253: predicting BraTS2021_00583
2024-10-29 21:37:26.628129: predicting BraTS2021_00586
2024-10-29 21:37:27.968706: predicting BraTS2021_00587
2024-10-29 21:37:29.315755: predicting BraTS2021_00597
2024-10-29 21:37:30.666281: predicting BraTS2021_00602
2024-10-29 21:37:32.000252: predicting BraTS2021_00605
2024-10-29 21:37:33.351253: predicting BraTS2021_00607
2024-10-29 21:37:34.694139: predicting BraTS2021_00630
2024-10-29 21:37:36.044691: predicting BraTS2021_00639
2024-10-29 21:37:37.380597: predicting BraTS2021_00641
2024-10-29 21:37:38.723644: predicting BraTS2021_00654
2024-10-29 21:37:40.149020: predicting BraTS2021_00693
2024-10-29 21:37:41.491994: predicting BraTS2021_00709
2024-10-29 21:37:42.829616: predicting BraTS2021_00715
2024-10-29 21:37:44.173022: predicting BraTS2021_00716
2024-10-29 21:37:45.515383: predicting BraTS2021_00727
2024-10-29 21:37:46.855950: predicting BraTS2021_00733
2024-10-29 21:37:48.202883: predicting BraTS2021_00735
2024-10-29 21:37:49.547500: predicting BraTS2021_00753
2024-10-29 21:37:50.886760: predicting BraTS2021_00757
2024-10-29 21:37:52.225257: predicting BraTS2021_00759
2024-10-29 21:37:53.570441: predicting BraTS2021_00772
2024-10-29 21:37:54.915671: predicting BraTS2021_00775
2024-10-29 21:37:56.252728: predicting BraTS2021_00777
2024-10-29 21:37:57.596270: predicting BraTS2021_00782
2024-10-29 21:37:58.948370: predicting BraTS2021_00791
2024-10-29 21:38:00.294102: predicting BraTS2021_00793
2024-10-29 21:38:01.636275: predicting BraTS2021_00810
2024-10-29 21:38:02.979550: predicting BraTS2021_00818
2024-10-29 21:38:04.331342: predicting BraTS2021_00834
2024-10-29 21:38:06.157717: predicting BraTS2021_00836
2024-10-29 21:38:07.509109: predicting BraTS2021_01001
2024-10-29 21:38:08.349763: predicting BraTS2021_01004
2024-10-29 21:38:09.688073: predicting BraTS2021_01005
2024-10-29 21:38:11.028805: predicting BraTS2021_01018
2024-10-29 21:38:12.368349: predicting BraTS2021_01037
2024-10-29 21:38:13.917878: predicting BraTS2021_01043
2024-10-29 21:38:15.259857: predicting BraTS2021_01050
2024-10-29 21:38:16.602564: predicting BraTS2021_01051
2024-10-29 21:38:17.947052: predicting BraTS2021_01058
2024-10-29 21:38:19.291173: predicting BraTS2021_01063
2024-10-29 21:38:20.631515: predicting BraTS2021_01066
2024-10-29 21:38:21.973467: predicting BraTS2021_01072
2024-10-29 21:38:22.681994: predicting BraTS2021_01079
2024-10-29 21:38:24.030620: predicting BraTS2021_01081
2024-10-29 21:38:25.380815: predicting BraTS2021_01088
2024-10-29 21:38:26.724189: predicting BraTS2021_01089
2024-10-29 21:38:28.152278: predicting BraTS2021_01096
2024-10-29 21:38:29.499174: predicting BraTS2021_01101
2024-10-29 21:38:30.840841: predicting BraTS2021_01110
2024-10-29 21:38:32.177497: predicting BraTS2021_01118
2024-10-29 21:38:33.518290: predicting BraTS2021_01123
2024-10-29 21:38:34.856666: predicting BraTS2021_01124
2024-10-29 21:38:36.582054: predicting BraTS2021_01137
2024-10-29 21:38:37.289795: predicting BraTS2021_01141
2024-10-29 21:38:38.633888: predicting BraTS2021_01142
2024-10-29 21:38:39.972807: predicting BraTS2021_01146
2024-10-29 21:38:41.303538: predicting BraTS2021_01147
2024-10-29 21:38:42.641050: predicting BraTS2021_01152
2024-10-29 21:38:43.977108: predicting BraTS2021_01162
2024-10-29 21:38:45.318605: predicting BraTS2021_01169
2024-10-29 21:38:46.049400: predicting BraTS2021_01170
2024-10-29 21:38:47.388275: predicting BraTS2021_01173
2024-10-29 21:38:48.724142: predicting BraTS2021_01181
2024-10-29 21:38:49.430545: predicting BraTS2021_01183
2024-10-29 21:38:50.133722: predicting BraTS2021_01191
2024-10-29 21:38:51.744744: predicting BraTS2021_01198
2024-10-29 21:38:52.451564: predicting BraTS2021_01206
2024-10-29 21:38:53.790348: predicting BraTS2021_01208
2024-10-29 21:38:55.913444: predicting BraTS2021_01209
2024-10-29 21:38:57.350232: predicting BraTS2021_01210
2024-10-29 21:38:58.692502: predicting BraTS2021_01211
2024-10-29 21:39:00.788541: predicting BraTS2021_01212
2024-10-29 21:39:02.139141: predicting BraTS2021_01225
2024-10-29 21:39:03.485641: predicting BraTS2021_01231
2024-10-29 21:39:04.822320: predicting BraTS2021_01237
2024-10-29 21:39:06.167254: predicting BraTS2021_01238
2024-10-29 21:39:07.509012: predicting BraTS2021_01239
2024-10-29 21:39:08.219913: predicting BraTS2021_01240
2024-10-29 21:39:09.561088: predicting BraTS2021_01262
2024-10-29 21:39:10.269517: predicting BraTS2021_01265
2024-10-29 21:39:11.622386: predicting BraTS2021_01273
2024-10-29 21:39:12.964081: predicting BraTS2021_01274
2024-10-29 21:39:14.303758: predicting BraTS2021_01282
2024-10-29 21:39:15.652140: predicting BraTS2021_01283
2024-10-29 21:39:16.997634: predicting BraTS2021_01284
2024-10-29 21:39:18.340347: predicting BraTS2021_01289
2024-10-29 21:39:19.683566: predicting BraTS2021_01294
2024-10-29 21:39:21.029909: predicting BraTS2021_01296
2024-10-29 21:39:22.372686: predicting BraTS2021_01305
2024-10-29 21:39:23.717774: predicting BraTS2021_01306
2024-10-29 21:39:25.055708: predicting BraTS2021_01312
2024-10-29 21:39:26.404521: predicting BraTS2021_01316
2024-10-29 21:39:27.249452: predicting BraTS2021_01319
2024-10-29 21:39:28.585979: predicting BraTS2021_01320
2024-10-29 21:39:29.938617: predicting BraTS2021_01326
2024-10-29 21:39:31.284462: predicting BraTS2021_01328
2024-10-29 21:39:33.575177: predicting BraTS2021_01331
2024-10-29 21:39:34.917381: predicting BraTS2021_01333
2024-10-29 21:39:35.623781: predicting BraTS2021_01335
2024-10-29 21:39:36.967999: predicting BraTS2021_01339
2024-10-29 21:39:38.317467: predicting BraTS2021_01340
2024-10-29 21:39:39.657888: predicting BraTS2021_01342
2024-10-29 21:39:40.994707: predicting BraTS2021_01345
2024-10-29 21:39:42.330950: predicting BraTS2021_01349
2024-10-29 21:39:43.671209: predicting BraTS2021_01350
2024-10-29 21:39:45.006688: predicting BraTS2021_01356
2024-10-29 21:39:46.344492: predicting BraTS2021_01357
2024-10-29 21:39:47.688390: predicting BraTS2021_01359
2024-10-29 21:39:49.088760: predicting BraTS2021_01361
2024-10-29 21:39:50.438001: predicting BraTS2021_01363
2024-10-29 21:39:51.148701: predicting BraTS2021_01367
2024-10-29 21:39:52.488387: predicting BraTS2021_01375
2024-10-29 21:39:53.197033: predicting BraTS2021_01387
2024-10-29 21:39:54.538860: predicting BraTS2021_01388
2024-10-29 21:39:55.890838: predicting BraTS2021_01389
2024-10-29 21:39:57.243051: predicting BraTS2021_01398
2024-10-29 21:39:58.589298: predicting BraTS2021_01399
2024-10-29 21:39:59.301527: predicting BraTS2021_01402
2024-10-29 21:40:00.645442: predicting BraTS2021_01423
2024-10-29 21:40:01.988162: predicting BraTS2021_01425
2024-10-29 21:40:03.343065: predicting BraTS2021_01440
2024-10-29 21:40:04.684682: predicting BraTS2021_01443
2024-10-29 21:40:06.031011: predicting BraTS2021_01450
2024-10-29 21:40:07.377316: predicting BraTS2021_01461
2024-10-29 21:40:08.719046: predicting BraTS2021_01465
2024-10-29 21:40:10.059207: predicting BraTS2021_01467
2024-10-29 21:40:11.403355: predicting BraTS2021_01476
2024-10-29 21:40:12.742382: predicting BraTS2021_01480
2024-10-29 21:40:14.078097: predicting BraTS2021_01481
2024-10-29 21:40:15.418861: predicting BraTS2021_01488
2024-10-29 21:40:16.765465: predicting BraTS2021_01489
2024-10-29 21:40:18.111655: predicting BraTS2021_01490
2024-10-29 21:40:19.449596: predicting BraTS2021_01492
2024-10-29 21:40:20.786054: predicting BraTS2021_01494
2024-10-29 21:40:22.121180: predicting BraTS2021_01496
2024-10-29 21:40:23.465966: predicting BraTS2021_01500
2024-10-29 21:40:24.803807: predicting BraTS2021_01501
2024-10-29 21:40:26.142932: predicting BraTS2021_01507
2024-10-29 21:40:27.483307: predicting BraTS2021_01513
2024-10-29 21:40:28.820814: predicting BraTS2021_01518
2024-10-29 21:40:30.164004: predicting BraTS2021_01521
2024-10-29 21:40:31.506062: predicting BraTS2021_01522
2024-10-29 21:40:32.850379: predicting BraTS2021_01524
2024-10-29 21:40:34.184693: predicting BraTS2021_01530
2024-10-29 21:40:35.525951: predicting BraTS2021_01538
2024-10-29 21:40:36.869009: predicting BraTS2021_01540
2024-10-29 21:40:38.218563: predicting BraTS2021_01547
2024-10-29 21:40:39.567782: predicting BraTS2021_01560
2024-10-29 21:40:40.910311: predicting BraTS2021_01562
2024-10-29 21:40:42.257375: predicting BraTS2021_01566
2024-10-29 21:40:43.610017: predicting BraTS2021_01567
2024-10-29 21:40:44.953666: predicting BraTS2021_01582
2024-10-29 21:40:46.305390: predicting BraTS2021_01584
2024-10-29 21:40:47.653111: predicting BraTS2021_01593
2024-10-29 21:40:48.366226: predicting BraTS2021_01594
2024-10-29 21:40:49.706965: predicting BraTS2021_01599
2024-10-29 21:40:51.049605: predicting BraTS2021_01601
2024-10-29 21:40:52.398423: predicting BraTS2021_01608
2024-10-29 21:40:53.752684: predicting BraTS2021_01609
2024-10-29 21:40:55.099597: predicting BraTS2021_01613
2024-10-29 21:40:56.455281: predicting BraTS2021_01614
2024-10-29 21:40:57.807286: predicting BraTS2021_01626
2024-10-29 21:40:59.155602: predicting BraTS2021_01634
2024-10-29 21:41:00.510272: predicting BraTS2021_01651
2024-10-29 21:41:01.858846: predicting BraTS2021_01652
2024-10-29 21:41:03.201339: predicting BraTS2021_01653
2024-10-29 21:41:03.914220: predicting BraTS2021_01660
2024-10-29 21:41:04.624605: predicting BraTS2021_01661
2024-10-29 21:41:05.327045: predicting BraTS2021_01662
2024-10-29 21:41:27.126758: Validation complete
2024-10-29 21:41:27.127426: Mean Validation Dice:  0.8988111603305985
using pin_memory on device 0
2024-10-29 16:20:09.340854: train_loss -0.6821
2024-10-29 16:20:09.341925: val_loss -0.8089
2024-10-29 16:20:09.342748: Pseudo dice [0.8879, 0.8208, 0.8676]
2024-10-29 16:20:09.344387: Epoch time: 1106.52 s
2024-10-29 16:20:09.345125: Yayy! New best EMA pseudo Dice: 0.8587
2024-10-29 16:20:13.068641: 
2024-10-29 16:20:13.069637: Epoch 1
2024-10-29 16:20:13.070421: Current learning rate: 0.001
2024-10-29 16:37:38.191514: train_loss -0.7919
2024-10-29 16:37:38.193798: val_loss -0.8184
2024-10-29 16:37:38.194991: Pseudo dice [0.9144, 0.8734, 0.8724]
2024-10-29 16:37:38.196683: Epoch time: 1045.12 s
2024-10-29 16:37:38.197507: Yayy! New best EMA pseudo Dice: 0.8615
2024-10-29 16:37:39.419897: 
2024-10-29 16:37:39.421548: Epoch 2
2024-10-29 16:37:39.422434: Current learning rate: 0.001
2024-10-29 16:55:04.100111: train_loss -0.8006
2024-10-29 16:55:04.101657: val_loss -0.8287
2024-10-29 16:55:04.102470: Pseudo dice [0.8962, 0.9093, 0.8963]
2024-10-29 16:55:04.104076: Epoch time: 1044.68 s
2024-10-29 16:55:04.104807: Yayy! New best EMA pseudo Dice: 0.8654
2024-10-29 16:55:05.415300: 
2024-10-29 16:55:05.418493: Epoch 3
2024-10-29 16:55:05.419950: Current learning rate: 0.00099
2024-10-29 17:12:37.501559: train_loss -0.8177
2024-10-29 17:12:37.524073: val_loss -0.8344
2024-10-29 17:12:37.524942: Pseudo dice [0.919, 0.8861, 0.8917]
2024-10-29 17:12:37.526247: Epoch time: 1052.09 s
2024-10-29 17:12:37.527051: Yayy! New best EMA pseudo Dice: 0.8688
2024-10-29 17:12:39.947910: 
2024-10-29 17:12:39.949614: Epoch 4
2024-10-29 17:12:39.950510: Current learning rate: 0.00098
2024-10-29 17:30:09.080423: train_loss -0.8128
2024-10-29 17:30:09.082826: val_loss -0.8512
2024-10-29 17:30:09.083893: Pseudo dice [0.9329, 0.9059, 0.8967]
2024-10-29 17:30:09.086888: Epoch time: 1049.15 s
2024-10-29 17:30:09.319859: Yayy! New best EMA pseudo Dice: 0.8731
2024-10-29 17:30:11.096911: 
2024-10-29 17:30:11.099028: Epoch 5
2024-10-29 17:30:11.099851: Current learning rate: 0.00098
2024-10-29 17:47:35.905639: train_loss -0.8303
2024-10-29 17:47:35.906777: val_loss -0.8372
2024-10-29 17:47:35.907875: Pseudo dice [0.9191, 0.9019, 0.8954]
2024-10-29 17:47:35.909167: Epoch time: 1044.81 s
2024-10-29 17:47:35.909937: Yayy! New best EMA pseudo Dice: 0.8763
2024-10-29 17:47:37.722140: 
2024-10-29 17:47:37.723705: Epoch 6
2024-10-29 17:47:37.724515: Current learning rate: 0.00096
2024-10-29 18:05:02.662348: train_loss -0.8341
2024-10-29 18:05:02.663708: val_loss -0.8582
2024-10-29 18:05:02.664564: Pseudo dice [0.9187, 0.9084, 0.8894]
2024-10-29 18:05:02.665704: Epoch time: 1044.94 s
2024-10-29 18:05:02.666494: Yayy! New best EMA pseudo Dice: 0.8792
2024-10-29 18:05:03.827346: 
2024-10-29 18:05:03.828715: Epoch 7
2024-10-29 18:05:03.829531: Current learning rate: 0.00095
2024-10-29 18:22:30.525555: train_loss -0.8322
2024-10-29 18:22:30.572447: val_loss -0.8375
2024-10-29 18:22:30.573357: Pseudo dice [0.9105, 0.8998, 0.8817]
2024-10-29 18:22:30.574728: Epoch time: 1046.7 s
2024-10-29 18:22:30.575561: Yayy! New best EMA pseudo Dice: 0.8811
2024-10-29 18:22:32.192327: 
2024-10-29 18:22:32.194747: Epoch 8
2024-10-29 18:22:32.195704: Current learning rate: 0.00094
2024-10-29 18:40:00.506955: train_loss -0.845
2024-10-29 18:40:00.508925: val_loss -0.8558
2024-10-29 18:40:00.509857: Pseudo dice [0.928, 0.8954, 0.8858]
2024-10-29 18:40:00.511751: Epoch time: 1048.32 s
2024-10-29 18:40:00.512472: Yayy! New best EMA pseudo Dice: 0.8833
2024-10-29 18:40:03.004454: 
2024-10-29 18:40:03.005701: Epoch 9
2024-10-29 18:40:03.006842: Current learning rate: 0.00092
2024-10-29 18:57:27.872289: train_loss -0.8411
2024-10-29 18:57:27.873405: val_loss -0.8428
2024-10-29 18:57:27.874363: Pseudo dice [0.9149, 0.8879, 0.9062]
2024-10-29 18:57:27.887148: Epoch time: 1044.87 s
2024-10-29 18:57:28.169965: Yayy! New best EMA pseudo Dice: 0.8852
2024-10-29 18:57:29.641382: 
2024-10-29 18:57:29.642703: Epoch 10
2024-10-29 18:57:29.643648: Current learning rate: 0.0009
2024-10-29 19:14:55.162991: train_loss -0.8557
2024-10-29 19:14:55.184020: val_loss -0.8538
2024-10-29 19:14:55.185093: Pseudo dice [0.9266, 0.9057, 0.9025]
2024-10-29 19:14:55.187589: Epoch time: 1045.52 s
2024-10-29 19:14:55.188467: Yayy! New best EMA pseudo Dice: 0.8879
2024-10-29 19:14:56.865284: 
2024-10-29 19:14:56.866590: Epoch 11
2024-10-29 19:14:56.867698: Current learning rate: 0.00089
2024-10-29 19:32:23.335290: train_loss -0.8555
2024-10-29 19:32:23.337952: val_loss -0.8373
2024-10-29 19:32:23.338940: Pseudo dice [0.9264, 0.8783, 0.8873]
2024-10-29 19:32:23.340216: Epoch time: 1046.47 s
2024-10-29 19:32:23.340939: Yayy! New best EMA pseudo Dice: 0.8888
2024-10-29 19:32:25.666112: 
2024-10-29 19:32:25.667539: Epoch 12
2024-10-29 19:32:25.668686: Current learning rate: 0.00086
2024-10-29 19:49:51.181501: train_loss -0.8531
2024-10-29 19:49:51.182788: val_loss -0.8768
2024-10-29 19:49:51.183770: Pseudo dice [0.9356, 0.9283, 0.9164]
2024-10-29 19:49:51.185202: Epoch time: 1045.52 s
2024-10-29 19:49:51.185972: Yayy! New best EMA pseudo Dice: 0.8926
2024-10-29 19:49:52.903760: 
2024-10-29 19:49:52.905098: Epoch 13
2024-10-29 19:49:52.906048: Current learning rate: 0.00084
2024-10-29 20:07:17.977539: train_loss -0.8672
2024-10-29 20:07:17.979710: val_loss -0.8674
2024-10-29 20:07:17.980664: Pseudo dice [0.9311, 0.9148, 0.8881]
2024-10-29 20:07:17.981985: Epoch time: 1045.07 s
2024-10-29 20:07:17.982716: Yayy! New best EMA pseudo Dice: 0.8945
2024-10-29 20:07:19.314389: 
2024-10-29 20:07:19.315942: Epoch 14
2024-10-29 20:07:19.316950: Current learning rate: 0.00082
2024-10-29 20:24:43.949725: train_loss -0.8551
2024-10-29 20:24:43.951095: val_loss -0.8744
2024-10-29 20:24:43.951839: Pseudo dice [0.9335, 0.9205, 0.9101]
2024-10-29 20:24:43.953077: Epoch time: 1044.64 s
2024-10-29 20:24:44.132604: Yayy! New best EMA pseudo Dice: 0.8972
2024-10-29 20:24:45.987835: 
2024-10-29 20:24:45.989083: Epoch 15
2024-10-29 20:24:45.989927: Current learning rate: 0.00079
2024-10-29 20:42:27.563379: train_loss -0.8684
2024-10-29 20:42:27.660162: val_loss -0.8593
2024-10-29 20:42:27.661381: Pseudo dice [0.9143, 0.8983, 0.893]
2024-10-29 20:42:27.663628: Epoch time: 1061.56 s
2024-10-29 20:42:27.664444: Yayy! New best EMA pseudo Dice: 0.8976
2024-10-29 20:42:30.283822: 
2024-10-29 20:42:30.285235: Epoch 16
2024-10-29 20:42:30.286110: Current learning rate: 0.00077
2024-10-29 20:59:55.065112: train_loss -0.8594
2024-10-29 20:59:55.068169: val_loss -0.8548
2024-10-29 20:59:55.069058: Pseudo dice [0.9265, 0.8917, 0.9118]
2024-10-29 20:59:55.070224: Epoch time: 1044.78 s
2024-10-29 20:59:55.070946: Yayy! New best EMA pseudo Dice: 0.8989
2024-10-29 20:59:58.830192: 
2024-10-29 20:59:58.831434: Epoch 17
2024-10-29 20:59:58.832204: Current learning rate: 0.00074
2024-10-29 21:17:23.352565: train_loss -0.8662
2024-10-29 21:17:23.353936: val_loss -0.8673
2024-10-29 21:17:23.354841: Pseudo dice [0.941, 0.9182, 0.9127]
2024-10-29 21:17:23.356292: Epoch time: 1044.52 s
2024-10-29 21:17:23.357026: Yayy! New best EMA pseudo Dice: 0.9014
2024-10-29 21:17:24.909909: 
2024-10-29 21:17:24.911789: Epoch 18
2024-10-29 21:17:24.912753: Current learning rate: 0.00071
2024-10-29 21:34:53.485599: train_loss -0.8762
2024-10-29 21:34:53.487761: val_loss -0.8866
2024-10-29 21:34:53.488836: Pseudo dice [0.9347, 0.9351, 0.908]
2024-10-29 21:34:53.490305: Epoch time: 1048.58 s
2024-10-29 21:34:53.491320: Yayy! New best EMA pseudo Dice: 0.9038
2024-10-29 21:34:54.849023: 
2024-10-29 21:34:54.850266: Epoch 19
2024-10-29 21:34:54.851074: Current learning rate: 0.00068
2024-10-29 21:52:27.475278: train_loss -0.8772
2024-10-29 21:52:27.479218: val_loss -0.871
2024-10-29 21:52:27.480274: Pseudo dice [0.9276, 0.9322, 0.9043]
2024-10-29 21:52:27.481916: Epoch time: 1052.63 s
2024-10-29 21:52:28.389947: Yayy! New best EMA pseudo Dice: 0.9056
2024-10-29 21:52:29.802745: 
2024-10-29 21:52:29.804102: Epoch 20
2024-10-29 21:52:29.805058: Current learning rate: 0.00065
2024-10-29 22:09:54.827041: train_loss -0.8811
2024-10-29 22:09:54.937759: val_loss -0.8664
2024-10-29 22:09:54.938808: Pseudo dice [0.9336, 0.9, 0.909]
2024-10-29 22:09:54.940096: Epoch time: 1045.02 s
2024-10-29 20:56:51.837873: Epoch 21
2024-10-29 20:56:51.838603: Current learning rate: 0.00612
2024-10-29 21:05:13.275718: train_loss -0.8577
2024-10-29 21:05:13.277814: val_loss -0.8563
2024-10-29 21:05:13.278726: Pseudo dice [0.9402, 0.9105, 0.9011]
2024-10-29 21:05:13.280464: Epoch time: 501.44 s
2024-10-29 21:05:13.281264: Yayy! New best EMA pseudo Dice: 0.901
2024-10-29 21:05:15.772727: 
2024-10-29 21:05:15.773944: Epoch 22
2024-10-29 21:05:15.774743: Current learning rate: 0.00593
2024-10-29 21:13:36.045119: train_loss -0.8563
2024-10-29 21:13:36.046866: val_loss -0.8379
2024-10-29 21:13:36.047782: Pseudo dice [0.9273, 0.9144, 0.9034]
2024-10-29 21:13:36.049543: Epoch time: 500.27 s
2024-10-29 21:13:36.050368: Yayy! New best EMA pseudo Dice: 0.9024
2024-10-29 21:13:38.699434: 
2024-10-29 21:13:38.700431: Epoch 23
2024-10-29 21:13:38.701227: Current learning rate: 0.00574
2024-10-29 21:21:59.809665: train_loss -0.8502
2024-10-29 21:21:59.811479: val_loss -0.8555
2024-10-29 21:21:59.812598: Pseudo dice [0.9343, 0.9052, 0.8953]
2024-10-29 21:21:59.814090: Epoch time: 501.11 s
2024-10-29 21:21:59.814887: Yayy! New best EMA pseudo Dice: 0.9033
2024-10-29 21:22:02.313091: 
2024-10-29 21:22:02.314103: Epoch 24
2024-10-29 21:22:02.314854: Current learning rate: 0.00555
2024-10-29 21:30:40.928137: train_loss -0.8451
2024-10-29 21:30:41.041464: val_loss -0.8515
2024-10-29 21:30:41.042404: Pseudo dice [0.9244, 0.9212, 0.914]
2024-10-29 21:30:41.044049: Epoch time: 518.62 s
2024-10-29 21:30:42.092609: Yayy! New best EMA pseudo Dice: 0.905
2024-10-29 21:30:49.435507: 
2024-10-29 21:30:49.436718: Epoch 25
2024-10-29 21:30:49.437430: Current learning rate: 0.00536
2024-10-29 21:39:09.747731: train_loss -0.8526
2024-10-29 21:39:09.749288: val_loss -0.8793
2024-10-29 21:39:09.750236: Pseudo dice [0.9348, 0.9156, 0.903]
2024-10-29 21:39:09.751825: Epoch time: 500.31 s
2024-10-29 21:39:09.752648: Yayy! New best EMA pseudo Dice: 0.9062
2024-10-29 21:39:12.233094: 
2024-10-29 21:39:12.234268: Epoch 26
2024-10-29 21:39:12.235016: Current learning rate: 0.00517
2024-10-29 21:47:31.376549: train_loss -0.8617
2024-10-29 21:47:31.379551: val_loss -0.8903
2024-10-29 21:47:31.380429: Pseudo dice [0.9373, 0.9396, 0.9166]
2024-10-29 21:47:31.382059: Epoch time: 499.15 s
2024-10-29 21:47:31.382842: Yayy! New best EMA pseudo Dice: 0.9087
2024-10-29 21:47:33.766482: 
2024-10-29 21:47:33.767592: Epoch 27
2024-10-29 21:47:33.768328: Current learning rate: 0.00497
2024-10-29 21:55:54.435852: train_loss -0.8693
2024-10-29 21:55:54.437539: val_loss -0.8701
2024-10-29 21:55:54.438448: Pseudo dice [0.9371, 0.9217, 0.9098]
2024-10-29 21:55:54.439999: Epoch time: 500.67 s
2024-10-29 21:55:54.440930: Yayy! New best EMA pseudo Dice: 0.9101
2024-10-29 21:55:57.744366: 
2024-10-29 21:55:57.745528: Epoch 28
2024-10-29 21:55:57.746267: Current learning rate: 0.00478
2024-10-29 22:04:20.053267: train_loss -0.8663
2024-10-29 22:04:20.055143: val_loss -0.8659
2024-10-29 22:04:20.056239: Pseudo dice [0.9361, 0.9033, 0.908]
2024-10-29 22:04:20.058098: Epoch time: 502.31 s
2024-10-29 22:04:20.059109: Yayy! New best EMA pseudo Dice: 0.9107
2024-10-29 22:04:22.808741: 
2024-10-29 22:04:22.809904: Epoch 29
2024-10-29 22:04:22.810652: Current learning rate: 0.00458
2024-10-29 22:12:44.230852: train_loss -0.8625
2024-10-29 22:12:44.232741: val_loss -0.875
2024-10-29 22:12:44.233579: Pseudo dice [0.9419, 0.9141, 0.9005]
2024-10-29 22:12:44.235089: Epoch time: 501.42 s
2024-10-29 22:12:45.871504: Yayy! New best EMA pseudo Dice: 0.9115
2024-10-29 22:12:50.977416: 
2024-10-29 22:12:50.978477: Epoch 30
2024-10-29 22:12:50.979222: Current learning rate: 0.00438
2024-10-29 22:21:12.626500: train_loss -0.8675
2024-10-29 22:21:12.628222: val_loss -0.875
2024-10-29 22:21:12.629173: Pseudo dice [0.9363, 0.9194, 0.9072]
2024-10-29 22:21:12.630767: Epoch time: 501.65 s
2024-10-29 22:21:12.631602: Yayy! New best EMA pseudo Dice: 0.9125
2024-10-29 22:21:15.104122: 
2024-10-29 22:21:15.105251: Epoch 31
2024-10-29 22:21:15.106059: Current learning rate: 0.00419
2024-10-29 22:29:36.027075: train_loss -0.8696
2024-10-29 22:29:36.029125: val_loss -0.8654
2024-10-29 22:29:36.030087: Pseudo dice [0.9363, 0.9023, 0.8966]
2024-10-29 22:29:36.031941: Epoch time: 500.92 s
2024-10-29 22:29:39.125617: 
2024-10-29 22:29:39.126728: Epoch 32
2024-10-29 22:29:39.127497: Current learning rate: 0.00399
2024-10-29 22:38:09.154544: train_loss -0.8628
2024-10-29 22:38:09.214651: val_loss -0.8648
2024-10-29 22:38:09.215813: Pseudo dice [0.9377, 0.8968, 0.9123]
2024-10-29 22:38:09.217352: Epoch time: 510.03 s
2024-10-29 22:38:09.218130: Yayy! New best EMA pseudo Dice: 0.9127
2024-10-29 22:38:13.214737: 
2024-10-29 22:38:13.215800: Epoch 33
2024-10-29 22:38:13.216486: Current learning rate: 0.00379
2024-10-29 22:46:33.001962: train_loss -0.8736
2024-10-29 22:46:33.003731: val_loss -0.8604
2024-10-29 22:46:33.004629: Pseudo dice [0.9316, 0.9205, 0.9]
2024-10-29 22:46:33.006126: Epoch time: 499.79 s
2024-10-29 22:46:33.006823: Yayy! New best EMA pseudo Dice: 0.9132
2024-10-29 22:46:35.716760: 
2024-10-29 22:46:35.718972: Epoch 34
2024-10-29 22:46:35.719748: Current learning rate: 0.00359
2024-10-29 22:54:59.997639: train_loss -0.8672
2024-10-29 22:54:59.999320: val_loss -0.8579
2024-10-29 22:55:00.000264: Pseudo dice [0.943, 0.907, 0.9017]
2024-10-29 22:55:00.002891: Epoch time: 504.28 s
2024-10-29 22:55:00.992655: Yayy! New best EMA pseudo Dice: 0.9136
2024-10-29 22:55:04.111678: 
2024-10-29 22:55:04.113022: Epoch 35
2024-10-29 22:55:04.113729: Current learning rate: 0.00338
2024-10-29 23:03:25.762240: train_loss -0.8674
2024-10-29 23:03:25.764127: val_loss -0.8533
2024-10-29 23:03:25.765181: Pseudo dice [0.929, 0.9193, 0.9072]
2024-10-29 23:03:25.766723: Epoch time: 501.65 s
2024-10-29 23:03:25.770949: Yayy! New best EMA pseudo Dice: 0.9141
2024-10-29 23:03:28.231924: 
2024-10-29 23:03:28.233158: Epoch 36
2024-10-29 23:03:28.234057: Current learning rate: 0.00318
2024-10-29 23:11:49.692050: train_loss -0.8676
2024-10-29 23:11:49.693839: val_loss -0.8951
2024-10-29 23:11:49.694754: Pseudo dice [0.9443, 0.9474, 0.9201]
2024-10-29 23:11:49.696533: Epoch time: 501.46 s
2024-10-29 23:11:49.697334: Yayy! New best EMA pseudo Dice: 0.9164
2024-10-29 23:11:53.597621: 
2024-10-29 23:11:53.598674: Epoch 37
2024-10-29 23:11:53.599362: Current learning rate: 0.00297
2024-10-29 23:20:14.298217: train_loss -0.8746
2024-10-29 23:20:14.387387: val_loss -0.8792
2024-10-29 23:20:14.388214: Pseudo dice [0.944, 0.9319, 0.9191]
2024-10-29 23:20:14.390062: Epoch time: 500.66 s
2024-10-29 23:20:14.390754: Yayy! New best EMA pseudo Dice: 0.9179
2024-10-29 23:20:17.670339: 
2024-10-29 23:20:17.671381: Epoch 38
2024-10-29 23:20:17.672177: Current learning rate: 0.00277
2024-10-29 23:28:39.165119: train_loss -0.8771
2024-10-29 23:28:39.166910: val_loss -0.8848
2024-10-29 23:28:39.167819: Pseudo dice [0.9401, 0.9376, 0.9138]
2024-10-29 23:28:39.169483: Epoch time: 501.5 s
2024-10-29 23:28:39.170419: Yayy! New best EMA pseudo Dice: 0.9192
2024-10-29 23:28:41.718317: 
2024-10-29 23:28:41.719601: Epoch 39
2024-10-29 23:28:41.720445: Current learning rate: 0.00256
2024-10-29 23:37:06.030098: train_loss -0.874
2024-10-29 23:37:06.031950: val_loss -0.8876
2024-10-29 23:37:06.032793: Pseudo dice [0.9429, 0.94, 0.9134]
2024-10-29 23:37:06.034339: Epoch time: 504.31 s
2024-10-29 23:37:07.298854: Yayy! New best EMA pseudo Dice: 0.9205
2024-10-29 23:37:09.973275: 
2024-10-29 23:37:09.974720: Epoch 40
2024-10-29 23:37:09.975558: Current learning rate: 0.00235
2024-10-29 23:45:37.598879: train_loss -0.8793
2024-10-29 23:45:37.671828: val_loss -0.8689
2024-10-29 23:45:37.672704: Pseudo dice [0.9448, 0.9251, 0.9201]
2024-10-29 23:45:37.674150: Epoch time: 507.63 s
2024-10-29 23:45:37.674877: Yayy! New best EMA pseudo Dice: 0.9214
2024-10-29 23:45:45.729640: 
2024-10-29 23:45:45.731003: Epoch 41
2024-10-29 23:45:45.731775: Current learning rate: 0.00214
2024-10-29 23:54:12.203208: train_loss -0.8786
2024-10-29 23:54:12.204942: val_loss -0.8712
2024-10-29 23:54:12.205850: Pseudo dice [0.939, 0.9408, 0.9128]
2024-10-29 23:54:12.207644: Epoch time: 506.48 s
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:         Average Dice ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà
wandb: Best EMA pseudo Dice ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: enhancing tumor Dice ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   lr ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:        training_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: training_loss_per_it ‚ñÖ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb:      tumor core Dice ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá
wandb:             val_loss ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:     whole tumor Dice ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:         Average Dice 0.932
wandb: Best EMA pseudo Dice 0.9273
wandb: enhancing tumor Dice 0.9231
wandb:                epoch 49
wandb:                   lr 0.0003
wandb:        training_loss -0.88694
wandb: training_loss_per_it -0.93111
wandb:      tumor core Dice 0.9253
wandb:             val_loss -0.8646
wandb:     whole tumor Dice 0.9477
wandb: 
wandb: üöÄ View run BraTs_2023_SegResNet_3d_fullres_fold_0_2024.10.29_17:56:54 at: https://wandb.ai/arildus/BraTs_2023_SegResNet/runs/fwv9pm4s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/arildus/BraTs_2023_SegResNet
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_175656-fwv9pm4s/logs
2024-10-29 23:54:12.208430: Yayy! New best EMA pseudo Dice: 0.9224
2024-10-29 23:54:14.649157: 
2024-10-29 23:54:14.650535: Epoch 42
2024-10-29 23:54:14.651370: Current learning rate: 0.00192
2024-10-30 00:02:45.534421: train_loss -0.8776
2024-10-30 00:02:45.673633: val_loss -0.8669
2024-10-30 00:02:45.674487: Pseudo dice [0.9453, 0.9207, 0.9099]
2024-10-30 00:02:45.676242: Epoch time: 510.89 s
2024-10-30 00:02:45.677001: Yayy! New best EMA pseudo Dice: 0.9227
2024-10-30 00:02:49.284809: 
2024-10-30 00:02:49.285961: Epoch 43
2024-10-30 00:02:49.286828: Current learning rate: 0.0017
2024-10-30 00:11:10.201475: train_loss -0.8813
2024-10-30 00:11:10.203194: val_loss -0.8614
2024-10-30 00:11:10.204197: Pseudo dice [0.9357, 0.9142, 0.9146]
2024-10-30 00:11:10.206039: Epoch time: 500.92 s
2024-10-30 00:11:11.700912: 
2024-10-30 00:11:11.702034: Epoch 44
2024-10-30 00:11:11.702800: Current learning rate: 0.00148
2024-10-30 00:19:32.169853: train_loss -0.878
2024-10-30 00:19:32.171808: val_loss -0.8838
2024-10-30 00:19:32.172743: Pseudo dice [0.9356, 0.9448, 0.9258]
2024-10-30 00:19:32.174429: Epoch time: 500.47 s
2024-10-30 00:19:33.291383: Yayy! New best EMA pseudo Dice: 0.9238
2024-10-30 00:19:35.799174: 
2024-10-30 00:19:35.800289: Epoch 45
2024-10-30 00:19:35.801034: Current learning rate: 0.00126
2024-10-30 00:27:57.737657: train_loss -0.8719
2024-10-30 00:27:57.739577: val_loss -0.8799
2024-10-30 00:27:57.740434: Pseudo dice [0.9449, 0.9467, 0.9198]
2024-10-30 00:27:57.742044: Epoch time: 501.94 s
2024-10-30 00:27:57.742839: Yayy! New best EMA pseudo Dice: 0.9252
2024-10-30 00:28:00.327385: 
2024-10-30 00:28:00.328954: Epoch 46
2024-10-30 00:28:00.329932: Current learning rate: 0.00103
2024-10-30 00:36:22.648715: train_loss -0.8799
2024-10-30 00:36:22.650438: val_loss -0.8692
2024-10-30 00:36:22.651520: Pseudo dice [0.9426, 0.9122, 0.9188]
2024-10-30 00:36:22.653141: Epoch time: 502.32 s
2024-10-30 00:36:24.128294: 
2024-10-30 00:36:24.129421: Epoch 47
2024-10-30 00:36:24.130283: Current learning rate: 0.00079
2024-10-30 00:45:07.771472: train_loss -0.8876
2024-10-30 00:45:07.798420: val_loss -0.8836
2024-10-30 00:45:07.799149: Pseudo dice [0.9502, 0.9276, 0.9219]
2024-10-30 00:45:07.800518: Epoch time: 523.65 s
2024-10-30 00:45:07.801193: Yayy! New best EMA pseudo Dice: 0.9259
2024-10-30 00:45:12.015243: 
2024-10-30 00:45:12.016275: Epoch 48
2024-10-30 00:45:12.017084: Current learning rate: 0.00055
2024-10-30 00:53:33.259960: train_loss -0.8862
2024-10-30 00:53:33.261586: val_loss -0.8968
2024-10-30 00:53:33.262739: Pseudo dice [0.9498, 0.9359, 0.917]
2024-10-30 00:53:33.264428: Epoch time: 501.25 s
2024-10-30 00:53:33.265192: Yayy! New best EMA pseudo Dice: 0.9267
2024-10-30 00:53:36.144194: 
2024-10-30 00:53:36.145525: Epoch 49
2024-10-30 00:53:36.146381: Current learning rate: 0.0003
2024-10-30 01:02:00.108924: train_loss -0.8869
2024-10-30 01:02:00.251627: val_loss -0.8646
2024-10-30 01:02:00.252421: Pseudo dice [0.9477, 0.9253, 0.9231]
2024-10-30 01:02:00.254179: Epoch time: 503.84 s
2024-10-30 01:02:00.254839: Yayy! New best EMA pseudo Dice: 0.9273
2024-10-30 01:02:05.133308: Training done.
2024-10-30 01:02:40.352038: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTs2021/splits_final.json
2024-10-30 01:02:40.368186: The split file contains 5 splits.
2024-10-30 01:02:40.368736: Desired fold for training: 0
2024-10-30 01:02:40.369199: This split has 1000 training and 251 validation cases.
2024-10-30 01:02:40.374083: predicting BraTS2021_00000
2024-10-30 01:03:27.372527: predicting BraTS2021_00009
2024-10-30 01:03:30.221882: predicting BraTS2021_00016
2024-10-30 01:03:33.035849: predicting BraTS2021_00024
2024-10-30 01:03:35.856268: predicting BraTS2021_00028
2024-10-30 01:03:38.669937: predicting BraTS2021_00031
2024-10-30 01:03:41.491870: predicting BraTS2021_00035
2024-10-30 01:03:44.303532: predicting BraTS2021_00045
2024-10-30 01:03:47.132282: predicting BraTS2021_00046
2024-10-30 01:03:49.941455: predicting BraTS2021_00051
2024-10-30 01:08:07.853011: predicting BraTS2021_00070
2024-10-30 01:08:10.693642: predicting BraTS2021_00078
2024-10-30 01:08:13.519187: predicting BraTS2021_00085
2024-10-30 01:08:16.347210: predicting BraTS2021_00087
2024-10-30 01:08:19.162983: predicting BraTS2021_00088
2024-10-30 01:08:21.975049: predicting BraTS2021_00089
2024-10-30 01:08:24.790180: predicting BraTS2021_00099
2024-10-30 01:08:27.605150: predicting BraTS2021_00102
2024-10-30 01:08:30.425954: predicting BraTS2021_00104
2024-10-30 01:08:33.244665: predicting BraTS2021_00106
2024-10-30 01:08:36.065670: predicting BraTS2021_00107
2024-10-30 01:08:38.878242: predicting BraTS2021_00110
2024-10-30 01:08:41.688664: predicting BraTS2021_00117
2024-10-30 01:08:44.503586: predicting BraTS2021_00123
2024-10-30 01:08:47.321029: predicting BraTS2021_00126
2024-10-30 01:08:50.164159: predicting BraTS2021_00127
2024-10-30 01:08:52.993040: predicting BraTS2021_00130
2024-10-30 01:08:55.873909: predicting BraTS2021_00136
2024-10-30 01:08:58.690078: predicting BraTS2021_00137
2024-10-30 01:09:01.504980: predicting BraTS2021_00140
2024-10-30 01:09:04.320879: predicting BraTS2021_00144
2024-10-30 01:09:07.136249: predicting BraTS2021_00152
2024-10-30 01:09:09.950886: predicting BraTS2021_00160
2024-10-30 01:09:12.772438: predicting BraTS2021_00185
2024-10-30 01:09:15.593211: predicting BraTS2021_00192
2024-10-30 01:09:19.215715: predicting BraTS2021_00194
2024-10-30 01:09:20.669922: predicting BraTS2021_00211
2024-10-30 01:09:23.490606: predicting BraTS2021_00212
2024-10-30 01:09:26.311057: predicting BraTS2021_00214
2024-10-30 01:09:29.139854: predicting BraTS2021_00217
2024-10-30 01:09:31.966322: predicting BraTS2021_00218
2024-10-30 01:09:34.798578: predicting BraTS2021_00231
2024-10-30 01:09:37.619734: predicting BraTS2021_00243
2024-10-30 01:09:40.437073: predicting BraTS2021_00251
2024-10-30 01:09:43.257635: predicting BraTS2021_00266
2024-10-30 01:09:46.073565: predicting BraTS2021_00270
2024-10-30 01:09:49.373511: predicting BraTS2021_00274
2024-10-30 01:09:52.191583: predicting BraTS2021_00283
2024-10-30 01:09:55.015526: predicting BraTS2021_00286
2024-10-30 01:09:57.845862: predicting BraTS2021_00292
2024-10-30 01:10:00.670040: predicting BraTS2021_00294
2024-10-30 01:10:03.492127: predicting BraTS2021_00304
2024-10-30 01:10:06.313407: predicting BraTS2021_00311
2024-10-30 01:10:09.137465: predicting BraTS2021_00313
2024-10-30 01:10:11.959455: predicting BraTS2021_00334
2024-10-30 01:10:14.779114: predicting BraTS2021_00336
2024-10-30 01:10:17.600307: predicting BraTS2021_00350
2024-10-30 01:10:20.421345: predicting BraTS2021_00353
2024-10-30 01:10:23.243228: predicting BraTS2021_00366
2024-10-30 01:10:26.059846: predicting BraTS2021_00375
2024-10-30 01:10:28.881183: predicting BraTS2021_00376
2024-10-30 01:10:31.706264: predicting BraTS2021_00379
2024-10-30 01:10:34.519903: predicting BraTS2021_00391
2024-10-30 01:10:37.339783: predicting BraTS2021_00399
2024-10-30 01:10:40.156504: predicting BraTS2021_00402
2024-10-30 01:10:41.615336: predicting BraTS2021_00419
2024-10-30 01:10:44.435287: predicting BraTS2021_00423
2024-10-30 01:10:47.253979: predicting BraTS2021_00426
2024-10-30 01:10:50.066704: predicting BraTS2021_00430
2024-10-30 01:10:52.881128: predicting BraTS2021_00433
2024-10-30 01:10:55.699696: predicting BraTS2021_00436
2024-10-30 01:10:58.512954: predicting BraTS2021_00442
2024-10-30 01:11:01.335757: predicting BraTS2021_00464
2024-10-30 01:11:04.161428: predicting BraTS2021_00480
2024-10-30 01:11:06.983544: predicting BraTS2021_00485
2024-10-30 01:11:09.797096: predicting BraTS2021_00495
2024-10-30 01:11:12.611898: predicting BraTS2021_00500
2024-10-30 01:11:15.513189: predicting BraTS2021_00514
2024-10-30 01:11:18.336103: predicting BraTS2021_00516
2024-10-30 01:11:21.151972: predicting BraTS2021_00549
2024-10-30 01:11:23.974084: predicting BraTS2021_00558
2024-10-30 01:11:26.799616: predicting BraTS2021_00583
2024-10-30 01:11:29.620569: predicting BraTS2021_00586
2024-10-30 01:11:32.438652: predicting BraTS2021_00587
2024-10-30 01:11:35.263236: predicting BraTS2021_00597
2024-10-30 01:11:38.086028: predicting BraTS2021_00602
2024-10-30 01:11:40.893171: predicting BraTS2021_00605
2024-10-30 01:11:43.720778: predicting BraTS2021_00607
2024-10-30 01:11:46.537694: predicting BraTS2021_00630
2024-10-30 01:11:49.366537: predicting BraTS2021_00639
2024-10-30 01:11:52.179099: predicting BraTS2021_00641
2024-10-30 01:11:54.999234: predicting BraTS2021_00654
2024-10-30 01:11:57.814448: predicting BraTS2021_00693
2024-10-30 01:12:00.630369: predicting BraTS2021_00709
2024-10-30 01:12:03.444621: predicting BraTS2021_00715
2024-10-30 01:12:06.265665: predicting BraTS2021_00716
2024-10-30 01:12:09.083894: predicting BraTS2021_00727
2024-10-30 01:12:11.900874: predicting BraTS2021_00733
2024-10-30 01:12:14.726011: predicting BraTS2021_00735
2024-10-30 01:12:17.546284: predicting BraTS2021_00753
2024-10-30 01:12:20.358144: predicting BraTS2021_00757
2024-10-30 01:12:23.175076: predicting BraTS2021_00759
2024-10-30 01:12:25.997714: predicting BraTS2021_00772
2024-10-30 01:12:28.823170: predicting BraTS2021_00775
2024-10-30 01:12:31.642511: predicting BraTS2021_00777
2024-10-30 01:12:34.463164: predicting BraTS2021_00782
2024-10-30 01:12:37.294085: predicting BraTS2021_00791
2024-10-30 01:12:40.116747: predicting BraTS2021_00793
2024-10-30 01:12:42.937716: predicting BraTS2021_00810
2024-10-30 01:12:45.756858: predicting BraTS2021_00818
2024-10-30 01:12:48.584079: predicting BraTS2021_00834
2024-10-30 01:12:51.402031: predicting BraTS2021_00836
2024-10-30 01:12:54.231350: predicting BraTS2021_01001
2024-10-30 01:12:55.688106: predicting BraTS2021_01004
2024-10-30 01:12:58.503518: predicting BraTS2021_01005
2024-10-30 01:13:01.365503: predicting BraTS2021_01018
2024-10-30 01:13:05.193398: predicting BraTS2021_01037
2024-10-30 01:13:08.019683: predicting BraTS2021_01043
2024-10-30 01:13:10.834555: predicting BraTS2021_01050
2024-10-30 01:13:13.654361: predicting BraTS2021_01051
2024-10-30 01:13:16.476981: predicting BraTS2021_01058
2024-10-30 01:13:20.049804: predicting BraTS2021_01063
2024-10-30 01:13:22.871187: predicting BraTS2021_01066
2024-10-30 01:13:25.692281: predicting BraTS2021_01072
2024-10-30 01:13:27.148100: predicting BraTS2021_01079
2024-10-30 01:13:29.970662: predicting BraTS2021_01081
2024-10-30 01:13:32.791171: predicting BraTS2021_01088
2024-10-30 01:13:35.610814: predicting BraTS2021_01089
2024-10-30 01:13:38.429378: predicting BraTS2021_01096
2024-10-30 01:13:41.254179: predicting BraTS2021_01101
2024-10-30 01:13:44.073615: predicting BraTS2021_01110
2024-10-30 01:13:46.887951: predicting BraTS2021_01118
2024-10-30 01:13:49.705292: predicting BraTS2021_01123
2024-10-30 01:13:52.522277: predicting BraTS2021_01124
2024-10-30 01:13:55.341707: predicting BraTS2021_01137
2024-10-30 01:13:56.789466: predicting BraTS2021_01141
2024-10-30 01:13:59.605872: predicting BraTS2021_01142
2024-10-30 01:14:02.421020: predicting BraTS2021_01146
2024-10-30 01:14:05.229543: predicting BraTS2021_01147
2024-10-30 01:14:08.046652: predicting BraTS2021_01152
2024-10-30 01:14:10.859284: predicting BraTS2021_01162
2024-10-30 01:14:13.678176: predicting BraTS2021_01169
2024-10-30 01:14:15.123741: predicting BraTS2021_01170
2024-10-30 01:14:18.034331: predicting BraTS2021_01173
2024-10-30 01:14:20.848092: predicting BraTS2021_01181
2024-10-30 01:14:22.296569: predicting BraTS2021_01183
2024-10-30 01:14:23.744998: predicting BraTS2021_01191
2024-10-30 01:14:26.559717: predicting BraTS2021_01198
2024-10-30 01:14:28.011077: predicting BraTS2021_01206
2024-10-30 01:14:30.832764: predicting BraTS2021_01208
2024-10-30 01:14:33.659392: predicting BraTS2021_01209
2024-10-30 01:14:36.479968: predicting BraTS2021_01210
2024-10-30 01:14:39.421170: predicting BraTS2021_01211
2024-10-30 01:14:42.243217: predicting BraTS2021_01212
2024-10-30 01:14:45.066901: predicting BraTS2021_01225
2024-10-30 01:14:47.886705: predicting BraTS2021_01231
2024-10-30 01:14:50.758924: predicting BraTS2021_01237
2024-10-30 01:14:53.578912: predicting BraTS2021_01238
2024-10-30 01:14:56.394572: predicting BraTS2021_01239
2024-10-30 01:14:57.944531: predicting BraTS2021_01240
2024-10-30 01:15:00.762512: predicting BraTS2021_01262
2024-10-30 01:15:02.212488: predicting BraTS2021_01265
2024-10-30 01:15:05.040890: predicting BraTS2021_01273
2024-10-30 01:15:07.856789: predicting BraTS2021_01274
2024-10-30 01:15:10.976164: predicting BraTS2021_01282
2024-10-30 01:15:13.803025: predicting BraTS2021_01283
2024-10-30 01:15:16.628640: predicting BraTS2021_01284
2024-10-30 01:15:19.448982: predicting BraTS2021_01289
2024-10-30 01:15:22.270842: predicting BraTS2021_01294
2024-10-30 01:15:25.093741: predicting BraTS2021_01296
2024-10-30 01:15:27.911830: predicting BraTS2021_01305
2024-10-30 01:15:30.733376: predicting BraTS2021_01306
2024-10-30 01:15:33.543857: predicting BraTS2021_01312
2024-10-30 01:15:36.486506: predicting BraTS2021_01316
2024-10-30 01:15:37.940894: predicting BraTS2021_01319
2024-10-30 01:15:40.755658: predicting BraTS2021_01320
2024-10-30 01:15:43.581251: predicting BraTS2021_01326
2024-10-30 01:15:46.401303: predicting BraTS2021_01328
2024-10-30 01:15:49.222673: predicting BraTS2021_01331
2024-10-30 01:15:52.041600: predicting BraTS2021_01333
2024-10-30 01:15:53.491526: predicting BraTS2021_01335
2024-10-30 01:15:56.312889: predicting BraTS2021_01339
2024-10-30 01:15:59.140712: predicting BraTS2021_01340
2024-10-30 01:16:01.954104: predicting BraTS2021_01342
2024-10-30 01:16:04.771024: predicting BraTS2021_01345
2024-10-30 01:16:07.584936: predicting BraTS2021_01349
2024-10-30 01:16:10.402878: predicting BraTS2021_01350
2024-10-30 01:16:13.212481: predicting BraTS2021_01356
2024-10-30 01:16:16.030543: predicting BraTS2021_01357
2024-10-30 01:16:18.846693: predicting BraTS2021_01359
2024-10-30 01:16:21.661171: predicting BraTS2021_01361
2024-10-30 01:16:24.485307: predicting BraTS2021_01363
2024-10-30 01:16:25.938200: predicting BraTS2021_01367
2024-10-30 01:16:28.753351: predicting BraTS2021_01375
2024-10-30 01:16:30.203943: predicting BraTS2021_01387
2024-10-30 01:16:33.305475: predicting BraTS2021_01388
2024-10-30 01:16:36.130173: predicting BraTS2021_01389
2024-10-30 01:16:38.962494: predicting BraTS2021_01398
2024-10-30 01:16:41.782281: predicting BraTS2021_01399
2024-10-30 01:16:43.236963: predicting BraTS2021_01402
2024-10-30 01:16:46.056152: predicting BraTS2021_01423
2024-10-30 01:16:48.874848: predicting BraTS2021_01425
2024-10-30 01:16:51.705073: predicting BraTS2021_01440
2024-10-30 01:16:54.522558: predicting BraTS2021_01443
2024-10-30 01:16:57.343707: predicting BraTS2021_01450
2024-10-30 01:17:00.162737: predicting BraTS2021_01461
2024-10-30 01:17:02.983712: predicting BraTS2021_01465
2024-10-30 01:17:05.805085: predicting BraTS2021_01467
2024-10-30 01:17:08.625090: predicting BraTS2021_01476
2024-10-30 01:17:11.442293: predicting BraTS2021_01480
2024-10-30 01:17:14.257165: predicting BraTS2021_01481
2024-10-30 01:17:17.075767: predicting BraTS2021_01488
2024-10-30 01:17:19.896523: predicting BraTS2021_01489
2024-10-30 01:17:22.721747: predicting BraTS2021_01490
2024-10-30 01:17:25.536495: predicting BraTS2021_01492
2024-10-30 01:17:28.353960: predicting BraTS2021_01494
2024-10-30 01:17:31.168438: predicting BraTS2021_01496
2024-10-30 01:17:33.992039: predicting BraTS2021_01500
2024-10-30 01:17:36.802386: predicting BraTS2021_01501
2024-10-30 01:17:39.614239: predicting BraTS2021_01507
2024-10-30 01:17:42.632084: predicting BraTS2021_01513
2024-10-30 01:17:45.447746: predicting BraTS2021_01518
2024-10-30 01:17:48.267950: predicting BraTS2021_01521
2024-10-30 01:17:51.082572: predicting BraTS2021_01522
2024-10-30 01:17:53.902053: predicting BraTS2021_01524
2024-10-30 01:18:11.048339: predicting BraTS2021_01530
2024-10-30 01:18:13.903148: predicting BraTS2021_01538
2024-10-30 01:18:16.721295: predicting BraTS2021_01540
2024-10-30 01:18:19.540177: predicting BraTS2021_01547
2024-10-30 01:18:22.362142: predicting BraTS2021_01560
2024-10-30 01:18:25.174775: predicting BraTS2021_01562
2024-10-30 01:18:27.990448: predicting BraTS2021_01566
2024-10-30 01:18:30.812969: predicting BraTS2021_01567
2024-10-30 01:18:33.627148: predicting BraTS2021_01582
2024-10-30 01:18:36.444539: predicting BraTS2021_01584
2024-10-30 01:18:39.263638: predicting BraTS2021_01593
2024-10-30 01:18:40.717195: predicting BraTS2021_01594
2024-10-30 01:18:43.533175: predicting BraTS2021_01599
2024-10-30 01:18:46.343451: predicting BraTS2021_01601
2024-10-30 01:18:49.165920: predicting BraTS2021_01608
2024-10-30 01:18:51.986346: predicting BraTS2021_01609
2024-10-30 01:18:54.802068: predicting BraTS2021_01613
2024-10-30 01:18:57.625841: predicting BraTS2021_01614
2024-10-30 01:19:00.444615: predicting BraTS2021_01626
2024-10-30 01:19:03.258272: predicting BraTS2021_01634
2024-10-30 01:19:06.084931: predicting BraTS2021_01651
2024-10-30 01:19:31.967780: predicting BraTS2021_01652
2024-10-30 01:19:34.801842: predicting BraTS2021_01653
2024-10-30 01:19:36.251614: predicting BraTS2021_01660
2024-10-30 01:19:37.702281: predicting BraTS2021_01661
2024-10-30 01:19:39.145734: predicting BraTS2021_01662
2024-10-30 01:20:31.466554: Validation complete
2024-10-30 01:20:31.467169: Mean Validation Dice:  0.8966849785166167
2024-10-29 22:09:54.940831: Yayy! New best EMA pseudo Dice: 0.9065
2024-10-29 22:09:58.061096: 
2024-10-29 22:09:58.062988: Epoch 21
2024-10-29 22:09:58.063922: Current learning rate: 0.00062
2024-10-29 22:27:25.447101: train_loss -0.8757
2024-10-29 22:27:25.448379: val_loss -0.8856
2024-10-29 22:27:25.449371: Pseudo dice [0.9363, 0.9198, 0.9075]
2024-10-29 22:27:25.450632: Epoch time: 1047.39 s
2024-10-29 22:27:25.451422: Yayy! New best EMA pseudo Dice: 0.9079
2024-10-29 22:27:26.769328: 
2024-10-29 22:27:26.770625: Epoch 22
2024-10-29 22:27:26.771542: Current learning rate: 0.00059
2024-10-29 22:44:52.533939: train_loss -0.8903
2024-10-29 22:44:52.535793: val_loss -0.8659
2024-10-29 22:44:52.536839: Pseudo dice [0.9374, 0.9198, 0.9098]
2024-10-29 22:44:52.538372: Epoch time: 1045.76 s
2024-10-29 22:44:52.539187: Yayy! New best EMA pseudo Dice: 0.9094
2024-10-29 22:44:53.703753: 
2024-10-29 22:44:53.705230: Epoch 23
2024-10-29 22:44:53.706054: Current learning rate: 0.00056
2024-10-29 23:02:26.319257: train_loss -0.8842
2024-10-29 23:02:26.459997: val_loss -0.88
2024-10-29 23:02:26.460907: Pseudo dice [0.9073, 0.9482, 0.911]
2024-10-29 23:02:26.602701: Epoch time: 1052.58 s
2024-10-29 23:02:26.603629: Yayy! New best EMA pseudo Dice: 0.9107
2024-10-29 23:02:29.354329: 
2024-10-29 23:02:29.355726: Epoch 24
2024-10-29 23:02:29.356612: Current learning rate: 0.00053
2024-10-29 23:19:54.092810: train_loss -0.888
2024-10-29 23:19:54.094570: val_loss -0.8598
2024-10-29 23:19:54.095582: Pseudo dice [0.9382, 0.91, 0.9019]
2024-10-29 23:19:54.098290: Epoch time: 1044.74 s
2024-10-29 23:19:54.353381: Yayy! New best EMA pseudo Dice: 0.9113
2024-10-29 23:19:56.415442: 
2024-10-29 23:19:56.416697: Epoch 25
2024-10-29 23:19:56.417671: Current learning rate: 0.0005
2024-10-29 23:37:30.191390: train_loss -0.8832
2024-10-29 23:37:30.193586: val_loss -0.8941
2024-10-29 23:37:30.194713: Pseudo dice [0.9443, 0.9386, 0.9165]
2024-10-29 23:37:30.201164: Epoch time: 1053.78 s
2024-10-29 23:37:30.202227: Yayy! New best EMA pseudo Dice: 0.9134
2024-10-29 23:37:31.364236: 
2024-10-29 23:37:31.365571: Epoch 26
2024-10-29 23:37:31.366378: Current learning rate: 0.00047
2024-10-29 23:54:55.568990: train_loss -0.8862
2024-10-29 23:54:55.570137: val_loss -0.8899
2024-10-29 23:54:55.571081: Pseudo dice [0.9418, 0.9386, 0.9136]
2024-10-29 23:54:55.572306: Epoch time: 1044.21 s
2024-10-29 23:54:55.573099: Yayy! New best EMA pseudo Dice: 0.9152
2024-10-29 23:54:56.895501: 
2024-10-29 23:54:56.896840: Epoch 27
2024-10-29 23:54:56.897780: Current learning rate: 0.00044
2024-10-30 00:12:24.922534: train_loss -0.8793
2024-10-30 00:12:25.026479: val_loss -0.8791
2024-10-30 00:12:25.027549: Pseudo dice [0.9418, 0.9163, 0.9138]
2024-10-30 00:12:25.029529: Epoch time: 1048.0 s
2024-10-30 00:12:25.030294: Yayy! New best EMA pseudo Dice: 0.9161
2024-10-30 00:12:31.167151: 
2024-10-30 00:12:31.168670: Epoch 28
2024-10-30 00:12:31.169610: Current learning rate: 0.00041
2024-10-30 00:29:57.834319: train_loss -0.8826
2024-10-30 00:29:57.835361: val_loss -0.9029
2024-10-30 00:29:57.836281: Pseudo dice [0.9453, 0.9447, 0.9259]
2024-10-30 00:29:57.837589: Epoch time: 1046.67 s
2024-10-30 00:29:57.838293: Yayy! New best EMA pseudo Dice: 0.9184
2024-10-30 00:30:00.897950: 
2024-10-30 00:30:00.899120: Epoch 29
2024-10-30 00:30:00.899970: Current learning rate: 0.00038
2024-10-30 00:47:31.245420: train_loss -0.8914
2024-10-30 00:47:31.246719: val_loss -0.8997
2024-10-30 00:47:31.247765: Pseudo dice [0.9496, 0.9435, 0.9268]
2024-10-30 00:47:31.250149: Epoch time: 1050.35 s
2024-10-30 00:47:31.696990: Yayy! New best EMA pseudo Dice: 0.9205
2024-10-30 00:47:32.965706: 
2024-10-30 00:47:32.966915: Epoch 30
2024-10-30 00:47:32.967818: Current learning rate: 0.00035
2024-10-30 01:04:58.835796: train_loss -0.8911
2024-10-30 01:04:58.837154: val_loss -0.8855
2024-10-30 01:04:58.838135: Pseudo dice [0.9482, 0.9248, 0.9167]
2024-10-30 01:04:58.839497: Epoch time: 1045.87 s
2024-10-30 01:04:58.840231: Yayy! New best EMA pseudo Dice: 0.9215
2024-10-30 01:05:00.059957: 
2024-10-30 01:05:00.061196: Epoch 31
2024-10-30 01:05:00.062196: Current learning rate: 0.00032
2024-10-30 01:22:32.086509: train_loss -0.89
2024-10-30 01:22:32.615135: val_loss -0.876
2024-10-30 01:22:32.616098: Pseudo dice [0.9387, 0.9262, 0.9195]
2024-10-30 01:22:32.617896: Epoch time: 1052.03 s
2024-10-30 01:22:32.618777: Yayy! New best EMA pseudo Dice: 0.9221
2024-10-30 01:22:35.622752: 
2024-10-30 01:22:35.624025: Epoch 32
2024-10-30 01:22:35.624844: Current learning rate: 0.00029
2024-10-30 01:40:05.419158: train_loss -0.8929
2024-10-30 01:40:05.420904: val_loss -0.8861
2024-10-30 01:40:05.421753: Pseudo dice [0.9464, 0.9308, 0.9157]
2024-10-30 01:40:05.423206: Epoch time: 1049.8 s
2024-10-30 01:40:05.423972: Yayy! New best EMA pseudo Dice: 0.923
2024-10-30 01:40:08.217569: 
2024-10-30 01:40:08.218648: Epoch 33
2024-10-30 01:40:08.219469: Current learning rate: 0.00026
2024-10-30 01:57:32.318523: train_loss -0.8927
2024-10-30 01:57:32.320096: val_loss -0.8874
2024-10-30 01:57:32.321021: Pseudo dice [0.9425, 0.9244, 0.9152]
2024-10-30 01:57:32.322433: Epoch time: 1044.1 s
2024-10-30 01:57:32.323262: Yayy! New best EMA pseudo Dice: 0.9235
2024-10-30 01:57:33.580389: 
2024-10-30 01:57:33.581677: Epoch 34
2024-10-30 01:57:33.582505: Current learning rate: 0.00023
2024-10-30 02:15:02.858531: train_loss -0.8884
2024-10-30 02:15:02.862188: val_loss -0.8486
2024-10-30 02:15:02.863101: Pseudo dice [0.9324, 0.924, 0.908]
2024-10-30 02:15:02.864416: Epoch time: 1049.28 s
2024-10-30 02:15:04.270939: 
2024-10-30 02:15:04.275007: Epoch 35
2024-10-30 02:15:04.275846: Current learning rate: 0.00021
2024-10-30 02:32:45.088567: train_loss -0.894
2024-10-30 02:32:45.427284: val_loss -0.8878
2024-10-30 02:32:45.428972: Pseudo dice [0.9517, 0.9526, 0.9325]
2024-10-30 02:32:45.495048: Epoch time: 1060.77 s
2024-10-30 02:32:45.496141: Yayy! New best EMA pseudo Dice: 0.9255
2024-10-30 02:32:49.228497: 
2024-10-30 02:32:49.230923: Epoch 36
2024-10-30 02:32:49.231901: Current learning rate: 0.00018
2024-10-30 02:50:20.860432: train_loss -0.897
2024-10-30 02:50:20.862135: val_loss -0.898
2024-10-30 02:50:20.863025: Pseudo dice [0.947, 0.9424, 0.9271]
2024-10-30 02:50:20.864458: Epoch time: 1051.63 s
2024-10-30 02:50:20.865201: Yayy! New best EMA pseudo Dice: 0.9268
2024-10-30 02:50:26.401466: 
2024-10-30 02:50:26.403055: Epoch 37
2024-10-30 02:50:26.403920: Current learning rate: 0.00016
2024-10-30 03:07:51.331717: train_loss -0.8957
2024-10-30 03:07:51.333478: val_loss -0.8888
2024-10-30 03:07:51.334305: Pseudo dice [0.9523, 0.9562, 0.9333]
2024-10-30 03:07:51.335556: Epoch time: 1044.93 s
2024-10-30 03:07:51.336284: Yayy! New best EMA pseudo Dice: 0.9289
2024-10-30 03:07:52.604799: 
2024-10-30 03:07:52.606160: Epoch 38
2024-10-30 03:07:52.606995: Current learning rate: 0.00014
2024-10-30 03:25:21.300988: train_loss -0.8966
2024-10-30 03:25:21.803312: val_loss -0.8773
2024-10-30 03:25:21.804461: Pseudo dice [0.9508, 0.9359, 0.9171]
2024-10-30 03:25:21.805692: Epoch time: 1048.7 s
2024-10-30 03:25:21.806353: Yayy! New best EMA pseudo Dice: 0.9294
2024-10-30 03:25:26.527579: 
2024-10-30 03:25:26.529265: Epoch 39
2024-10-30 03:25:26.530113: Current learning rate: 0.00012
2024-10-30 03:43:33.652061: train_loss -0.902
2024-10-30 03:43:33.862684: val_loss -0.8971
2024-10-30 03:43:33.863576: Pseudo dice [0.9458, 0.9421, 0.927]
2024-10-30 03:43:33.866062: Epoch time: 1087.1 s
2024-10-30 03:43:36.666215: Yayy! New best EMA pseudo Dice: 0.9303
2024-10-30 03:43:40.153588: 
2024-10-30 03:43:40.155243: Epoch 40
2024-10-30 03:43:40.156326: Current learning rate: 0.0001
2024-10-30 04:01:04.910376: train_loss -0.9016
2024-10-30 04:01:04.912162: val_loss -0.8996
2024-10-30 04:01:04.912999: Pseudo dice [0.951, 0.9423, 0.9228]
2024-10-30 04:01:04.914309: Epoch time: 1044.76 s
2024-10-30 04:01:04.915217: Yayy! New best EMA pseudo Dice: 0.9312
2024-10-30 04:01:08.040777: 
2024-10-30 04:01:08.042312: Epoch 41
2024-10-30 04:01:08.043243: Current learning rate: 8e-05
2024-10-30 04:18:33.878701: train_loss -0.8998
2024-10-30 04:18:33.880172: val_loss -0.9138
Exception in thread Thread-7 (results_loop):
Traceback (most recent call last):
  File "/cluster/home/aris/.conda/envs/nnUNet_env/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/cluster/home/aris/.conda/envs/nnUNet_env/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/cluster/home/aris/.conda/envs/nnUNet_env/lib/python3.10/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e/cluster/home/aris/.conda/envs/nnUNet_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2386: UserWarning: Run (loo01mm9) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),

  File "/cluster/home/aris/.conda/envs/nnUNet_env/lib/python3.10/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         Average Dice ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá
wandb: Best EMA pseudo Dice ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: enhancing tumor Dice ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÜ
wandb:                epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        training_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: training_loss_per_it ‚ñÜ‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÇ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÑ
wandb:      tumor core Dice ‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà
wandb:             val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÉ
wandb:     whole tumor Dice ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb:         Average Dice 0.9327
wandb: Best EMA pseudo Dice 0.9335
wandb: enhancing tumor Dice 0.9167
wandb:                epoch 49
wandb:                   lr 0.0
wandb:        training_loss -0.89537
wandb: training_loss_per_it -0.75304
wandb:      tumor core Dice 0.9459
wandb:             val_loss -0.89117
wandb:     whole tumor Dice 0.9355
wandb: 
wandb: üöÄ View run BraTs_2023_MedNext_3d_fullres_fold_0_2024.10.29_16:01:40 at: https://wandb.ai/arildus/BraTs_2023_MedNext/runs/loo01mm9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/arildus/BraTs_2023_MedNext
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_160140-loo01mm9/logs
2024-10-30 04:18:33.881011: Pseudo dice [0.9491, 0.9488, 0.9307]
2024-10-30 04:18:33.882854: Epoch time: 1045.84 s
2024-10-30 04:18:33.883639: Yayy! New best EMA pseudo Dice: 0.9323
2024-10-30 04:18:35.042438: 
2024-10-30 04:18:35.043839: Epoch 42
2024-10-30 04:18:35.044755: Current learning rate: 6e-05
2024-10-30 04:36:02.413686: train_loss -0.8993
2024-10-30 04:36:03.377876: val_loss -0.9024
2024-10-30 04:36:03.378460: Pseudo dice [0.9515, 0.9411, 0.9362]
2024-10-30 04:36:03.379929: Epoch time: 1047.37 s
2024-10-30 04:36:03.380711: Yayy! New best EMA pseudo Dice: 0.9334
2024-10-30 04:36:07.935394: 
2024-10-30 04:36:07.936734: Epoch 43
2024-10-30 04:36:07.937742: Current learning rate: 5e-05
2024-10-30 04:53:42.368886: train_loss -0.9004
2024-10-30 04:53:42.525851: val_loss -0.902
2024-10-30 04:53:42.528542: Pseudo dice [0.9437, 0.9398, 0.9189]
2024-10-30 04:53:42.530655: Epoch time: 1054.4 s
2024-10-30 04:53:42.531354: Yayy! New best EMA pseudo Dice: 0.9335
2024-10-30 04:53:45.798084: 
2024-10-30 04:53:45.799446: Epoch 44
2024-10-30 04:53:45.800200: Current learning rate: 4e-05
2024-10-30 05:11:10.483901: train_loss -0.9008
2024-10-30 05:11:10.485793: val_loss -0.8871
2024-10-30 05:11:10.486944: Pseudo dice [0.9445, 0.9159, 0.9228]
2024-10-30 05:11:10.488293: Epoch time: 1044.69 s
2024-10-30 05:11:12.260257: 
2024-10-30 05:11:12.261660: Epoch 45
2024-10-30 05:11:12.262609: Current learning rate: 3e-05
2024-10-30 05:28:42.414102: train_loss -0.9022
2024-10-30 05:28:42.415632: val_loss -0.8906
2024-10-30 05:28:42.416445: Pseudo dice [0.9462, 0.9292, 0.9244]
2024-10-30 05:28:42.417723: Epoch time: 1050.15 s
2024-10-30 05:28:46.852323: 
2024-10-30 05:28:46.853706: Epoch 46
2024-10-30 05:28:46.854596: Current learning rate: 2e-05
2024-10-30 05:46:12.026026: train_loss -0.8977
2024-10-30 05:46:12.027504: val_loss -0.8983
2024-10-30 05:46:12.028476: Pseudo dice [0.9453, 0.9367, 0.9327]
2024-10-30 05:46:12.029891: Epoch time: 1045.17 s
2024-10-30 05:46:13.005862: 
2024-10-30 05:46:13.007042: Epoch 47
2024-10-30 05:46:13.007873: Current learning rate: 1e-05
2024-10-30 06:03:45.453688: train_loss -0.9014
2024-10-30 06:03:45.674949: val_loss -0.8414
2024-10-30 06:03:45.675982: Pseudo dice [0.9335, 0.9228, 0.9166]
2024-10-30 06:03:45.699737: Epoch time: 1052.43 s
2024-10-30 06:03:48.148518: 
2024-10-30 06:03:48.150141: Epoch 48
2024-10-30 06:03:48.151200: Current learning rate: 0.0
2024-10-30 06:21:15.516921: train_loss -0.9013
2024-10-30 06:21:15.518300: val_loss -0.8846
2024-10-30 06:21:15.519200: Pseudo dice [0.9437, 0.9007, 0.9225]
2024-10-30 06:21:15.520356: Epoch time: 1047.37 s
2024-10-30 06:21:17.938854: 
2024-10-30 06:21:17.940368: Epoch 49
2024-10-30 06:21:17.941298: Current learning rate: 0.0
2024-10-30 06:38:42.818114: train_loss -0.8954
2024-10-30 06:38:42.949323: val_loss -0.8912
2024-10-30 06:38:42.950359: Pseudo dice [0.9355, 0.9459, 0.9167]
2024-10-30 06:38:42.951702: Epoch time: 1044.88 s
2024-10-30 06:38:49.679816: Training done.
2024-10-30 06:38:52.467330: Using splits from existing split file: content/data/nnUNet_preprocessed/Dataset137_BraTs2021/splits_final.json
2024-10-30 06:38:52.483360: The split file contains 5 splits.
2024-10-30 06:38:52.484017: Desired fold for training: 0
2024-10-30 06:38:52.484673: This split has 1000 training and 251 validation cases.
2024-10-30 06:38:52.487775: predicting BraTS2021_00000
2024-10-30 06:39:31.659447: predicting BraTS2021_00009
2024-10-30 06:39:35.481839: predicting BraTS2021_00016
2024-10-30 06:39:39.229539: predicting BraTS2021_00024
2024-10-30 06:39:42.984093: predicting BraTS2021_00028
2024-10-30 06:39:46.731898: predicting BraTS2021_00031
2024-10-30 06:39:50.480772: predicting BraTS2021_00035
2024-10-30 06:39:54.226936: predicting BraTS2021_00045
2024-10-30 06:39:57.986058: predicting BraTS2021_00046
2024-10-30 06:40:01.735700: predicting BraTS2021_00051
2024-10-30 06:43:39.442618: predicting BraTS2021_00070
2024-10-30 06:43:43.241119: predicting BraTS2021_00078
2024-10-30 06:43:47.006331: predicting BraTS2021_00085
2024-10-30 06:43:50.770144: predicting BraTS2021_00087
2024-10-30 06:43:54.530384: predicting BraTS2021_00088
2024-10-30 06:43:58.285135: predicting BraTS2021_00089
2024-10-30 06:44:02.045973: predicting BraTS2021_00099
2024-10-30 06:44:05.801480: predicting BraTS2021_00102
2024-10-30 06:44:09.558469: predicting BraTS2021_00104
2024-10-30 06:44:13.318584: predicting BraTS2021_00106
2024-10-30 06:44:17.081797: predicting BraTS2021_00107
2024-10-30 06:44:20.840302: predicting BraTS2021_00110
2024-10-30 06:44:24.593682: predicting BraTS2021_00117
2024-10-30 06:44:28.352229: predicting BraTS2021_00123
2024-10-30 06:44:32.513082: predicting BraTS2021_00126
2024-10-30 06:44:36.888377: predicting BraTS2021_00127
2024-10-30 06:44:40.657364: predicting BraTS2021_00130
2024-10-30 06:44:44.416517: predicting BraTS2021_00136
2024-10-30 06:44:48.178758: predicting BraTS2021_00137
2024-10-30 06:44:51.937968: predicting BraTS2021_00140
2024-10-30 06:44:56.212530: predicting BraTS2021_00144
2024-10-30 06:44:59.968764: predicting BraTS2021_00152
2024-10-30 06:45:03.722865: predicting BraTS2021_00160
2024-10-30 06:45:07.483070: predicting BraTS2021_00185
2024-10-30 06:45:11.241328: predicting BraTS2021_00192
2024-10-30 06:45:15.000356: predicting BraTS2021_00194
2024-10-30 06:45:16.921465: predicting BraTS2021_00211
2024-10-30 06:45:20.680818: predicting BraTS2021_00212
2024-10-30 06:45:24.440278: predicting BraTS2021_00214
2024-10-30 06:45:28.209544: predicting BraTS2021_00217
2024-10-30 06:45:31.970583: predicting BraTS2021_00218
2024-10-30 06:45:35.736766: predicting BraTS2021_00231
2024-10-30 06:45:39.497656: predicting BraTS2021_00243
2024-10-30 06:45:43.252558: predicting BraTS2021_00251
2024-10-30 06:45:47.531327: predicting BraTS2021_00266
2024-10-30 06:45:51.287831: predicting BraTS2021_00270
2024-10-30 06:45:55.041166: predicting BraTS2021_00274
2024-10-30 06:45:59.328130: predicting BraTS2021_00283
2024-10-30 06:46:03.089295: predicting BraTS2021_00286
2024-10-30 06:46:06.853846: predicting BraTS2021_00292
2024-10-30 06:46:10.617373: predicting BraTS2021_00294
2024-10-30 06:46:14.376712: predicting BraTS2021_00304
2024-10-30 06:46:18.485600: predicting BraTS2021_00311
2024-10-30 06:46:22.246630: predicting BraTS2021_00313
2024-10-30 06:46:26.007909: predicting BraTS2021_00334
2024-10-30 06:46:29.767772: predicting BraTS2021_00336
2024-10-30 06:46:33.530922: predicting BraTS2021_00350
2024-10-30 06:46:37.286046: predicting BraTS2021_00353
2024-10-30 06:46:41.043465: predicting BraTS2021_00366
2024-10-30 06:46:44.799091: predicting BraTS2021_00375
2024-10-30 06:46:48.559398: predicting BraTS2021_00376
2024-10-30 06:46:53.282515: predicting BraTS2021_00379
2024-10-30 06:46:57.037920: predicting BraTS2021_00391
2024-10-30 06:47:01.853590: predicting BraTS2021_00399
2024-10-30 06:47:05.636560: predicting BraTS2021_00402
2024-10-30 06:47:07.559218: predicting BraTS2021_00419
2024-10-30 06:47:11.316511: predicting BraTS2021_00423
2024-10-30 06:47:15.072520: predicting BraTS2021_00426
2024-10-30 06:47:19.245361: predicting BraTS2021_00430
2024-10-30 06:47:22.999372: predicting BraTS2021_00433
2024-10-30 06:47:26.820310: predicting BraTS2021_00436
2024-10-30 06:47:31.203982: predicting BraTS2021_00442
2024-10-30 06:47:34.968976: predicting BraTS2021_00464
2024-10-30 06:47:39.596097: predicting BraTS2021_00480
2024-10-30 06:47:43.358598: predicting BraTS2021_00485
2024-10-30 06:47:47.115664: predicting BraTS2021_00495
2024-10-30 06:47:50.870258: predicting BraTS2021_00500
2024-10-30 06:47:54.628235: predicting BraTS2021_00514
2024-10-30 06:47:58.387465: predicting BraTS2021_00516
2024-10-30 06:48:02.576231: predicting BraTS2021_00549
2024-10-30 06:48:06.333549: predicting BraTS2021_00558
2024-10-30 06:48:10.096562: predicting BraTS2021_00583
2024-10-30 06:48:15.830611: predicting BraTS2021_00586
2024-10-30 06:48:19.585434: predicting BraTS2021_00587
2024-10-30 06:48:23.347772: predicting BraTS2021_00597
2024-10-30 06:48:27.107550: predicting BraTS2021_00602
2024-10-30 06:48:30.859460: predicting BraTS2021_00605
2024-10-30 06:48:35.939881: predicting BraTS2021_00607
2024-10-30 06:48:39.697404: predicting BraTS2021_00630
2024-10-30 06:48:43.461063: predicting BraTS2021_00639
2024-10-30 06:48:47.216714: predicting BraTS2021_00641
2024-10-30 06:48:50.974988: predicting BraTS2021_00654
2024-10-30 06:48:55.007419: predicting BraTS2021_00693
2024-10-30 06:48:59.182265: predicting BraTS2021_00709
2024-10-30 06:49:03.425417: predicting BraTS2021_00715
2024-10-30 06:49:07.184633: predicting BraTS2021_00716
2024-10-30 06:49:11.270499: predicting BraTS2021_00727
2024-10-30 06:49:15.026329: predicting BraTS2021_00733
2024-10-30 06:49:18.788527: predicting BraTS2021_00735
2024-10-30 06:49:22.546756: predicting BraTS2021_00753
2024-10-30 06:49:26.306758: predicting BraTS2021_00757
2024-10-30 06:49:30.061075: predicting BraTS2021_00759
2024-10-30 06:49:34.277767: predicting BraTS2021_00772
2024-10-30 06:49:38.035983: predicting BraTS2021_00775
2024-10-30 06:49:41.792812: predicting BraTS2021_00777
2024-10-30 06:49:45.554285: predicting BraTS2021_00782
2024-10-30 06:49:50.115176: predicting BraTS2021_00791
2024-10-30 06:49:54.404915: predicting BraTS2021_00793
2024-10-30 06:49:58.164920: predicting BraTS2021_00810
2024-10-30 06:50:01.928055: predicting BraTS2021_00818
2024-10-30 06:50:05.693501: predicting BraTS2021_00834
2024-10-30 06:50:09.448123: predicting BraTS2021_00836
2024-10-30 06:50:13.205474: predicting BraTS2021_01001
2024-10-30 06:50:15.693557: predicting BraTS2021_01004
2024-10-30 06:50:19.454422: predicting BraTS2021_01005
2024-10-30 06:50:23.212553: predicting BraTS2021_01018
2024-10-30 06:50:27.500690: predicting BraTS2021_01037
2024-10-30 06:50:32.111532: predicting BraTS2021_01043
2024-10-30 06:50:35.867640: predicting BraTS2021_01050
2024-10-30 06:50:39.627186: predicting BraTS2021_01051
2024-10-30 06:50:43.386483: predicting BraTS2021_01058
2024-10-30 06:50:47.140859: predicting BraTS2021_01063
2024-10-30 06:50:50.896701: predicting BraTS2021_01066
2024-10-30 06:50:54.652704: predicting BraTS2021_01072
2024-10-30 06:50:56.572788: predicting BraTS2021_01079
2024-10-30 06:51:00.333962: predicting BraTS2021_01081
2024-10-30 06:51:04.095645: predicting BraTS2021_01088
2024-10-30 06:51:07.853176: predicting BraTS2021_01089
2024-10-30 06:51:12.220759: predicting BraTS2021_01096
2024-10-30 06:51:16.439008: predicting BraTS2021_01101
2024-10-30 06:51:20.199931: predicting BraTS2021_01110
2024-10-30 06:51:23.958187: predicting BraTS2021_01118
2024-10-30 06:51:27.712522: predicting BraTS2021_01123
2024-10-30 06:51:31.466960: predicting BraTS2021_01124
2024-10-30 06:51:35.226388: predicting BraTS2021_01137
2024-10-30 06:51:37.145932: predicting BraTS2021_01141
2024-10-30 06:51:40.912139: predicting BraTS2021_01142
2024-10-30 06:51:44.672593: predicting BraTS2021_01146
2024-10-30 06:51:48.422028: predicting BraTS2021_01147
2024-10-30 06:51:52.178543: predicting BraTS2021_01152
2024-10-30 06:51:55.931357: predicting BraTS2021_01162
2024-10-30 06:51:59.687795: predicting BraTS2021_01169
2024-10-30 06:52:02.072124: predicting BraTS2021_01170
2024-10-30 06:52:06.718108: predicting BraTS2021_01173
2024-10-30 06:52:10.469200: predicting BraTS2021_01181
2024-10-30 06:52:12.386401: predicting BraTS2021_01183
2024-10-30 06:52:14.950530: predicting BraTS2021_01191
2024-10-30 06:52:18.712684: predicting BraTS2021_01198
2024-10-30 06:52:20.630732: predicting BraTS2021_01206
2024-10-30 06:52:27.290148: predicting BraTS2021_01208
2024-10-30 06:52:31.055177: predicting BraTS2021_01209
2024-10-30 06:52:34.810378: predicting BraTS2021_01210
2024-10-30 06:52:38.566615: predicting BraTS2021_01211
2024-10-30 06:52:42.331408: predicting BraTS2021_01212
2024-10-30 06:52:46.567940: predicting BraTS2021_01225
2024-10-30 06:52:50.907537: predicting BraTS2021_01231
2024-10-30 06:52:54.662145: predicting BraTS2021_01237
2024-10-30 06:52:58.896813: predicting BraTS2021_01238
2024-10-30 06:53:02.651386: predicting BraTS2021_01239
2024-10-30 06:53:04.572262: predicting BraTS2021_01240
2024-10-30 06:53:08.773125: predicting BraTS2021_01262
2024-10-30 06:53:10.693692: predicting BraTS2021_01265
2024-10-30 06:53:14.458619: predicting BraTS2021_01273
2024-10-30 06:53:18.213500: predicting BraTS2021_01274
2024-10-30 06:53:21.968327: predicting BraTS2021_01282
2024-10-30 06:53:26.644094: predicting BraTS2021_01283
2024-10-30 06:53:30.405672: predicting BraTS2021_01284
2024-10-30 06:53:34.162762: predicting BraTS2021_01289
2024-10-30 06:53:37.936625: predicting BraTS2021_01294
2024-10-30 06:53:41.699653: predicting BraTS2021_01296
2024-10-30 06:53:45.455069: predicting BraTS2021_01305
2024-10-30 06:53:49.846330: predicting BraTS2021_01306
2024-10-30 06:53:53.601208: predicting BraTS2021_01312
2024-10-30 06:53:57.373390: predicting BraTS2021_01316
2024-10-30 06:54:00.340504: predicting BraTS2021_01319
2024-10-30 06:54:04.094103: predicting BraTS2021_01320
2024-10-30 06:54:07.856660: predicting BraTS2021_01326
2024-10-30 06:54:12.279999: predicting BraTS2021_01328
2024-10-30 06:54:16.064161: predicting BraTS2021_01331
2024-10-30 06:54:19.821869: predicting BraTS2021_01333
2024-10-30 06:54:21.740429: predicting BraTS2021_01335
2024-10-30 06:54:25.498666: predicting BraTS2021_01339
2024-10-30 06:54:30.316808: predicting BraTS2021_01340
2024-10-30 06:54:34.073142: predicting BraTS2021_01342
2024-10-30 06:54:37.829744: predicting BraTS2021_01345
2024-10-30 06:54:41.582858: predicting BraTS2021_01349
2024-10-30 06:54:45.336941: predicting BraTS2021_01350
2024-10-30 06:54:49.089316: predicting BraTS2021_01356
2024-10-30 06:54:52.845422: predicting BraTS2021_01357
2024-10-30 06:54:56.603525: predicting BraTS2021_01359
2024-10-30 06:55:00.360468: predicting BraTS2021_01361
2024-10-30 06:55:04.117874: predicting BraTS2021_01363
2024-10-30 06:55:06.044468: predicting BraTS2021_01367
2024-10-30 06:55:10.414840: predicting BraTS2021_01375
2024-10-30 06:55:12.334896: predicting BraTS2021_01387
2024-10-30 06:55:16.547422: predicting BraTS2021_01388
2024-10-30 06:55:20.753021: predicting BraTS2021_01389
2024-10-30 06:55:24.514637: predicting BraTS2021_01398
2024-10-30 06:55:28.281335: predicting BraTS2021_01399
2024-10-30 06:55:30.207165: predicting BraTS2021_01402
2024-10-30 06:55:34.539119: predicting BraTS2021_01423
2024-10-30 06:55:38.302315: predicting BraTS2021_01425
2024-10-30 06:55:42.556866: predicting BraTS2021_01440
2024-10-30 06:55:46.317772: predicting BraTS2021_01443
2024-10-30 06:55:50.081189: predicting BraTS2021_01450
2024-10-30 06:55:53.839140: predicting BraTS2021_01461
2024-10-30 06:56:00.230680: predicting BraTS2021_01465
2024-10-30 06:56:03.989869: predicting BraTS2021_01467
2024-10-30 06:56:07.755794: predicting BraTS2021_01476
2024-10-30 06:56:12.241414: predicting BraTS2021_01480
2024-10-30 06:56:15.996303: predicting BraTS2021_01481
2024-10-30 06:56:21.556084: predicting BraTS2021_01488
2024-10-30 06:56:26.138518: predicting BraTS2021_01489
2024-10-30 06:56:29.897050: predicting BraTS2021_01490
2024-10-30 06:56:34.835136: predicting BraTS2021_01492
2024-10-30 06:56:38.598660: predicting BraTS2021_01494
2024-10-30 06:56:42.775764: predicting BraTS2021_01496
2024-10-30 06:56:46.539214: predicting BraTS2021_01500
2024-10-30 06:56:50.293704: predicting BraTS2021_01501
2024-10-30 06:56:54.627540: predicting BraTS2021_01507
2024-10-30 06:56:58.386682: predicting BraTS2021_01513
2024-10-30 06:57:02.142802: predicting BraTS2021_01518
2024-10-30 06:57:05.898141: predicting BraTS2021_01521
2024-10-30 06:57:09.653991: predicting BraTS2021_01522
2024-10-30 06:57:13.411404: predicting BraTS2021_01524
2024-10-30 06:57:17.166674: predicting BraTS2021_01530
2024-10-30 06:57:20.922453: predicting BraTS2021_01538
2024-10-30 06:57:24.968153: predicting BraTS2021_01540
2024-10-30 06:57:28.720197: predicting BraTS2021_01547
2024-10-30 06:57:32.485677: predicting BraTS2021_01560
2024-10-30 06:57:36.987668: predicting BraTS2021_01562
2024-10-30 06:57:40.741725: predicting BraTS2021_01566
2024-10-30 06:57:45.312564: predicting BraTS2021_01567
2024-10-30 06:57:49.655507: predicting BraTS2021_01582
2024-10-30 06:57:53.962080: predicting BraTS2021_01584
2024-10-30 06:57:58.076683: predicting BraTS2021_01593
2024-10-30 06:57:59.999166: predicting BraTS2021_01594
2024-10-30 06:58:03.755682: predicting BraTS2021_01599
2024-10-30 06:58:09.744367: predicting BraTS2021_01601
2024-10-30 06:58:13.507277: predicting BraTS2021_01608
2024-10-30 06:58:18.115953: predicting BraTS2021_01609
2024-10-30 06:58:22.676369: predicting BraTS2021_01613
2024-10-30 06:58:26.440780: predicting BraTS2021_01614
2024-10-30 06:58:30.212627: predicting BraTS2021_01626
2024-10-30 06:58:33.968510: predicting BraTS2021_01634
2024-10-30 06:58:37.739740: predicting BraTS2021_01651
2024-10-30 06:58:41.499121: predicting BraTS2021_01652
2024-10-30 06:58:45.255692: predicting BraTS2021_01653
2024-10-30 06:58:47.178794: predicting BraTS2021_01660
2024-10-30 06:58:49.099321: predicting BraTS2021_01661
2024-10-30 06:58:51.355660: predicting BraTS2021_01662
2024-10-30 06:59:17.631859: Validation complete
2024-10-30 06:59:17.632695: Mean Validation Dice:  0.9033687085028953
